"""
æ‰¹é‡æ·»åŠ å¼•ç”¨æ¥æº
ä¸º ML Series æ–‡ç« æ·»åŠ çœŸå®çš„å­¦æœ¯å’ŒæŠ€æœ¯å¼•ç”¨
"""

import os
import re
from pathlib import Path
from typing import Dict, List

# ä¸»é¢˜åˆ°å¼•ç”¨çš„æ˜ å°„
REFERENCES_MAP = {
    # æœºå™¨å­¦ä¹ åŸºç¡€
    "çº¿æ€§å›å½’": [
        "[The Elements of Statistical Learning](https://hastie.su.domains/ElemStatLearn/) - Hastie et al., Springer",
        "[Linear Regression in scikit-learn](https://scikit-learn.org/stable/modules/linear_model.html) - å®˜æ–¹æ–‡æ¡£",
    ],
    "é€»è¾‘å›å½’": [
        "[Logistic Regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression) - scikit-learn",
        "[Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/publication/pattern-recognition-and-machine-learning/) - Bishop, Springer",
    ],
    "å†³ç­–æ ‘": [
        "[Classification and Regression Trees](https://www.taylorfrancis.com/books/mono/10.1201/9781315139470/classification-regression-trees-leo-breiman) - Breiman et al.",
        "[XGBoost Documentation](https://xgboost.readthedocs.io/) - é™ˆå¤©å¥‡ç­‰",
    ],
    "é›†æˆå­¦ä¹ ": [
        "[Ensemble Methods in Machine Learning](https://www.sciencedirect.com/science/article/pii/S0893608000000124) - Dietterich, 2000",
        "[Random Forests](https://link.springer.com/article/10.1023/A:1010933404324) - Breiman, 2001",
    ],

    # æ·±åº¦å­¦ä¹ 
    "CNN|å·ç§¯": [
        "[ImageNet Classification with Deep CNNs](https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html) - AlexNet, 2012",
        "[Very Deep Convolutional Networks](https://arxiv.org/abs/1409.1556) - VGGNet, 2014",
        "[Deep Residual Learning](https://arxiv.org/abs/1512.03385) - ResNet, 2015",
    ],
    "RNN|å¾ªç¯": [
        "[Long Short-Term Memory](https://www.bioinf.jku.at/publications/older/2604.pdf) - Hochreiter & Schmidhuber, 1997",
        "[Sequence to Sequence Learning](https://arxiv.org/abs/1409.3215) - Sutskever et al., 2014",
    ],
    "Transformer|Attention": [
        "[Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Vaswani et al., 2017",
        "[BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805) - Devlin et al., 2018",
    ],
    "GPT|è¯­è¨€æ¨¡å‹": [
        "[Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) - GPT-2, 2019",
        "[GPT-4 Technical Report](https://arxiv.org/abs/2303.08774) - OpenAI, 2023",
    ],

    # æ¨èç³»ç»Ÿ
    "æ¨è|ååŒè¿‡æ»¤": [
        "[Matrix Factorization Techniques for Recommender Systems](https://ieeexplore.ieee.org/document/5197422) - Koren et al., 2009",
        "[Wide & Deep Learning for Recommender Systems](https://arxiv.org/abs/1606.07792) - Google, 2016",
        "[DeepFM: A Factorization-Machine based Neural Network](https://arxiv.org/abs/1703.04247) - 2017",
    ],
    "å¬å›|å¬å›ç®—æ³•": [
        "[Approximate Nearest Neighbor Search](https://arxiv.org/abs/1603.09320) - FAISS",
        "[Efficient and robust approximate nearest neighbor search](https://ieeexplore.ieee.org/document/7001931) - HNSW",
    ],

    # å¼ºåŒ–å­¦ä¹ 
    "å¼ºåŒ–å­¦ä¹ |RL": [
        "[Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book.html) - Sutton & Barto",
        "[Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602) - DQN, 2013",
        "[Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347) - PPO, 2017",
    ],
    "DQN": [
        "[Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236) - Nature 2015",
        "[Deep Q-Network](https://arxiv.org/abs/1312.5602) - Mnih et al., 2013",
    ],

    # NLP
    "NLP|è‡ªç„¶è¯­è¨€": [
        "[Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) - Jurafsky & Martin",
        "[Natural Language Processing with Transformers](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/) - O'Reilly",
    ],
    "BERT|é¢„è®­ç»ƒ": [
        "[BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805) - Google, 2018",
        "[RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) - Facebook, 2019",
    ],
    "RAG|æ£€ç´¢å¢å¼º": [
        "[Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) - Facebook, 2020",
        "[Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906) - Karpukhin et al., 2020",
    ],

    # æ¨¡å‹ä¼˜åŒ–
    "é‡åŒ–|æ¨¡å‹å‹ç¼©": [
        "[Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference](https://arxiv.org/abs/1712.05877) - Google, 2017",
        "[ONNX Runtime Quantization](https://onnxruntime.ai/docs/performance/quantization.html) - Microsoft",
    ],
    "è’¸é¦|çŸ¥è¯†è’¸é¦": [
        "[Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531) - Hinton et al., 2015",
        "[Knowledge Distillation Survey](https://arxiv.org/abs/2006.05525) - 2020",
    ],

    # ç‰¹å¾å·¥ç¨‹
    "ç‰¹å¾å·¥ç¨‹|ç‰¹å¾é€‰æ‹©": [
        "[Feature Engineering for Machine Learning](https://www.oreilly.com/library/view/feature-engineering-for/9781491953235/) - O'Reilly",
        "[sklearn.feature_selection](https://scikit-learn.org/stable/modules/feature_selection.html) - å®˜æ–¹æ–‡æ¡£",
    ],

    # è®¡ç®—æœºè§†è§‰
    "ç›®æ ‡æ£€æµ‹|æ£€æµ‹": [
        "[Rich feature hierarchies for accurate object detection](https://arxiv.org/abs/1311.2524) - R-CNN, 2014",
        "[You Only Look Once](https://arxiv.org/abs/1506.02640) - YOLO, 2015",
        "[Faster R-CNN](https://arxiv.org/abs/1506.01497) - Ren et al., 2015",
    ],
    "å›¾åƒåˆ†å‰²|åˆ†å‰²": [
        "[Fully Convolutional Networks for Semantic Segmentation](https://arxiv.org/abs/1411.4038) - FCN, 2014",
        "[U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597) - 2015",
    ],

    # å…¶ä»–
    "é™ç»´|PCA": [
        "[PCA on sklearn](https://scikit-learn.org/stable/modules/decomposition.html#pca) - å®˜æ–¹æ–‡æ¡£",
        "[Dimensionality Reduction: A Comparative Review](https://www.mdpi.com/1407064) - 2021",
    ],
    "è´å¶æ–¯": [
        "[Probabilistic Programming & Bayesian Methods for Hackers](https://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/) - å¼€æºä¹¦ç±",
        "[PyMC3 Documentation](https://docs.pymc.io/) - è´å¶æ–¯å»ºæ¨¡æ¡†æ¶",
    ],
    "å¼‚å¸¸æ£€æµ‹": [
        "[Isolation Forest](https://ieeexplore.ieee.org/document/4781136) - Liu et al., 2008",
        "[Anomaly Detection Survey](https://arxiv.org/abs/1901.03407) - 2019",
    ],
}

# é€šç”¨å¼•ç”¨ï¼ˆå½“æ— æ³•åŒ¹é…ç‰¹å®šä¸»é¢˜æ—¶ä½¿ç”¨ï¼‰
GENERIC_REFERENCES = [
    "**æ ¸å¿ƒè®ºæ–‡**ï¼š",
    "- [Machine Learning](https://www.nature.com/articles/nature14539) - Nature 2015 æ·±åº¦å­¦ä¹ ç»¼è¿°",
    "- [Deep Learning](https://www.deeplearningbook.org/) - Goodfellow, Bengio, Courville",
    "",
    "**å¼€æºå·¥å…·**ï¼š",
    "- [PyTorch](https://pytorch.org/) - æ·±åº¦å­¦ä¹ æ¡†æ¶",
    "- [scikit-learn](https://scikit-learn.org/) - æœºå™¨å­¦ä¹ åº“",
    "- [Hugging Face](https://huggingface.co/) - é¢„è®­ç»ƒæ¨¡å‹åº“",
]


def find_article_file(episode: int, base_path: str = "/Users/z/Documents/work/content-forge-ai/data/series/ML_series") -> Path:
    """æŸ¥æ‰¾æ–‡ç« æ–‡ä»¶"""
    base = Path(base_path)
    ep_str = f"episode_{episode:03d}"

    for series_dir in sorted(base.iterdir()):
        if not series_dir.is_dir():
            continue
        ep_dir = series_dir / ep_str
        if ep_dir.exists():
            articles = list(ep_dir.glob("*_article.md"))
            if articles:
                return max(articles, key=lambda p: p.stat().st_size)
    return None


def get_references_for_topic(title: str) -> List[str]:
    """æ ¹æ®æ ‡é¢˜è·å–ç›¸å…³å¼•ç”¨"""
    refs = []

    for pattern, ref_list in REFERENCES_MAP.items():
        if re.search(pattern, title, re.IGNORECASE):
            refs.extend(ref_list)

    if not refs:
        refs = GENERIC_REFERENCES.copy()

    return refs[:5]  # æœ€å¤š5ä¸ªå¼•ç”¨


def add_references_to_article(file_path: Path) -> bool:
    """ä¸ºæ–‡ç« æ·»åŠ å¼•ç”¨"""
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # æ£€æŸ¥æ˜¯å¦å·²æœ‰è¶³å¤Ÿå¼•ç”¨
    current_refs = len(re.findall(r'\[.*?\]\(https?://', content))
    if current_refs >= 5:
        return False  # å·²æœ‰è¶³å¤Ÿå¼•ç”¨

    # æå–æ ‡é¢˜
    title_match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
    title = title_match.group(1) if title_match else ""

    # è·å–ç›¸å…³å¼•ç”¨
    refs = get_references_for_topic(title)

    # æ„å»ºå¼•ç”¨æ–‡æœ¬
    refs_text = "\n**å»¶ä¼¸é˜…è¯»**ï¼š\n\n" + "\n".join(refs)

    # æŸ¥æ‰¾æ’å…¥ä½ç½®ï¼ˆåœ¨"å»¶ä¼¸é˜…è¯»"æˆ–"å…³äºä½œè€…"ä¹‹å‰ï¼‰
    insert_patterns = [
        r'(\*\*å»¶ä¼¸é˜…è¯»\*\*ï¼š)',
        r'(\*\*å…³äºä½œè€…\*\*ï¼š)',
        r'(\*\*äº’åŠ¨äº¤æµ\*\*ï¼š)',
        r'(-{3,}\n\*\*å…ƒæ•°æ®\*\*)',
    ]

    inserted = False
    for pattern in insert_patterns:
        if re.search(pattern, content):
            # åœ¨è¯¥ä½ç½®ä¹‹å‰æ’å…¥
            content = re.sub(pattern, refs_text + "\n\n\\1", content, count=1)
            inserted = True
            break

    if not inserted:
        # åœ¨æ–‡ç« æœ«å°¾æ·»åŠ 
        content += "\n\n" + refs_text

    # å†™å›æ–‡ä»¶
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(content)

    return True


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="ä¸ºML Seriesæ–‡ç« æ·»åŠ å¼•ç”¨")
    parser.add_argument("--episode", type=int, help="æŒ‡å®šæœŸå·")
    parser.add_argument("--start", type=int, default=1, help="èµ·å§‹æœŸå·")
    parser.add_argument("--end", type=int, default=100, help="ç»“æŸæœŸå·")
    parser.add_argument("--dry-run", action="store_true", help="ä»…é¢„è§ˆï¼Œä¸ä¿®æ”¹")

    args = parser.parse_args()

    if args.episode:
        episodes = [args.episode]
    else:
        episodes = range(args.start, args.end + 1)

    updated = 0
    for ep in episodes:
        file_path = find_article_file(ep)
        if not file_path:
            continue

        if args.dry_run:
            print(f"[é¢„è§ˆ] ç¬¬{ep:03d}æœŸ: {file_path.name}")
        else:
            if add_references_to_article(file_path):
                print(f"âœ… ç¬¬{ep:03d}æœŸ: å·²æ·»åŠ å¼•ç”¨")
                updated += 1
            else:
                print(f"â­ï¸  ç¬¬{ep:03d}æœŸ: å·²æœ‰è¶³å¤Ÿå¼•ç”¨")

    print(f"\nğŸ“Š ç»Ÿè®¡: æ›´æ–°äº† {updated} ç¯‡æ–‡ç« ")


if __name__ == "__main__":
    main()
