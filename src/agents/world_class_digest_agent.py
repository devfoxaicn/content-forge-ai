"""
World Class AI News Digest Generator
é¡¶çº§AIæ–°é—»ç®€æŠ¥ç”Ÿæˆå™¨ v7.0 - å¢å¼ºç‰ˆ

v7.0 æ–°ç‰¹æ€§:
- ä½¿ç”¨è¯„åˆ†ç­›é€‰åçš„æ–°é—» (scored_trends)
- ç”Ÿæˆå®Œæ•´JSONç»“æ„åŒ–æ•°æ®
- ä¸ºæ¯æ¡æ–°é—»æ·»åŠ èƒŒæ™¯åˆ†æå’Œè¡Œä¸šå½±å“
- æå–æ ¸å¿ƒæ´å¯Ÿ (Core Insights)
- è¯†åˆ«çƒ­é—¨è¯é¢˜ (Trending Topics)
- ä¼˜åŒ–ç¿»è¯‘è´¨é‡ (Few-shotæç¤º)
"""

from datetime import datetime
from typing import Dict, Any, List
from loguru import logger
import json
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from collections import Counter


class WorldClassDigestAgent:
    """ä¸–ç•Œé¡¶çº§AIæ–°é—»ç®€æŠ¥ç”Ÿæˆå™¨ v7.0"""

    def __init__(self, config: Dict[str, Any], prompts: Dict[str, Any]):
        self.config = config
        self.prompts = prompts
        self.name = "world_class_digest"
        self.llm = self._init_llm()

        # ç¿»è¯‘é…ç½®
        agent_config = config.get("agents", {}).get("world_class_digest", {})
        self.translate_enabled = agent_config.get("translate_enabled", True)
        self.batch_size = agent_config.get("batch_size", 5)

        self.log(f"v7.0åˆå§‹åŒ–å®Œæˆï¼Œç¿»è¯‘åŠŸèƒ½: {'å¯ç”¨' if self.translate_enabled else 'ç¦ç”¨'}")

    def _init_llm(self) -> ChatOpenAI:
        """åˆå§‹åŒ–LLM"""
        try:
            import os
            from pathlib import Path
            from dotenv import load_dotenv

            # åŠ è½½.envæ–‡ä»¶
            project_root = Path(__file__).parent.parent.parent
            env_file = project_root / ".env"
            if env_file.exists():
                load_dotenv(env_file)
                self.log(f"å·²åŠ è½½ç¯å¢ƒå˜é‡æ–‡ä»¶: {env_file}")

            llm_config = self.config.get("llm", {})
            provider = llm_config.get("provider", "zhipuai")

            if provider == "zhipuai":
                api_key = os.getenv("ZHIPUAI_API_KEY")
                if not api_key:
                    api_key = self.config.get("api_keys", {}).get("zhipuai")

                if not api_key:
                    self.log("æœªé…ç½®ZHIPUAI_API_KEY", "WARNING")
                    return None

                zhipu_config = llm_config.get("zhipuai", {})
                return ChatOpenAI(
                    model=zhipu_config.get("model", "glm-4-flash"),
                    openai_api_key=api_key,
                    base_url=zhipu_config.get("base_url", "https://open.bigmodel.cn/api/coding/paas/v4/"),
                    temperature=zhipu_config.get("temperature", 0.7),
                    max_tokens=zhipu_config.get("max_tokens", 8000),
                    timeout=zhipu_config.get("timeout", 600)
                )
            return None
        except Exception as e:
            self.log(f"LLMåˆå§‹åŒ–å¤±è´¥: {e}", "WARNING")
            return None

    def log(self, message: str, level: str = "INFO"):
        logger.log(level, f"[WorldClassDigestAgent] {message}")

    def execute(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œç®€æŠ¥ç”Ÿæˆ"""
        try:
            self.log("å¼€å§‹ç”Ÿæˆä¸–ç•Œé¡¶çº§AIæ–°é—»ç®€æŠ¥ v7.0...")

            # ä½¿ç”¨ scored_trends è€Œä¸æ˜¯ categorized_trends
            scored_trends = state.get("scored_trends", {})
            editors_pick = state.get("editors_pick", [])
            source_status = state.get("source_status", {})

            # ç”Ÿæˆç®€æŠ¥
            digest = self._generate_world_class_digest(
                scored_trends,
                editors_pick,
                source_status
            )

            return {
                **state,
                "news_digest": digest,
                "current_step": "digest_generated"
            }

        except Exception as e:
            self.log(f"ç®€æŠ¥ç”Ÿæˆå¤±è´¥: {e}", "ERROR")
            return {
                **state,
                "error_message": str(e),
                "current_step": "world_class_digest_failed"
            }

    def _generate_world_class_digest(
        self,
        scored_trends: Dict[str, Dict],
        editors_pick: List[Dict],
        source_status: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ç”Ÿæˆä¸–ç•Œé¡¶çº§AIæ–°é—»ç®€æŠ¥ v7.0"""

        today = datetime.now()
        issue_number = today.strftime("%Y%m%d")

        # è®¡ç®—æ€»æ•°
        total_count = sum(
            cat_data.get("count", 0)
            for cat_data in scored_trends.values()
        )

        self.log(f"ç”Ÿæˆç®€æŠ¥: {total_count}æ¡ç²¾é€‰çƒ­ç‚¹")

        # ä¸ºæ–°é—»å¢å¼ºä¿¡æ¯ï¼ˆç¿»è¯‘ã€èƒŒæ™¯ã€å½±å“ã€æ ‡ç­¾ï¼‰
        enhanced_editors_pick = self._enhance_news_items(editors_pick)
        for cat_name, cat_data in scored_trends.items():
            items = cat_data.get("items", [])
            enhanced_items = self._enhance_news_items(items)
            cat_data["items"] = enhanced_items

        # æå–æ ¸å¿ƒæ´å¯Ÿ
        all_items = []
        for cat_data in scored_trends.values():
            all_items.extend(cat_data.get("items", []))

        core_insights = self._extract_core_insights(all_items)

        # è¯†åˆ«çƒ­é—¨è¯é¢˜
        trending_topics = self._identify_trending_topics(all_items)

        # ç”ŸæˆMarkdownå†…å®¹
        markdown_content = self._generate_markdown_v7(
            scored_trends,
            enhanced_editors_pick,
            core_insights,
            trending_topics,
            source_status,
            today,
            issue_number,
            total_count
        )

        # ç”ŸæˆJSONæ•°æ®
        json_data = self._generate_json_v7(
            scored_trends,
            enhanced_editors_pick,
            core_insights,
            trending_topics,
            source_status,
            today,
            issue_number,
            total_count,
            markdown_content
        )

        return json_data

    def _enhance_news_items(self, items: List[Dict]) -> List[Dict]:
        """ä¸ºæ–°é—»æ¡ç›®å¢å¼ºä¿¡æ¯ï¼ˆç¿»è¯‘ã€èƒŒæ™¯ã€å½±å“ã€æ ‡ç­¾ï¼‰"""
        if not items:
            return items

        # æ‰¹é‡ç¿»è¯‘
        if self.translate_enabled and self.llm:
            items = self._batch_translate_items(items)

        # ä¸ºæ¯æ¡æ–°é—»ç”ŸæˆèƒŒæ™¯ã€å½±å“ã€æ ‡ç­¾ï¼ˆä»…å¯¹é‡è¦æ–°é—»ï¼‰
        for item in items:
            importance = item.get("importance_score", 0)
            if importance >= 70:  # åªä¸ºé‡è¦æ–°é—»ç”Ÿæˆè¯¦ç»†åˆ†æ
                enhanced = self._generate_background_analysis(item)
                item.update(enhanced)

        return items

    def _batch_translate_items(self, items: List[Dict]) -> List[Dict]:
        """æ‰¹é‡ç¿»è¯‘æ–°é—»æ ‡é¢˜å’Œæ‘˜è¦"""
        if not items or not self.translate_enabled:
            return items

        # æ£€æŸ¥æ˜¯å¦å·²æœ‰ä¸­æ–‡
        if items[0].get("title_cn"):
            return items

        # æ„å»ºç¿»è¯‘æç¤ºï¼ˆä½¿ç”¨Few-shotç¤ºä¾‹ï¼‰
        news_items = []
        for i, item in enumerate(items):
            title = item.get("title", "").replace('&amp;', '&').replace('&quot;', '"')
            desc = item.get("description", "").replace('&amp;', '&').replace('&quot;', '"')
            desc = desc.replace('<p>', '').replace('</p>', '').replace('<br>', ' ')[:200]
            news_items.append(f"{i+1}. æ ‡é¢˜: {title}\n   æ‘˜è¦: {desc}")

        # æ„å»ºé¡¶çº§ç§‘æŠ€åª’ä½“çº§åˆ«çš„ç¿»è¯‘promptï¼ˆv8.0ï¼‰
        prompt = f"""ä½ æ˜¯TechCrunchã€The Vergeã€36æ°ªã€è™å—…ç­‰ä¸–ç•Œé¡¶çº§ç§‘æŠ€åª’ä½“çš„ä¸­æ–‡ä¸»ç¼–ï¼Œæ‹¥æœ‰20å¹´ç§‘æŠ€æ–°é—»ç¿»è¯‘ç»éªŒã€‚

ã€é‡è¦ã€‘ä½ å¿…é¡»å°†è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡ï¼Œè¾“å‡ºæ ¼å¼å¿…é¡»æ˜¯JSONå¯¹è±¡ï¼Œé”®åä¸ºåºå·ï¼ˆ"1", "2", "3"...ï¼‰ã€‚

ã€ç¿»è¯‘åŸåˆ™ - é¡¶çº§ç§‘æŠ€åª’ä½“æ ‡å‡†ã€‘
1. **æ ‡é¢˜è¦æ±‚**ï¼š
   - ç®€æ´æœ‰åŠ›ï¼Œç›´å‡»è¦ç‚¹ï¼ˆä¸è¶…è¿‡30å­—ï¼‰
   - âœ… å¥½çš„é£æ ¼ï¼š"OpenAIå‘å¸ƒGPT-5ï¼Œæ”¯æŒ100ä¸‡tokensä¸Šä¸‹æ–‡"
   - âŒ å·®çš„é£æ ¼ï¼š"OpenAIä»Šå¤©å‘å¸ƒäº†æ–°çš„GPT-5æ¨¡å‹"
   - çªå‡ºæŠ€æœ¯äº®ç‚¹æˆ–å•†ä¸šä»·å€¼
   - ä½¿ç”¨ä¸»åŠ¨è¯­æ€ï¼Œé¿å…æ‹–æ²“

2. **æœ¯è¯­å¤„ç†**ï¼š
   - ä¿ç•™ä¸“ä¸šæœ¯è¯­ä¸ç¿»è¯‘ï¼šLLMã€RAGã€Transformerã€Agentã€GPUã€APIã€SDKç­‰
   - æœºæ„åä¿ç•™åŸæ–‡ï¼šOpenAIã€Metaã€Googleã€Microsoftç­‰
   - äº§å“åä¿ç•™åŸæ–‡ï¼šChatGPTã€GitHubã€Hugging Faceç­‰

3. **æ‘˜è¦è¦æ±‚**ï¼š
   - ç²¾ç‚¼æœ‰åŠ›ï¼Œæ§åˆ¶åœ¨60-100å­—
   - çªå‡ºæ ¸å¿ƒä¿¡æ¯ï¼Œå»é™¤å†—ä½™
   - ä½¿ç”¨ç§‘æŠ€åª’ä½“å¸¸ç”¨è¡¨è¾¾

4. **è¯­è¨€é£æ ¼**ï¼š
   - ç¬¦åˆä¸­æ–‡ç§‘æŠ€åª’ä½“ä¹ æƒ¯
   - ç®€æ´æµç•…ï¼Œæ˜“äºå¿«é€Ÿé˜…è¯»
   - å‡†ç¡®ä¼ è¾¾æŠ€æœ¯ç»†èŠ‚

ã€è¾“å‡ºæ ¼å¼ç¤ºä¾‹ã€‘
{{
  "1": {{"title": "OpenAIå‘å¸ƒGPT-5ï¼Œæ”¯æŒ100ä¸‡tokensä¸Šä¸‹æ–‡", "summary": "OpenAIæ¨å‡ºGPT-5ï¼Œä¸Šä¸‹æ–‡çª—å£æ‰©å¤§è‡³100ä¸‡tokensï¼Œæ€§èƒ½æå‡40%"}},
  "2": {{"title": "Metaå‘å¸ƒæ–°å¼€æºå¤§æ¨¡å‹", "summary": "Metaæ¨å‡ºå…¨æ–°å¼€æºLLMï¼Œæ€§èƒ½åª²ç¾GPT-4ï¼Œæ”¯æŒå•†ç”¨"}}
}}

ã€å¾…ç¿»è¯‘æ–°é—»ã€‘
{chr(10).join(news_items)}

è¯·ç›´æ¥è¾“å‡ºJSONæ ¼å¼ï¼ˆä¸è¦æœ‰ä»»ä½•é¢å¤–è¯´æ˜ï¼Œæ‰€æœ‰æ ‡é¢˜å’Œæ‘˜è¦å¿…é¡»æ˜¯ä¸­æ–‡ï¼‰ï¼š"""

        try:
            # ä½¿ç”¨SystemMessage + HumanMessageå¢å¼ºæŒ‡ä»¤éµå¾ªï¼ˆv8.0 - é¡¶çº§ç§‘æŠ€åª’ä½“çº§åˆ«ï¼‰
            system_msg = """ä½ æ˜¯TechCrunchã€The Vergeã€36æ°ªã€è™å—…ç­‰ä¸–ç•Œé¡¶çº§ç§‘æŠ€åª’ä½“çš„ä¸­æ–‡ä¸»ç¼–ï¼Œæ‹¥æœ‰20å¹´ç§‘æŠ€æ–°é—»ç¿»è¯‘ç»éªŒã€‚

ã€æ ¸å¿ƒè¦æ±‚ã€‘
1. ä½ å¿…é¡»å°†è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡
2. è¾“å‡ºå¿…é¡»æ˜¯JSONæ ¼å¼ï¼Œé”®åä¸ºåºå·ï¼š"1", "2", "3"...
3. ä¿ç•™ä¸“ä¸šæœ¯è¯­ä¸ç¿»è¯‘ï¼šLLMã€RAGã€Transformerã€Agentã€GPUã€APIã€SDKç­‰
4. æœºæ„åä¿ç•™åŸæ–‡ï¼šOpenAIã€Metaã€Googleã€Microsoftç­‰

ã€ç¿»è¯‘ç¤ºä¾‹ã€‘
{{"1": {{"title": "OpenAIå‘å¸ƒGPT-5ï¼Œæ”¯æŒ100ä¸‡tokensä¸Šä¸‹æ–‡", "summary": "OpenAIæ¨å‡ºGPT-5ï¼Œä¸Šä¸‹æ–‡çª—å£æ‰©å¤§è‡³100ä¸‡tokensï¼Œæ€§èƒ½æå‡40%"}}}}
{{"2": {{"title": "Metaå‘å¸ƒæ–°å¼€æºå¤§æ¨¡å‹", "summary": "Metaæ¨å‡ºå…¨æ–°å¼€æºLLMï¼Œæ€§èƒ½åª²ç¾GPT-4ï¼Œæ”¯æŒå•†ç”¨"}}}}"""

            response = self.llm.invoke([
                SystemMessage(content=system_msg),
                HumanMessage(content=prompt)
            ])
            result = response.content.strip()

            # æ¸…ç†markdownä»£ç å—
            if result.startswith('```'):
                result = result.split('```', 2)[1] if '```' in result[3:] else result
                result = result.strip()
                if result.startswith('json'):
                    result = result[4:].strip()
                if result.endswith('```'):
                    result = result[:-3].strip()

            # è§£æJSON
            translated_data = json.loads(result)

            # å¤„ç†ä¸åŒæ ¼å¼
            # æ ¼å¼1: {"1": {"title": "...", "summary": "..."}, ...}
            # æ ¼å¼2: [{"title": "...", "summary": "..."}, ...] (æŒ‰é¡ºåºå¯¹åº”)
            if isinstance(translated_data, list):
                # å¦‚æœè¿”å›åˆ—è¡¨ï¼ŒæŒ‰é¡ºåºæ˜ å°„
                for i, item in enumerate(items):
                    if i < len(translated_data):
                        item["title_cn"] = translated_data[i].get("title", item.get("title", ""))
                        item["summary_cn"] = translated_data[i].get("summary", item.get("description", ""))[:150]
                    else:
                        item["title_cn"] = item.get("title", "")
                        item["summary_cn"] = item.get("description", "")[:150]
            else:
                # å­—å…¸æ ¼å¼ï¼ŒæŒ‰ç¼–å·é”®æŸ¥æ‰¾
                for i, item in enumerate(items):
                    key = str(i + 1)
                    if key in translated_data:
                        item["title_cn"] = translated_data[key]["title"]
                        item["summary_cn"] = translated_data[key].get("summary", item.get("description", ""))[:150]
                    else:
                        item["title_cn"] = item.get("title", "")
                        item["summary_cn"] = item.get("description", "")[:150]

            self.log(f"æ‰¹é‡ç¿»è¯‘å®Œæˆ: {len(items)}æ¡")
            return items

        except json.JSONDecodeError as e:
            self.log(f"JSONè§£æå¤±è´¥: {e}ï¼ŒLLMè¿”å›: {result[:200]}...", "WARNING")
            for item in items:
                item["title_cn"] = item.get("title", "")
                item["summary_cn"] = item.get("description", "")[:150]
            return items
        except Exception as e:
            self.log(f"æ‰¹é‡ç¿»è¯‘å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹å†…å®¹", "WARNING")
            for item in items:
                item["title_cn"] = item.get("title", "")
                item["summary_cn"] = item.get("description", "")[:150]
            return items

    def _generate_background_analysis(self, item: Dict) -> Dict:
        """ä¸ºå•æ¡æ–°é—»ç”ŸæˆèƒŒæ™¯åˆ†æå’Œè¡Œä¸šå½±å“"""
        if not self.llm:
            return {"background": "", "impact": "", "tags": []}

        title = item.get("title_cn", item.get("title", ""))
        summary = item.get("summary_cn", item.get("description", ""))

        prompt = f"""åŸºäºä»¥ä¸‹AIæ–°é—»ï¼Œç”ŸæˆèƒŒæ™¯åˆ†æå’Œè¡Œä¸šå½±å“ï¼š

ã€æ–°é—»ã€‘
æ ‡é¢˜: {title}
æ‘˜è¦: {summary}

è¯·ç”Ÿæˆï¼š
1. background (100-150å­—): èƒŒæ™¯ä»‹ç»ï¼Œå¸®åŠ©è¯»è€…ç†è§£ä¸Šä¸‹æ–‡
2. impact (100-150å­—): è¡Œä¸šå½±å“åˆ†æï¼Œè¯´æ˜ä¸ºä»€ä¹ˆé‡è¦
3. tags (3-5ä¸ªå…³é”®è¯): ç”¨äºåˆ†ç±»å’Œæ£€ç´¢

ç›´æ¥è¾“å‡ºJSONæ ¼å¼ï¼Œä¸è¦æœ‰ä»»ä½•é¢å¤–è¯´æ˜ã€‚"""

        try:
            response = self.llm.invoke([HumanMessage(content=prompt)])
            result = response.content.strip()

            if result.startswith('```'):
                result = result.split('```', 2)[1] if '```' in result[3:] else result
                result = result.strip()
                if result.startswith('json'):
                    result = result[4:].strip()
                if result.endswith('```'):
                    result = result[:-3].strip()

            analysis = json.loads(result)
            return {
                "background": analysis.get("background", ""),
                "impact": analysis.get("impact", ""),
                "tags": analysis.get("tags", [])
            }
        except Exception as e:
            self.log(f"èƒŒæ™¯åˆ†æç”Ÿæˆå¤±è´¥: {e}", "DEBUG")
            return {"background": "", "impact": "", "tags": []}

    def _extract_core_insights(self, items: List[Dict]) -> List[str]:
        """ä»æ‰€æœ‰æ–°é—»ä¸­æå–æ ¸å¿ƒæ´å¯Ÿï¼ˆv8.0 - é¡¶çº§ç§‘æŠ€åª’ä½“çº§åˆ«ï¼‰"""
        if not self.llm or not items:
            return []

        # é€‰æ‹©æœ€é‡è¦çš„10æ¡æ–°é—»
        top_items = sorted(items, key=lambda x: x.get("importance_score", 0), reverse=True)[:10]

        news_summary = "\n".join([
            f"- {item.get('title_cn', item.get('title', ''))}"
            for item in top_items
        ])

        # v8.0 - é¡¶çº§ç§‘æŠ€åª’ä½“çº§åˆ«çš„æ ¸å¿ƒæ´å¯Ÿæå–
        prompt = f"""ä½ æ˜¯TechCrunchã€36æ°ªã€è™å—…ç­‰ä¸–ç•Œé¡¶çº§ç§‘æŠ€åª’ä½“çš„æ€»ç¼–è¾‘ï¼Œæ‹¥æœ‰20å¹´AIè¡Œä¸šæ·±åº¦æŠ¥é“ç»éªŒã€‚

è¯·åŸºäºä»Šæ—¥AIæ–°é—»ï¼Œæå–3-5æ¡**å…·æœ‰å‰ç»æ€§çš„æ ¸å¿ƒæ´å¯Ÿ**ã€‚

ã€ä»Šæ—¥é‡è¦æ–°é—»ã€‘
{news_summary}

ã€æ´å¯Ÿè¦æ±‚ - é¡¶çº§ç§‘æŠ€åª’ä½“æ ‡å‡†ã€‘
1. **æ´å¯Ÿæ·±åº¦**: é€è¿‡è¡¨è±¡çœ‹åˆ°è¡Œä¸šæœ¬è´¨å˜åŒ–ï¼Œæ­ç¤ºæ·±å±‚è¶‹åŠ¿
2. **è§‚ç‚¹é²œæ˜**: é¿å…æ³›æ³›è€Œè°ˆï¼Œæ¯æ¡éƒ½è¦æœ‰ç‹¬ç‰¹è§‚ç‚¹
3. **å‰ç»æ€§**: èƒ½å¤Ÿé¢„è§6-12ä¸ªæœˆçš„è¡Œä¸šå‘å±•æ–¹å‘
4. **å¯å‘æ€§**: èƒ½å¼•å‘è¯»è€…æ€è€ƒå’Œè®¨è®º
5. **è¯­è¨€ç²¾ç‚¼**: æ¯æ¡30-50å­—ï¼Œä¿¡æ¯å¯†åº¦é«˜
6. **ä½¿ç”¨æœ‰åŠ›åŠ¨è¯**: "æ­ç¤ºäº†"ã€"æ ‡å¿—ç€"ã€"é¢„ç¤ºç€"ã€"é‡å¡‘äº†"ã€"é¢ è¦†äº†"
7. **é¿å…ç½—åˆ—**: ä¸åˆ—ä¸¾å…·ä½“å…¬å¸/äº§å“åç§°ï¼Œèšç„¦è¡Œä¸šè¶‹åŠ¿
8. **å¤šè§’åº¦**: ä»æŠ€æœ¯ã€å•†ä¸šã€ç”Ÿæ€ç­‰ä¸åŒç»´åº¦è§‚å¯Ÿ

ã€æ´å¯Ÿç¤ºä¾‹ã€‘ï¼ˆå‚è€ƒé£æ ¼ï¼‰:
- âŒ å·®: "ä»Šå¤©æœ‰å¾ˆå¤šAIæ–°é—»"
- âœ… å¥½: "å¤šæ™ºèƒ½ä½“åä½œèŒƒå¼ç¡®ç«‹ï¼Œæ ‡å¿—ç€AIä»å•ä¸€å¯¹è¯è¿ˆå‘è‡ªä¸»æ‰§è¡Œæ–°é˜¶æ®µ"
- âŒ å·®: "å¤§æ¨¡å‹æ€§èƒ½æŒç»­æå‡"
- âœ… å¥½: "å¼€æºæ¨¡å‹é€¼è¿‘é—­æºæ°´å¹³ï¼Œé‡å¡‘AIäº§ä¸šç«äº‰æ ¼å±€"

ç›´æ¥è¾“å‡ºJSONæ•°ç»„æ ¼å¼ï¼Œä¸è¦æœ‰ä»»ä½•é¢å¤–è¯´æ˜ã€‚"""

        try:
            response = self.llm.invoke([HumanMessage(content=prompt)])
            result = response.content.strip()

            if result.startswith('```'):
                result = result.split('```', 2)[1] if '```' in result[3:] else result
                result = result.strip()
                if result.startswith('json'):
                    result = result[4:].strip()
                if result.endswith('```'):
                    result = result[:-3].strip()

            insights = json.loads(result)
            return insights if isinstance(insights, list) else []
        except Exception as e:
            self.log(f"æ ¸å¿ƒæ´å¯Ÿæå–å¤±è´¥: {e}", "DEBUG")
            return []

    def _identify_trending_topics(self, items: List[Dict]) -> List[Dict]:
        """è¯†åˆ«çƒ­é—¨è¯é¢˜"""
        # ä»æ‰€æœ‰æ ‡ç­¾ä¸­ç»Ÿè®¡çƒ­é—¨è¯é¢˜
        all_tags = []
        for item in items:
            tags = item.get("tags", [])
            all_tags.extend(tags)

        # å¦‚æœæ²¡æœ‰æ ‡ç­¾ï¼Œä»æ ‡é¢˜ä¸­æå–å…³é”®è¯
        if not all_tags:
            for item in items:
                title = item.get("title_cn", item.get("title", ""))
                # ç®€å•çš„å…³é”®è¯æå–ï¼ˆå®é™…åº”è¯¥ç”¨æ›´å¤æ‚çš„NLPï¼‰
                keywords = ["GPT", "LLM", "RAG", "Agent", "Transformer", "AI", "å¤§æ¨¡å‹", "å¼€æº"]
                for kw in keywords:
                    if kw in title:
                        all_tags.append(kw)

        # ç»Ÿè®¡è¯é¢‘
        tag_counts = Counter(all_tags)

        # è½¬æ¢ä¸ºçƒ­é—¨è¯é¢˜åˆ—è¡¨
        trending = []
        for tag, count in tag_counts.most_common(10):
            if count >= 2:  # è‡³å°‘å‡ºç°2æ¬¡
                trending.append({
                    "name": tag,
                    "count": count,
                    "trend": "rising" if count >= 4 else "stable"
                })

        return trending[:5]  # è¿”å›Top 5

    def _generate_markdown_v7(
        self,
        scored_trends: Dict[str, Dict],
        editors_pick: List[Dict],
        core_insights: List[str],
        trending_topics: List[Dict],
        source_status: Dict[str, Any],
        today: datetime,
        issue_number: str,
        total_count: int
    ) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼ç®€æŠ¥ v8.0 - é¡¶çº§ç§‘æŠ€åª’ä½“é£æ ¼"""

        parts = []

        # ========== Header - å¢å¼ºç‰ˆ ==========
        parts.append(f"# AIæ¯æ—¥çƒ­ç‚¹ Â· {today.strftime('%Yå¹´%mæœˆ%dæ—¥')}\n\n")
        parts.append(f"> **æœŸå·**: #{issue_number}  |  **é˜…è¯»æ—¶é—´**: ~{max(3, total_count * 15 // 60)}åˆ†é’Ÿ  |  **æœ¬æœŸç²¾é€‰**: {total_count}æ¡å‰æ²¿åŠ¨æ€\n\n")
        parts.append("---\n\n")

        # ========== æ ¸å¿ƒæ´å¯Ÿ - ä¼˜åŒ–å±•ç¤º ==========
        if core_insights:
            parts.append("## ğŸ’¡ æ ¸å¿ƒæ´å¯Ÿ\n\n")
            parts.append("*åŸºäºä»Šæ—¥AIçƒ­ç‚¹ï¼Œæç‚¼è¡Œä¸šæœ¬è´¨å˜åŒ–ä¸æœªæ¥è¶‹åŠ¿*\n\n")
            for insight in core_insights:
                parts.append(f"- **{insight}**\n")
            parts.append("\n---\n\n")

        # ========== çƒ­é—¨è¯é¢˜ ==========
        if trending_topics:
            parts.append("## ğŸ“Š çƒ­é—¨è¯é¢˜\n\n")
            parts.append("| è¯é¢˜ | ç›¸å…³æ–°é—» | è¶‹åŠ¿ |\n")
            parts.append("|------|---------|------|\n")
            for topic in trending_topics:
                trend_icon = "ğŸ“ˆ ä¸Šå‡" if topic.get("trend") == "rising" else "â¡ï¸ ç¨³å®š"
                parts.append(f"| {topic['name']} | {topic['count']}æ¡ | {trend_icon} |\n")
            parts.append("\n---\n\n")

        # ========== åˆ†ç±»çƒ­ç‚¹ - ä¼˜åŒ–å±•ç¤ºæ ¼å¼ ==========
        parts.append("## ğŸ” åˆ†ç±»çƒ­ç‚¹\n\n")

        for cat_name, cat_data in scored_trends.items():
            items = cat_data.get("items", [])
            if not items:
                continue

            icon = cat_data.get("icon", "ğŸ“Œ")
            name = self._get_category_name(cat_name)
            parts.append(f"### {icon} {name} ({len(items)}æ¡)\n\n")

            for i, item in enumerate(items, 1):
                title = item.get("title_cn", item.get("title", ""))
                summary = item.get("summary_cn", item.get("description", ""))
                source = item.get("source", "")
                url = item.get("url", "")
                score = item.get("importance_score", 0)
                background = item.get("background", "")
                impact = item.get("impact", "")

                # ä¼˜åŒ–å±•ç¤ºæ ¼å¼
                parts.append(f"#### {i}. {title}\n\n")
                parts.append(f"> ğŸ“° **{source}**  |  â­ **è¯„åˆ†**: {int(score)}/100  |  ğŸ”— [åŸæ–‡é“¾æ¥]({url})\n\n")

                if summary:
                    parts.append(f"{summary}\n\n")

                if background:
                    parts.append(f"ğŸ“– **èƒŒæ™¯**: {background}\n\n")

                if impact:
                    parts.append(f"ğŸ¯ **å½±å“**: {impact}\n\n")

                parts.append("---\n\n")

        # ========== æ•°æ®æ¥æº ==========
        parts.append("## ğŸ“š æ•°æ®æ¥æº\n\n")
        success_sources = [s for s, status in source_status.items() if status.get("success", False)]
        for source in success_sources:
            count = source_status[source].get("count", 0)
            parts.append(f"- **{source}**: {count}æ¡\n")
        parts.append("\n---\n\n")

        # ========== Footer ==========
        parts.append("*ğŸ¤– Generated by [ContentForge AI](https://github.com/devfoxaicn/content-forge-ai)*\n")

        return "".join(parts)

    def _generate_json_v7(
        self,
        scored_trends: Dict[str, Dict],
        editors_pick: List[Dict],
        core_insights: List[str],
        trending_topics: List[Dict],
        source_status: Dict[str, Any],
        today: datetime,
        issue_number: str,
        total_count: int,
        markdown_content: str
    ) -> Dict[str, Any]:
        """ç”ŸæˆJSONæ ¼å¼æ•°æ® v7.0"""

        # æ„å»ºåˆ†ç±»æ•°æ®
        categories = []
        category_id_map = {
            "ğŸ“ˆ è¡Œä¸šåŠ¨æ€": ("industry", "äº§ä¸šåŠ¨æ€", "ğŸ“ˆ"),
            "ğŸ“ å­¦æœ¯å‰æ²¿": ("academic", "å­¦æœ¯å‰æ²¿", "ğŸ“"),
            "ğŸ”¬ æŠ€æœ¯åˆ›æ–°": ("tech", "æŠ€æœ¯åˆ›æ–°", "ğŸ”¬"),
            "ğŸ› ï¸ AIå·¥å…·/äº§å“": ("product", "äº§å“å·¥å…·", "ğŸ› ï¸"),
            "ğŸ’¼ AIåº”ç”¨": ("application", "è¡Œä¸šåº”ç”¨", "ğŸ’¼")
        }

        for cat_name, cat_data in scored_trends.items():
            cat_id, name, icon = category_id_map.get(cat_name, (cat_name, cat_name, "ğŸ“Œ"))
            items = []
            for item in cat_data.get("items", []):
                # ä½¿ç”¨hash()å‡½æ•°ç”Ÿæˆå”¯ä¸€ID
                url_hash = hash(item.get("url", "")) & 0xffffff
                items.append({
                    "id": f"{cat_id}_{url_hash:06x}",
                    "title": item.get("title", ""),
                    "title_cn": item.get("title_cn", ""),
                    "summary": item.get("description", "")[:200],
                    "summary_cn": item.get("summary_cn", "")[:200],
                    "url": item.get("url", ""),
                    "source": item.get("source", ""),
                    "category": name,
                    "importance_score": item.get("importance_score", 0),
                    "published_at": item.get("timestamp", ""),
                    "tags": item.get("tags", []),
                    "background": item.get("background", ""),
                    "impact": item.get("impact", "")
                })

            categories.append({
                "id": cat_id,
                "name": name,
                "icon": icon,
                "count": len(items),
                "items": items
            })

        # æ„å»ºç¼–è¾‘ç²¾é€‰
        editors_pick_data = []
        for item in editors_pick:
            editors_pick_data.append({
                "id": item.get("id", ""),
                "title": item.get("title", ""),
                "title_cn": item.get("title_cn", ""),
                "summary": item.get("description", "")[:200],
                "summary_cn": item.get("summary_cn", "")[:200],
                "url": item.get("url", ""),
                "source": item.get("source", ""),
                "category": self._get_category_name(item.get("category", "")),
                "importance_score": item.get("importance_score", 0),
                "published_at": item.get("timestamp", ""),
                "tags": item.get("tags", []),
                "background": item.get("background", ""),
                "impact": item.get("impact", ""),
                "pick_rank": item.get("pick_rank", 0)
            })

        # æ„å»ºæ•°æ®æ¥æº
        sources = []
        for source, status in source_status.items():
            if status.get("success", False):
                sources.append({
                    "name": source,
                    "count": status.get("count", 0)
                })

        return {
            "metadata": {
                "title": f"AIæ¯æ—¥çƒ­ç‚¹ Â· {today.strftime('%Yå¹´%mæœˆ%dæ—¥')}",
                "issue_number": issue_number,
                "publish_date": today.strftime("%Y-%m-%d"),
                "generated_at": today.isoformat(),
                "word_count": len(markdown_content),
                "reading_time": f"{total_count * 15 // 60}åˆ†é’Ÿ",
                "total_items": total_count,
                "version": "v7.0"
            },
            "editors_pick": editors_pick_data,
            "categories": categories,
            "core_insights": core_insights,
            "trending_topics": trending_topics,
            "sources": sources,
            "markdown_content": markdown_content
        }

    def _get_category_name(self, key: str) -> str:
        """è·å–åˆ†ç±»ä¸­æ–‡å"""
        mapping = {
            "ğŸ“ˆ è¡Œä¸šåŠ¨æ€": "äº§ä¸šåŠ¨æ€",
            "ğŸ“ å­¦æœ¯å‰æ²¿": "å­¦æœ¯å‰æ²¿",
            "ğŸ”¬ æŠ€æœ¯åˆ›æ–°": "æŠ€æœ¯åˆ›æ–°",
            "ğŸ› ï¸ AIå·¥å…·/äº§å“": "äº§å“å·¥å…·",
            "ğŸ’¼ AIåº”ç”¨": "è¡Œä¸šåº”ç”¨"
        }
        return mapping.get(key, key)
