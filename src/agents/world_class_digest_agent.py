"""
World Class AI News Digest Generator
é¡¶çº§AIæ–°é—»ç®€æŠ¥ç”Ÿæˆå™¨ - å‚è€ƒThe Verge/TechCrunch/Wiredè®¾è®¡é£æ ¼
"""

from datetime import datetime
from typing import Dict, Any, List, Tuple
from loguru import logger
import json
from langchain_openai import ChatOpenAI


class WorldClassDigestAgent:
    """ä¸–ç•Œé¡¶çº§AIæ–°é—»ç®€æŠ¥ç”Ÿæˆå™¨ v6.0 - å…¨ä¸­æ–‡LLMç”Ÿæˆ"""

    def __init__(self, config: Dict[str, Any], prompts: Dict[str, Any]):
        self.config = config
        self.prompts = prompts
        self.name = "world_class_digest"
        self.llm = self._init_llm()

        # ç¿»è¯‘é…ç½®ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
        agent_config = config.get("agents", {}).get("world_class_digest", {})
        self.translate_enabled = agent_config.get("translate_enabled", True)
        self.batch_size = agent_config.get("batch_size", 5)  # æ‰¹é‡å¤„ç†å¤§å°
        self.max_items_per_category = agent_config.get("max_items_per_category", 15)  # æ¯ä¸ªåˆ†ç±»æœ€å¤šæ˜¾ç¤ºæ•°é‡

        self.log(f"v6.0åˆå§‹åŒ–å®Œæˆï¼Œç¿»è¯‘åŠŸèƒ½: {'å¯ç”¨' if self.translate_enabled else 'ç¦ç”¨'}")

    def _init_llm(self):
        """åˆå§‹åŒ–LLMç”¨äºç¿»è¯‘"""
        try:
            import os
            from pathlib import Path
            from dotenv import load_dotenv

            # æ˜¾å¼åŠ è½½.envæ–‡ä»¶ï¼ˆä»é¡¹ç›®æ ¹ç›®å½•ï¼‰
            project_root = Path(__file__).parent.parent.parent
            env_file = project_root / ".env"
            if env_file.exists():
                load_dotenv(env_file)
                self.log(f"å·²åŠ è½½ç¯å¢ƒå˜é‡æ–‡ä»¶: {env_file}")

            llm_config = self.config.get("llm", {})
            provider = llm_config.get("provider", "zhipuai")

            if provider == "zhipuai":
                # ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è·å–API key
                api_key = os.getenv("ZHIPUAI_API_KEY")
                if not api_key:
                    # å°è¯•ä»configçš„api_keysè·å–
                    api_key = self.config.get("api_keys", {}).get("zhipuai")

                if not api_key:
                    self.log("æœªé…ç½®ZHIPUAI_API_KEYï¼Œç¿»è¯‘åŠŸèƒ½å°†ä¸å¯ç”¨", "WARNING")
                    return None

                zhipu_config = llm_config.get("zhipuai", {})
                return ChatOpenAI(
                    model=zhipu_config.get("model", "glm-4-flash"),
                    openai_api_key=api_key,
                    base_url=zhipu_config.get("base_url", "https://open.bigmodel.cn/api/coding/paas/v4/"),
                    temperature=zhipu_config.get("temperature", 0.7),
                    max_tokens=zhipu_config.get("max_tokens", 8000),
                    timeout=zhipu_config.get("timeout", 600)
                )
            else:
                return None
        except Exception as e:
            self.log(f"LLMåˆå§‹åŒ–å¤±è´¥: {e}", "WARNING")
            return None

    def log(self, message: str, level: str = "INFO"):
        logger.log(level, f"[WorldClassDigestAgent] {message}")

    def execute(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œç®€æŠ¥ç”Ÿæˆ"""
        try:
            self.log("å¼€å§‹ç”Ÿæˆä¸–ç•Œé¡¶çº§AIæ–°é—»ç®€æŠ¥...")

            categorized_trends = state.get("categorized_trends", {})
            source_status = state.get("source_status", {})

            # ç”Ÿæˆç®€æŠ¥
            digest = self._generate_world_class_digest(
                categorized_trends,
                source_status
            )

            return {
                **state,
                "news_digest": digest
            }

        except Exception as e:
            self.log(f"ç®€æŠ¥ç”Ÿæˆå¤±è´¥: {e}", "ERROR")
            return {
                **state,
                "error_message": str(e),
                "current_step": "world_class_digest_failed"
            }

    def _generate_world_class_digest(
        self,
        categorized_trends: Dict[str, Dict],
        source_status: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ç”Ÿæˆä¸–ç•Œé¡¶çº§AIæ–°é—»ç®€æŠ¥"""

        today = datetime.now()
        issue_number = today.strftime("%Y%m%d")

        # è®¡ç®—æ€»æ•°ï¼ˆä¿®å¤ï¼šä»itemsä¸­è·å–ï¼‰
        total_count = 0
        for category_data in categorized_trends.values():
            if isinstance(category_data, dict) and "items" in category_data:
                total_count += len(category_data["items"])
            elif isinstance(category_data, list):
                total_count += len(category_data)

        self.log(f"ç”Ÿæˆç®€æŠ¥: {total_count}æ¡çƒ­ç‚¹")

        # ç”Ÿæˆä¸­æ–‡ç®€æŠ¥
        chinese_content = self._generate_chinese_digest_v2(
            categorized_trends,
            source_status,
            today,
            issue_number,
            total_count
        )

        word_count = len(chinese_content)

        return {
            "title": f"AI Daily Â· {today.strftime('%Yå¹´%mæœˆ%dæ—¥')}",
            "subtitle": f"ä»Šæ—¥{total_count}æ¡å‰æ²¿åŠ¨æ€",
            "issue_number": issue_number,
            "publish_date": today.strftime("%Y-%m-%d"),
            "full_content": chinese_content,
            "word_count": word_count,
            "reading_time": f"{word_count // 500}åˆ†é’Ÿ",
            "total_topics": total_count,
            "source_status": source_status,
            "version": "v5.0"
        }

    def _generate_chinese_digest_v2(
        self,
        categorized_trends: Dict[str, Dict],
        source_status: Dict[str, Any],
        today: datetime,
        issue_number: str,
        total_count: int
    ) -> str:
        """ç”Ÿæˆä¸­æ–‡ç®€æŠ¥ v6.0 - é¡¶çº§ç§‘æŠ€åª’ä½“é£æ ¼ï¼Œå…¨ä¸­æ–‡LLMç”Ÿæˆ"""

        parts = []

        # ========== Header ==========
        parts.append("# " + "â”" * 50 + "\n")
        parts.append(f"# ğŸ”¥ AI Daily Â· {today.strftime('%Yå¹´%mæœˆ%dæ—¥')}\n")
        parts.append("# " + "â”" * 50 + "\n\n")
        parts.append(f"### ğŸŒ æ±‡èšå…¨çƒé¡¶å°–AIèµ„è®¯  |  ğŸ“Š ä»Šæ—¥ **{total_count}** æ¡æ›´æ–°\n\n")
        parts.append(f"**ğŸ“… {today.strftime('%Yå¹´%mæœˆ%dæ—¥')}**  Â·  **Issue #{issue_number}**\n\n")
        parts.append("---\n\n")

        # ========== æ ¸å¿ƒå¤´æ¡ ==========
        parts.append("## â­ æ ¸å¿ƒå¤´æ¡\n\n")

        # è·å–æœ€é«˜çƒ­åº¦çš„æ–°é—»ï¼ˆè·¨æ‰€æœ‰åˆ†ç±»ï¼‰
        top_stories = self._get_top_stories(categorized_trends, limit=5)

        # æ‰¹é‡ç”Ÿæˆæ ¸å¿ƒå¤´æ¡çš„ä¸­æ–‡æ‘˜è¦
        if self.translate_enabled and self.llm:
            top_stories = self._batch_generate_summaries(top_stories)

        for i, story in enumerate(top_stories, 1):
            title_cn = story.get("title_cn", story.get("title", ""))
            summary_cn = story.get("summary_cn", story.get("description", ""))[:150]
            source = story.get("source", "").split("(")[0].strip()
            url = story.get("url", "")

            parts.append(f"### {i}. {title_cn}\n\n")
            parts.append(f"> ğŸ“° {source}  |  ğŸ”— [åŸæ–‡é“¾æ¥]({url})\n\n")
            parts.append(f"{summary_cn}\n\n")
            parts.append("---\n\n")

        # ========== åˆ†ç±»èµ„è®¯ ==========
        parts.append("## ğŸ“‚ åˆ†ç±»èµ„è®¯\n\n")

        category_icons = {
            "ğŸ“ˆ äº§ä¸šåŠ¨æ€": "ğŸ’¼",
            "ğŸ“ å­¦æœ¯å‰æ²¿": "ğŸ”¬",
            "ğŸ”§ æŠ€æœ¯åˆ›æ–°": "âš¡",
            "ğŸ› ï¸ AIå·¥å…·/äº§å“": "ğŸš€",
            "ğŸ’¼ AIåº”ç”¨": "ğŸŒ"
        }

        for category_key, category_data in categorized_trends.items():
            # è·å–å®é™…çš„æ¡ç›®åˆ—è¡¨
            if isinstance(category_data, dict) and "items" in category_data:
                trends = category_data["items"]
            else:
                trends = category_data

            if not trends or not isinstance(trends, list):
                continue

            icon = category_icons.get(category_key, "ğŸ“Œ")
            category_name = self._get_category_name(category_key)

            # é™åˆ¶æ¯ä¸ªåˆ†ç±»æ˜¾ç¤ºçš„æ•°é‡
            display_trends = trends[:self.max_items_per_category]
            parts.append(f"### {icon} {category_name} ({len(display_trends)}æ¡ï¼Œå…±{len(trends)}æ¡)\n\n")

            # æ‰¹é‡ç”Ÿæˆå½“å‰åˆ†ç±»çš„ä¸­æ–‡æ‘˜è¦
            if self.translate_enabled and self.llm:
                display_trends = self._batch_generate_summaries(display_trends)

            for i, trend in enumerate(display_trends, 1):
                title_cn = trend.get("title_cn", trend.get("title", ""))
                summary_cn = trend.get("summary_cn", trend.get("description", ""))[:150]
                source = trend.get("source", "").split("(")[0].strip()
                url = trend.get("url", "")

                parts.append(f"**{i}. {title_cn}**\n\n")
                parts.append(f"{summary_cn}\n\n")
                parts.append(f"ğŸ“ {source} | [é˜…è¯»æ›´å¤š]({url})\n\n")

        # ========== æ•°æ®æ¥æº ==========
        parts.append("---\n\n")
        parts.append("## ğŸ“Š æ•°æ®æ¥æº\n\n")

        success_sources = [s for s, status in source_status.items() if status.get("success", False)]
        total_sources = len(source_status)

        parts.append(f"**æ•°æ®è·å–æˆåŠŸç‡**: {len(success_sources)}/{total_sources} ({len(success_sources)*100//total_sources}%)\n\n")

        if success_sources:
            parts.append("**âœ… æˆåŠŸè·å–çš„æ•°æ®æº:**\n\n")
            for source in success_sources:
                count = source_status[source].get("count", 0)
                parts.append(f"- **{source}**: {count}æ¡\n")
            parts.append("\n")

        failed_sources = [s for s, status in source_status.items() if not status.get("success", False)]
        if failed_sources:
            parts.append("**âŒ æš‚æ—¶ä¸å¯ç”¨çš„æ•°æ®æº:**\n\n")
            for source in failed_sources:
                parts.append(f"- **{source}**: {source_status[source].get('message', 'æœªçŸ¥é”™è¯¯')}\n")
            parts.append("\n")

        # ========== Footer ==========
        parts.append("---\n\n")
        parts.append("<div align='center'>\n\n")
        parts.append("### ğŸ¤– ç”± ContentForge AI è‡ªåŠ¨ç”Ÿæˆ\n\n")
        parts.append(f"{today.strftime('%Yå¹´%mæœˆ%dæ—¥')}\n\n")
        parts.append("**æ•°æ®æ¥æº**: TechCrunch AI Â· MIT Technology Review Â· OpenAI Blog Â· NewsAPI Â· arXiv Â· Hacker News\n\n")
        parts.append("</div>\n")

        return "".join(parts)

    def _get_top_stories(self, categorized_trends: Dict[str, Dict], limit: int = 5) -> List[Dict]:
        """è·å–æœ€çƒ­é—¨çš„æ–°é—»"""
        all_trends = []
        for category, category_data in categorized_trends.items():
            if isinstance(category_data, dict) and "items" in category_data:
                all_trends.extend(category_data["items"])
            else:
                # å…¼å®¹æ—§æ ¼å¼
                all_trends.extend(category_data)

        # æŒ‰çƒ­åº¦åˆ†æ•°æ’åº
        all_trends.sort(key=lambda x: x.get("heat_score", 0), reverse=True)
        return all_trends[:limit]

    def _batch_generate_summaries(self, trends: List[Dict]) -> List[Dict]:
        """æ‰¹é‡ç”Ÿæˆæ–°é—»çš„ä¸­æ–‡æ‘˜è¦

        Args:
            trends: æ–°é—»åˆ—è¡¨ï¼Œæ¯æ¡åŒ…å« title, description, url, source

        Returns:
            å¤„ç†åçš„æ–°é—»åˆ—è¡¨ï¼ŒåŒ…å«ä¸­æ–‡æ ‡é¢˜å’Œæ‘˜è¦
        """
        if not trends or not self.translate_enabled:
            return trends

        # å¦‚æœå·²ç»æ˜¯ä¸­æ–‡å†…å®¹ï¼Œç›´æ¥è¿”å›
        first_item = trends[0] if trends else {}
        if first_item.get("title") and any('\u4e00' <= c <= '\u9fff' for c in first_item["title"]):
            return trends

        # æ„å»ºæ‰¹é‡ç¿»è¯‘æç¤º
        news_items = []
        for i, item in enumerate(trends):
            title = item.get("title", "").replace('&amp;', '&').replace('&quot;', '"')
            desc = item.get("description", "").replace('&amp;', '&').replace('&quot;', '"')
            desc = desc.replace('<p>', '').replace('</p>', '').replace('<br>', ' ')[:200]  # é™åˆ¶é•¿åº¦
            news_items.append(f"{i+1}. æ ‡é¢˜: {title}\n   æ‘˜è¦: {desc}")

        prompt = f"""ä½ æ˜¯ä¸€ä½é¡¶çº§ç§‘æŠ€åª’ä½“ç¼–è¾‘ï¼ˆå¦‚TechCrunchã€The Vergeï¼‰ã€‚è¯·å°†ä»¥ä¸‹AIæ–°é—»ç¿»è¯‘å¹¶ç²¾ç®€æˆä¸“ä¸šçš„ä¸­æ–‡ç®€æŠ¥ã€‚

è¦æ±‚ï¼š
1. æ ‡é¢˜ç¿»è¯‘è¦ç®€æ´æœ‰åŠ›ï¼Œç¬¦åˆç§‘æŠ€åª’ä½“é£æ ¼
2. æ‘˜è¦è¦ç²¾ç‚¼ï¼Œæ§åˆ¶åœ¨50å­—ä»¥å†…ï¼Œçªå‡ºæ ¸å¿ƒä¿¡æ¯
3. ä¿æŒä¸“ä¸šæœ¯è¯­å‡†ç¡®æ€§ï¼ˆå¦‚LLMã€RAGã€Transformerç­‰ï¼‰
4. ç›´æ¥è¾“å‡ºJSONæ ¼å¼ï¼Œä¸è¦æœ‰ä»»ä½•é¢å¤–è¯´æ˜

è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
{{
  "1": {{"title": "ä¸­æ–‡æ ‡é¢˜", "summary": "ä¸­æ–‡æ‘˜è¦"}},
  "2": {{"title": "ä¸­æ–‡æ ‡é¢˜", "summary": "ä¸­æ–‡æ‘˜è¦"}}
}}

å¾…å¤„ç†çš„æ–°é—»ï¼š
{chr(10).join(news_items)}

è¯·ç›´æ¥è¾“å‡ºJSONï¼š"""

        try:
            from langchain_core.messages import HumanMessage
            response = self.llm.invoke([HumanMessage(content=prompt)])
            result = response.content.strip()

            # æ¸…ç†å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
            result = result.strip()
            if result.startswith('```'):
                result = result.split('```', 2)[1] if '```' in result[3:] else result
                result = result.strip()
                if result.startswith('json'):
                    result = result[4:].strip()
                if result.endswith('```'):
                    result = result[:-3].strip()

            # è§£æJSON
            import json
            translated_data = json.loads(result)

            # æ›´æ–°åŸå§‹æ•°æ®
            for i, item in enumerate(trends):
                key = str(i + 1)
                if key in translated_data:
                    item["title_cn"] = translated_data[key]["title"]
                    item["summary_cn"] = translated_data[key]["summary"]
                else:
                    # è§£æå¤±è´¥ï¼Œä½¿ç”¨åŸæ•°æ®
                    item["title_cn"] = item.get("title", "")
                    item["summary_cn"] = item.get("description", "")[:150] + "..."

            self.log(f"æ‰¹é‡ç”Ÿæˆæ‘˜è¦å®Œæˆ: {len(trends)}æ¡")
            return trends

        except Exception as e:
            self.log(f"æ‰¹é‡ç”Ÿæˆæ‘˜è¦å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹å†…å®¹", "WARNING")
            # é™çº§å¤„ç†ï¼šç›´æ¥ä½¿ç”¨åŸå§‹å†…å®¹
            for item in trends:
                item["title_cn"] = item.get("title", "")
                item["summary_cn"] = item.get("description", "")[:150] + "..." if len(item.get("description", "")) > 150 else item.get("description", "")
            return trends

    def _get_category_name(self, key: str) -> str:
        """è·å–åˆ†ç±»ä¸­æ–‡å"""
        mapping = {
            "ğŸ“ˆ äº§ä¸šåŠ¨æ€": "äº§ä¸šåŠ¨æ€",
            "ğŸ“ å­¦æœ¯å‰æ²¿": "å­¦æœ¯å‰æ²¿",
            "ğŸ”§ æŠ€æœ¯åˆ›æ–°": "æŠ€æœ¯åˆ›æ–°",
            "ğŸ› ï¸ AIå·¥å…·/äº§å“": "äº§å“å·¥å…·",
            "ğŸ’¼ AIåº”ç”¨": "è¡Œä¸šåº”ç”¨"
        }
        return mapping.get(key, key)
