"""
ä¸–ç•Œé¡¶çº§AIæ–°é—»ç®€æŠ¥Agent - å…¨ä¸­æ–‡ç‰ˆ
æŒ‰ç…§ä¸–ç•Œä¸€æµç§‘æŠ€åª’ä½“æ ‡å‡†ï¼Œç”Ÿæˆä¸“ä¸šçš„AIæ–°é—»ç®€æŠ¥
"""

from typing import Dict, Any, List
import yaml
from pathlib import Path
from datetime import datetime
from src.agents.base import BaseAgent


class WorldClassDigestAgent(BaseAgent):
    """
    ä¸–ç•Œé¡¶çº§AIæ–°é—»ç®€æŠ¥Agent
    å‚ç…§36æ°ªã€è™å—…ã€å“ç©ç­‰ä¸“ä¸šç§‘æŠ€åª’ä½“çš„ä¸­æ–‡å†™ä½œæ ‡å‡†
    """

    def __init__(self, config: Dict[str, Any], prompts: Dict[str, Any]):
        super().__init__(config, prompts)

        digest_config = config.get("agents", {}).get("trends_digest", {})
        self.include_analysis = digest_config.get("include_analysis", True)
        self.llm.temperature = 0.8  # æ›´é«˜æ¸©åº¦ï¼Œå¢å¼ºä¸­æ–‡è¡¨è¾¾çš„ä¸°å¯Œæ€§

        # 5å¤§åˆ†ç±»å®šä¹‰
        self.CATEGORIES = {
            "industry": {
                "name": "äº§ä¸šåŠ¨æ€",
                "icon": "ğŸ“ˆ",
                "description": "è¿½è¸ªAIäº§ä¸šèµ„æœ¬æµå‘ã€å·¨å¤´æˆ˜ç•¥å¸ƒå±€ã€åˆåˆ›ä¼ä¸šèèµ„ï¼Œç¬¬ä¸€æ—¶é—´æŒæ¡å…¨çƒå•†ä¸šæ ¸å¿ƒåŠ¨æ€"
            },
            "academic": {
                "name": "å­¦æœ¯å‰æ²¿",
                "icon": "ğŸ“",
                "description": "ç²¾é€‰é¡¶çº§æœŸåˆŠè®ºæ–‡ã€å‰æ²¿ç ”ç©¶æˆæœï¼Œè§£è¯»å­¦æœ¯ç•Œæœ€æ–°çªç ´ä¸ç†è®ºåˆ›æ–°"
            },
            "tech": {
                "name": "æŠ€æœ¯åˆ›æ–°",
                "icon": "ğŸ”¬",
                "description": "æ·±åº¦è§£ææ¨¡å‹æ¶æ„ã€ç®—æ³•çªç ´ã€å·¥ç¨‹åˆ›æ–°ï¼Œè¿½è¸ªAIæŠ€æœ¯åº•å±‚æ¼”è¿›"
            },
            "product": {
                "name": "äº§å“å·¥å…·",
                "icon": "ğŸ› ï¸",
                "description": "å‘ç°æå‡å¼€å‘æ•ˆç‡çš„å®ç”¨å·¥å…·ã€é¢ è¦†æ€§çš„AIäº§å“ï¼Œè¯„æµ‹æœ€æ–°åº”ç”¨ä½“éªŒ"
            },
            "application": {
                "name": "è¡Œä¸šåº”ç”¨",
                "icon": "ğŸ’¼",
                "description": "å±•ç¤ºAIåœ¨å„è¡Œä¸šçš„åˆ›æ–°åº”ç”¨æ¡ˆä¾‹ï¼Œåˆ†æè½åœ°å®è·µä¸å•†ä¸šä»·å€¼"
            }
        }

        self.log("ä¸–ç•Œé¡¶çº§ä¸­æ–‡ç®€æŠ¥æ ‡å‡†å·²åŠ è½½ - ä¸“ä¸šç§‘æŠ€åª’ä½“é£æ ¼")

    def execute(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆä¸–ç•Œé¡¶çº§AIæ–°é—»ç®€æŠ¥"""
        self.log("å¼€å§‹ç”Ÿæˆä¸–ç•Œé¡¶çº§AIæ–°é—»ç®€æŠ¥ï¼ˆå…¨ä¸­æ–‡ç‰ˆï¼‰")

        try:
            categorized_trends = state.get("categorized_trends")
            if not categorized_trends:
                self.log("æœªæ‰¾åˆ° categorized_trendsï¼Œæ— æ³•ç”Ÿæˆç®€æŠ¥")
                return {
                    **state,
                    "error_message": "æœªæ‰¾åˆ°åˆ†ç±»åçš„çƒ­ç‚¹æ•°æ®",
                    "current_step": "digest_failed"
                }

            total_count = state.get("total_trends_count", 0)
            self.log(f"å¼€å§‹å¤„ç† {total_count} æ¡AIèµ„è®¯")

            # ç”Ÿæˆå®Œæ•´ç®€æŠ¥
            digest = self._generate_world_class_digest(categorized_trends, total_count)

            self.log(f"æˆåŠŸç”Ÿæˆä¸–ç•Œé¡¶çº§AIæ–°é—»ç®€æŠ¥ï¼Œå…± {total_count} æ¡èµ„è®¯")

            return {
                **state,
                "news_digest": digest,
                "current_step": "digest_completed"
            }
        except Exception as e:
            self.log(f"ç®€æŠ¥ç”Ÿæˆå¤±è´¥: {str(e)}", "ERROR")
            return {
                **state,
                "error_message": f"ç®€æŠ¥ç”Ÿæˆå¤±è´¥: {str(e)}",
                "current_step": "digest_failed"
            }

    def _generate_world_class_digest(
        self,
        categorized_trends: Dict[str, Dict],
        total_count: int
    ) -> Dict[str, Any]:
        """ç”Ÿæˆä¸–ç•Œé¡¶çº§ç®€æŠ¥å†…å®¹"""

        today = datetime.now()
        issue_number = today.strftime("%Y%m%d")

        # 1. ç”Ÿæˆæ ¸å¿ƒæ´å¯Ÿ
        key_insights = self._generate_key_insights(categorized_trends)

        # 2. ç”Ÿæˆæ·±åº¦è§‚å¯Ÿ
        deep_analysis = self._generate_deep_analysis(categorized_trends)

        # 3. ç”Ÿæˆæ¯ä¸ªåˆ†ç±»çš„å¯¼è¯­
        category_intros = self._generate_category_intros(categorized_trends)

        # 4. ç¿»è¯‘å¹¶æ•´ç†æ‰€æœ‰çƒ­ç‚¹å†…å®¹
        translated_items = self._translate_and_format_items(categorized_trends)

        # 5. ç»„è£…å®Œæ•´ç®€æŠ¥
        full_content = self._assemble_full_content(
            today, issue_number, total_count,
            key_insights, deep_analysis,
            category_intros, translated_items
        )

        word_count = len(full_content)

        return {
            "title": f"AIæ¯æ—¥çƒ­ç‚¹ Â· {today.strftime('%Yå¹´%mæœˆ%dæ—¥')}",
            "subtitle": f"æ±‡èšå…¨çƒ8å¤§AIèµ„è®¯æºï¼Œç²¾é€‰{total_count}æ¡å‰æ²¿åŠ¨æ€",
            "issue_number": issue_number,
            "publish_date": today.strftime("%Y-%m-%d"),
            "full_content": full_content,
            "key_insights": key_insights,
            "deep_analysis": deep_analysis,
            "category_intros": category_intros,
            "word_count": word_count,
            "reading_time": f"{word_count // 400}-{word_count // 250}åˆ†é’Ÿ",
            "total_topics": total_count,
            "version": "v4.0"
        }

    def _generate_key_insights(self, categorized_trends: Dict[str, Dict]) -> List[str]:
        """ç”Ÿæˆæ ¸å¿ƒæ´å¯Ÿï¼ˆä¸­æ–‡ï¼‰"""
        try:
            # æå–æ‰€æœ‰é«˜çƒ­åº¦çƒ­ç‚¹
            all_items = []
            for category_data in categorized_trends.values():
                items = category_data.get("items", [])
                all_items.extend(items)

            # æŒ‰çƒ­åº¦æ’åº
            all_items.sort(key=lambda x: x.get("heat_score", 0), reverse=True)
            top_items = all_items[:15]

            # æ„å»ºæ ‡é¢˜åˆ—è¡¨
            titles_text = "\n".join([
                f"{i+1}. {item.get('title', '')}"
                for i, item in enumerate(top_items)
            ])

            prompt = f"""ä½ æ˜¯ä¸€ä½ä¸–ç•Œé¡¶çº§ç§‘æŠ€åª’ä½“çš„æ€»ç¼–è¾‘ã€‚è¯·ä»ä»¥ä¸‹AIçƒ­ç‚¹ä¸­æå–3-5ä¸ªæ ¸å¿ƒæ´å¯Ÿï¼ˆæ¯æ¡25-35å­—ï¼‰ã€‚

çƒ­ç‚¹åˆ—è¡¨ï¼š
{titles_text}

è¦æ±‚ï¼š
1. æ´å¯Ÿæ·±åˆ»ï¼Œé€è¿‡ç°è±¡çœ‹æœ¬è´¨
2. è§‚ç‚¹é²œæ˜ï¼Œä¸€é’ˆè§è¡€
3. è¯­è¨€ç²¾ç‚¼ï¼Œä¸“ä¸šè¡¨è¾¾
4. ä½¿ç”¨"æ ‡å¿—ç€"ã€"æ­ç¤ºäº†"ã€"åæ˜ å‡º"ç­‰åˆ¤æ–­æ€§åŠ¨è¯
5. å…¨ä¸­æ–‡è¡¨è¿°

æ ¸å¿ƒæ´å¯Ÿï¼ˆæ¯æ¡ä¸€è¡Œï¼‰ï¼š"""

            response = self._call_llm(prompt)
            insights = [line.strip() for line in response.strip().split('\n') if line.strip()]
            return insights[:5]
        except Exception as e:
            self.log(f"æ ¸å¿ƒæ´å¯Ÿç”Ÿæˆå¤±è´¥: {e}", "WARNING")
            return []

    def _generate_deep_analysis(self, categorized_trends: Dict[str, Dict]) -> str:
        """ç”Ÿæˆæ·±åº¦è§‚å¯Ÿï¼ˆä¸­æ–‡ï¼‰"""
        try:
            # ç»Ÿè®¡å„åˆ†ç±»æ•°é‡
            category_counts = {}
            all_titles = {}
            for key, cat_data in categorized_trends.items():
                count = cat_data.get("count", 0)
                if count > 0:
                    cat_name = self._get_category_name(key)
                    category_counts[cat_name] = count
                    items = cat_data.get("items", [])[:8]
                    all_titles[cat_name] = [item.get("title", "") for item in items]

            # æ„å»ºè¾“å…¥æ–‡æœ¬
            input_text = "æœ¬æœŸçƒ­ç‚¹åˆ†ç±»ç»Ÿè®¡ï¼š\n"
            for cat, count in category_counts.items():
                input_text += f"- {cat}: {count}æ¡\n"

            input_text += "\nå„åˆ†ç±»ä»£è¡¨æ€§çƒ­ç‚¹ï¼š\n"
            for cat, titles in all_titles.items():
                input_text += f"\nã€{cat}ã€‘\n"
                for title in titles[:5]:
                    input_text += f"  â€¢ {title}\n"

            prompt = f"""ä½ æ˜¯ä¸€ä½ä¸–ç•Œé¡¶çº§ç§‘æŠ€åª’ä½“çš„èµ„æ·±åˆ†æå¸ˆã€‚è¯·åŸºäºä»¥ä¸‹AIçƒ­ç‚¹ï¼Œæ’°å†™ä¸€ç¯‡æ·±åº¦è§‚å¯Ÿæ–‡ç« ï¼ˆ350-450å­—ï¼‰ã€‚

{input_text}

å†™ä½œè¦æ±‚ï¼š
1. ç«‹æ„é«˜è¿œï¼šä»äº§ä¸šæ ¼å±€ã€æŠ€æœ¯æ¼”è¿›ã€å•†ä¸šä»·å€¼ç­‰å®è§‚è§†è§’åˆ‡å…¥
2. é€»è¾‘æ¸…æ™°ï¼šç°è±¡æè¿° â†’ åŸå› åˆ†æ â†’ å½±å“åˆ¤æ–­ â†’ è¶‹åŠ¿å±•æœ›
3. æ•°æ®æ”¯æ’‘ï¼šå¼•ç”¨å…·ä½“åˆ†ç±»æ•°é‡å’Œä»£è¡¨æ€§çƒ­ç‚¹
4. è§‚ç‚¹é²œæ˜ï¼šæå‡ºæœ‰æ·±åº¦çš„åˆ¤æ–­å’Œé¢„æµ‹
5. è¯­è¨€ä¸“ä¸šï¼šä½¿ç”¨"åº•å±‚é€»è¾‘"ã€"èŒƒå¼è½¬ç§»"ã€"ç”Ÿæ€é‡æ„"ç­‰ä¸“ä¸šè¡¨è¾¾
6. å…¨ä¸­æ–‡å†™ä½œ

æ·±åº¦è§‚å¯Ÿæ–‡ç« ï¼š"""

            response = self._call_llm(prompt)
            return response.strip()
        except Exception as e:
            self.log(f"æ·±åº¦è§‚å¯Ÿç”Ÿæˆå¤±è´¥: {e}", "WARNING")
            return ""

    def _generate_category_intros(self, categorized_trends: Dict[str, Dict]) -> Dict[str, str]:
        """ç”Ÿæˆå„åˆ†ç±»å¯¼è¯­ï¼ˆä¸­æ–‡ï¼‰"""
        intros = {}

        for key, cat_data in categorized_trends.items():
            items = cat_data.get("items", [])
            count = cat_data.get("count", 0)

            if count == 0:
                continue

            cat_name = self._get_category_name(key)
            cat_desc = self.CATEGORIES.get(self._get_key_by_name(cat_name), {}).get("description", "")

            # è·å–å‰5ä¸ªæ ‡é¢˜
            titles = [item.get("title", "") for item in items[:5]]
            titles_text = "\n".join([f"  â€¢ {title}" for title in titles])

            prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šç§‘æŠ€åª’ä½“çš„ç‰ˆå—ä¸»ç¼–ã€‚è¯·ä¸º"{cat_name}"åˆ†ç±»æ’°å†™ä¸€æ®µç²¾å½©å¯¼è¯­ï¼ˆ50-70å­—ï¼‰ã€‚

åˆ†ç±»å®šä½ï¼š{cat_desc}

æœ¬åˆ†ç±»ç²¾é€‰çƒ­ç‚¹ï¼š
{titles_text}

è¦æ±‚ï¼š
1. çªå‡ºåˆ†ç±»ä»·å€¼
2. è¯­è¨€ç”ŸåŠ¨æœ‰åŠ›
3. å¸å¼•è¯»è€…ç»§ç»­é˜…è¯»
4. å…¨ä¸­æ–‡è¡¨è¾¾

å¯¼è¯­ï¼š"""

            try:
                response = self._call_llm(prompt)
                intros[cat_name] = response.strip()
            except Exception as e:
                self.log(f"{cat_name}å¯¼è¯­ç”Ÿæˆå¤±è´¥: {e}", "WARNING")
                intros[cat_name] = cat_desc

        return intros

    def _translate_and_format_items(self, categorized_trends: Dict[str, Dict]) -> Dict[str, List[Dict]]:
        """ç¿»è¯‘å¹¶æ ¼å¼åŒ–æ‰€æœ‰çƒ­ç‚¹æ¡ç›®ï¼ˆå…¨ä¸­æ–‡ï¼‰"""
        result = {}

        for key, cat_data in categorized_trends.items():
            items = cat_data.get("items", [])
            cat_name = self._get_category_name(key)

            formatted_items = []
            for item in items:
                # ç¿»è¯‘æ ‡é¢˜å’Œæè¿°
                translated = self._translate_item(item)
                formatted_items.append(translated)

            result[cat_name] = formatted_items

        return result

    def _translate_item(self, item: Dict[str, Any]) -> Dict[str, Any]:
        """ç¿»è¯‘å•æ¡çƒ­ç‚¹ï¼ˆå…¨ä¸­æ–‡ï¼‰"""
        title = item.get("title", "")
        description = item.get("description", "")
        url = item.get("url", "")
        source = item.get("source", "")
        heat_score = item.get("heat_score", 0)

        # ä½¿ç”¨LLMç¿»è¯‘æ ‡é¢˜å’Œæè¿°
        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šç§‘æŠ€åª’ä½“çš„ç¿»è¯‘ç¼–è¾‘ã€‚è¯·å°†ä»¥ä¸‹AIæ–°é—»ç¿»è¯‘æˆæµç•…çš„ä¸­æ–‡ã€‚

åŸæ–‡æ ‡é¢˜ï¼š
{title}

åŸæ–‡æè¿°ï¼š
{description}

ç¿»è¯‘è¦æ±‚ï¼š
1. æ ‡é¢˜ï¼šç®€æ´æœ‰åŠ›ï¼Œä¿ç•™å…³é”®ä¿¡æ¯ï¼Œç¬¦åˆä¸­æ–‡æ–°é—»æ ‡é¢˜ä¹ æƒ¯
2. æè¿°ï¼šå®Œæ•´å‡†ç¡®ï¼Œè¯­è¨€æµç•…ï¼Œä¸è¶…è¿‡150å­—
3. ä¸“ä¸šæœ¯è¯­ï¼šå¦‚"Generative AI"è¯‘ä¸º"ç”Ÿæˆå¼AI"ï¼Œ"Large Language Model"è¯‘ä¸º"å¤§è¯­è¨€æ¨¡å‹"
4. ä¿ç•™è‹±æ–‡ä¸“æœ‰åè¯ï¼ˆå¦‚äº§å“åã€å…¬å¸åï¼‰

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¿”å›ï¼š
æ ‡é¢˜ï¼š[ä¸­æ–‡æ ‡é¢˜]
æè¿°ï¼š[ä¸­æ–‡æè¿°]"""

        try:
            response = self._call_llm(prompt)

            # è§£æç¿»è¯‘ç»“æœ
            lines = response.strip().split('\n')
            zh_title = title
            zh_description = description

            for line in lines:
                if line.startswith("æ ‡é¢˜ï¼š"):
                    zh_title = line.replace("æ ‡é¢˜ï¼š", "").strip()
                elif line.startswith("æè¿°ï¼š"):
                    zh_description = line.replace("æè¿°ï¼š", "").strip()

            return {
                "title": zh_title,
                "description": zh_description,
                "url": url,
                "source": source,
                "heat_score": heat_score
            }
        except Exception as e:
            self.log(f"ç¿»è¯‘å¤±è´¥: {e}", "WARNING")
            return {
                "title": title,
                "description": description,
                "url": url,
                "source": source,
                "heat_score": heat_score
            }

    def _assemble_full_content(
        self,
        today: datetime,
        issue_number: str,
        total_count: int,
        key_insights: List[str],
        deep_analysis: str,
        category_intros: Dict[str, str],
        translated_items: Dict[str, List[Dict]]
    ) -> str:
        """ç»„è£…å®Œæ•´ç®€æŠ¥å†…å®¹ï¼ˆå…¨ä¸­æ–‡ï¼‰"""
        parts = []

        # ========== å¤´éƒ¨ ==========
        parts.append(f"# AIæ¯æ—¥çƒ­ç‚¹ Â· {today.strftime('%Yå¹´%mæœˆ%dæ—¥')}\n\n")
        parts.append("> ğŸ“¡ æ±‡èšå…¨çƒ8å¤§AIèµ„è®¯æºï¼Œæ¯å¤©ä¸ºä½ ç²¾é€‰æœ€å‰æ²¿çš„æŠ€æœ¯åŠ¨æ€\n\n")
        parts.append(f"**æœ¬æœŸå…±æ”¶å½• {total_count} æ¡AIèµ„è®¯**\n\n")
        parts.append(f"ğŸ“… {today.strftime('%Yå¹´%mæœˆ%dæ—¥')}  Â·  ğŸ†” ç¬¬ {issue_number} æœŸ\n\n")
        parts.append("---\n\n")

        # ========== æ ¸å¿ƒæ´å¯Ÿ ==========
        if key_insights:
            parts.append("## ğŸ’¡ æ ¸å¿ƒæ´å¯Ÿ\n\n")
            for insight in key_insights:
                parts.append(f"- {insight}\n")
            parts.append("\n---\n\n")

        # ========== æ·±åº¦è§‚å¯Ÿ ==========
        if deep_analysis:
            parts.append("## ğŸ“° æ·±åº¦è§‚å¯Ÿ\n\n")
            parts.append(f"{deep_analysis}\n")
            parts.append("\n---\n\n")

        # ========== åˆ†ç±»çƒ­ç‚¹ ==========
        parts.append("## ğŸ” æœ¬æœŸçƒ­ç‚¹\n\n")

        # æŒ‰é¡ºåºè¾“å‡ºå„åˆ†ç±»
        category_order = ["äº§ä¸šåŠ¨æ€", "å­¦æœ¯å‰æ²¿", "æŠ€æœ¯åˆ›æ–°", "äº§å“å·¥å…·", "è¡Œä¸šåº”ç”¨"]

        for category in category_order:
            if category not in translated_items or not translated_items[category]:
                continue

            items = translated_items[category]

            # è·å–icon
            icon = self._get_category_icon(category)

            parts.append(f"### {icon} {category}ï¼ˆ{len(items)}æ¡ï¼‰\n\n")

            # åˆ†ç±»å¯¼è¯­
            if category in category_intros:
                parts.append(f"*{category_intros[category]}*\n\n")

            # è¯¥åˆ†ç±»çš„æ‰€æœ‰çƒ­ç‚¹
            for item in items:
                title = item.get("title", "")
                description = item.get("description", "")
                url = item.get("url", "")
                source = item.get("source", "")
                heat_score = item.get("heat_score", 0)

                parts.append(f"#### [{title}]({url})\n\n")
                parts.append(f"**æ¥æº**ï¼š{source}  Â·  **çƒ­åº¦**ï¼š{heat_score}\n\n")

                if description and len(description) > 20:
                    parts.append(f"{description}\n\n")

                parts.append("---\n\n")

        # ========== æ•°æ®æ¥æº ==========
        parts.append("## ğŸ“Š æ•°æ®æ¥æº\n\n")
        parts.append("æœ¬æœŸæ•°æ®æ¥è‡ªä»¥ä¸‹å…¨çƒAIèµ„è®¯æºï¼š\n\n")
        parts.append("- **TechCrunch AI** - ç¡…è°·ç§‘æŠ€åª’ä½“çš„AIé£å‘æ ‡\n")
        parts.append("- **NewsAPI** - å…¨çƒAIæ–°é—»èšåˆå¹³å°\n")
        parts.append("- **arXiv** - é¢„å°æœ¬è®ºæ–‡åº“ï¼Œå­¦æœ¯å‰æ²¿é¦–å‘\n")
        parts.append("- **Hacker News** - ç¡…è°·æŠ€æœ¯ç¤¾åŒºçƒ­è®®\n")
        parts.append("- **Product Hunt** - å…¨çƒAIäº§å“å‘ç°å¹³å°\n")
        parts.append("- **GitHub Trending** - å¼€æºAIé¡¹ç›®è¶‹åŠ¿\n")
        parts.append("- **The Verge AI** - æ·±åº¦æŠ€æœ¯æŠ¥é“\n")
        parts.append("- **VentureBeat AI** - AIå•†ä¸šèµ„è®¯\n")

        parts.append("\n---\n\n")
        parts.append("<div align='center'>\n\n")
        parts.append("**AIæ¯æ—¥çƒ­ç‚¹** Â· ç”± ContentForge AI è‡ªåŠ¨ç”Ÿæˆ\n\n")
        parts.append(f"{today.strftime('%Yå¹´%mæœˆ%dæ—¥')}\n\n")
        parts.append("</div>\n")

        return "".join(parts)

    def _get_category_name(self, key: str) -> str:
        """è·å–åˆ†ç±»ä¸­æ–‡å"""
        mapping = {
            "ğŸ“ˆ è¡Œä¸šåŠ¨æ€": "äº§ä¸šåŠ¨æ€",
            "ğŸ“ å­¦æœ¯çªç ´": "å­¦æœ¯å‰æ²¿",
            "ğŸ”¬ æŠ€æœ¯åˆ›æ–°": "æŠ€æœ¯åˆ›æ–°",
            "ğŸ› ï¸ AIå·¥å…·/äº§å“": "äº§å“å·¥å…·",
            "ğŸ’¼ AIåº”ç”¨": "è¡Œä¸šåº”ç”¨"
        }
        return mapping.get(key, key)

    def _get_key_by_name(self, name: str) -> str:
        """æ ¹æ®ä¸­æ–‡åè·å–key"""
        reverse_mapping = {
            "äº§ä¸šåŠ¨æ€": "industry",
            "å­¦æœ¯å‰æ²¿": "academic",
            "æŠ€æœ¯åˆ›æ–°": "tech",
            "äº§å“å·¥å…·": "product",
            "è¡Œä¸šåº”ç”¨": "application"
        }
        return reverse_mapping.get(name, name)

    def _get_category_icon(self, name: str) -> str:
        """è·å–åˆ†ç±»å›¾æ ‡"""
        mapping = {
            "äº§ä¸šåŠ¨æ€": "ğŸ“ˆ",
            "å­¦æœ¯å‰æ²¿": "ğŸ“",
            "æŠ€æœ¯åˆ›æ–°": "ğŸ”¬",
            "äº§å“å·¥å…·": "ğŸ› ï¸",
            "è¡Œä¸šåº”ç”¨": "ğŸ’¼"
        }
        return mapping.get(name, "ğŸ“Œ")
