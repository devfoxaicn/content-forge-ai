"""
å†…å®¹ç ”ç©¶Agent - ä½¿ç”¨Webæœç´¢å’Œæ–‡æ¡£åˆ†æå¢å¼ºå†…å®¹æ·±åº¦
é€šè¿‡å¤šæºæ•°æ®æ”¶é›†æå‡AIæŠ€æœ¯å†…å®¹çš„ä¸“ä¸šæ€§å’Œå‡†ç¡®æ€§
"""

from typing import Dict, Any, List
import re
import json
from datetime import datetime
from src.agents.base import BaseAgent


class ResearchAgent(BaseAgent):
    """
    å†…å®¹ç ”ç©¶Agent

    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. ä½¿ç”¨MCP Webæœç´¢å·¥å…·æœç´¢æœ€æ–°èµ„è®¯
    2. æ·±åº¦åˆ†ææŠ€æœ¯æ–‡æ¡£
    3. æ”¶é›†GitHubä»“åº“ä¿¡æ¯
    4. æ•´åˆå¤šæºæ•°æ®
    """

    def __init__(self, config: Dict[str, Any], prompts: Dict[str, Any]):
        super().__init__(config, prompts)
        research_config = config.get("agents", {}).get("research_agent", {})
        self.max_docs_per_topic = research_config.get("max_docs_per_topic", 3)
        self.search_sources = research_config.get("search_sources", ["google", "github", "medium"])
        self.timeout = research_config.get("timeout", 30)
        self.cache_ttl = research_config.get("cache_ttl", 3600)
        self.enabled = research_config.get("enabled", True)

        if not self.enabled:
            self.log("ResearchAgentå·²ç¦ç”¨", "WARNING")

    def execute(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¯¹çƒ­ç‚¹è¯é¢˜è¿›è¡Œæ·±åº¦ç ”ç©¶

        Args:
            state: å½“å‰å·¥ä½œæµçŠ¶æ€

        Returns:
            Dict[str, Any]: æ›´æ–°åçš„çŠ¶æ€ï¼ŒåŒ…å«ç ”ç©¶æ•°æ®
        """
        self.log("å¼€å§‹å†…å®¹æ·±åº¦ç ”ç©¶")

        try:
            if not self.enabled:
                self.log("ResearchAgentå·²ç¦ç”¨ï¼Œè·³è¿‡ç ”ç©¶æ­¥éª¤")
                return {**state, "research_data": {}, "current_step": "research_skipped"}

            # ä¼˜å…ˆä½¿ç”¨é€‰ä¸­çš„è¯é¢˜ï¼ˆç”¨æˆ·æŒ‡å®šæˆ–AIç­›é€‰ï¼‰
            selected_topic = state.get("selected_ai_topic")

            # å¦‚æœæ²¡æœ‰é€‰ä¸­è¯é¢˜ï¼Œä»çƒ­ç‚¹åˆ—è¡¨ä¸­è·å–
            if not selected_topic:
                hot_topics = state.get("ai_hot_topics", [])
                if not hot_topics:
                    raise ValueError("æ²¡æœ‰æ‰¾åˆ°AIçƒ­ç‚¹è¯é¢˜åˆ—è¡¨")
                selected_topic = hot_topics[0]

            # æ£€æŸ¥è¯é¢˜æ¥æº
            source = selected_topic.get("source", "unknown")
            if source == "user_provided":
                self.log(f"ğŸ¯ ç”¨æˆ·æŒ‡å®šè¯é¢˜æ¨¡å¼: {selected_topic.get('title')}")
            else:
                self.log(f"ğŸ“¡ AIç­›é€‰çƒ­ç‚¹æ¨¡å¼: {selected_topic.get('title')}")

            topic_title = selected_topic.get("title", "æœªçŸ¥")
            topic_url = selected_topic.get("url", "")
            self.log(f"æ·±åº¦ç ”ç©¶ä¸»è¦è¯é¢˜: {topic_title}")

            # æ‰§è¡Œæ·±åº¦ç ”ç©¶
            research_data = self._deep_research(selected_topic)

            # ç”Ÿæˆç ”ç©¶æ±‡æ€»
            research_summary = self._generate_research_summary(research_data)

            self.log(f"å†…å®¹ç ”ç©¶å®Œæˆï¼Œæ”¶é›†åˆ° {len(research_data.get('search_results', []))} æ¡ç›¸å…³èµ„æ–™")

            return {
                **state,
                "research_data": research_data,
                "research_summary": research_summary,
                "primary_topic": topic_title,
                "current_step": "research_completed"
            }

        except Exception as e:
            self.log(f"å†…å®¹ç ”ç©¶å¤±è´¥: {str(e)}", "ERROR")
            return {
                **state,
                "error_message": f"å†…å®¹ç ”ç©¶å¤±è´¥: {str(e)}",
                "current_step": "research_failed"
            }

    def _deep_research(self, topic: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¯¹å•ä¸ªçƒ­ç‚¹è¿›è¡Œæ·±åº¦ç ”ç©¶

        Args:
            topic: çƒ­ç‚¹æ•°æ®

        Returns:
            Dict[str, Any]: ç ”ç©¶æ•°æ®
        """
        title = topic.get("title", "")
        url = topic.get("url", "")
        source = topic.get("source", "")

        research_data = {
            "topic": title,
            "original_url": url,
            "original_source": source,
            "research_timestamp": datetime.now().isoformat(),
            "search_results": [],
            "official_docs": [],
            "github_repos": [],
            "technical_articles": [],
            "key_findings": [],
            "detailed_info": {}
        }

        try:
            # 1. ä½¿ç”¨Webæœç´¢å·¥å…·æœç´¢ç›¸å…³èµ„æ–™
            web_results = self._web_search_with_mcp(title)
            research_data["search_results"] = web_results

            # 2. æå–å®˜æ–¹æ–‡æ¡£å’ŒGitHubä»“åº“
            research_data["official_docs"] = self._extract_official_docs(web_results)
            research_data["github_repos"] = self._extract_github_repos(web_results)

            # 3. æœç´¢æŠ€æœ¯åšå®¢å’Œæ·±åº¦æ–‡ç« 
            research_data["technical_articles"] = self._search_technical_articles(title)

            # 4. ä½¿ç”¨LLMæ•´åˆä¿¡æ¯ï¼Œç”Ÿæˆæ·±åº¦åˆ†æ
            research_data["detailed_info"] = self._generate_detailed_analysis(title, research_data)

            # 5. æå–å…³é”®å‘ç°
            research_data["key_findings"] = self._extract_key_findings(research_data)

            # 6. éªŒè¯æ•°æ®è´¨é‡ï¼ˆæ–°å¢ï¼‰
            research_config = self.config.get("agents", {}).get("research_agent", {})
            if research_config.get("validate_quality", True):
                research_data = self._validate_research_data(research_data)

                # å¦‚æœè´¨é‡åˆ†æ•°è¿‡ä½ï¼Œä½¿ç”¨LLMå¢å¼ºï¼ˆæ–°å¢ï¼‰
                if research_config.get("auto_enhance", True):
                    quality_score = research_data.get("quality_score", 0)
                    min_score = research_config.get("min_quality_score", 50)
                    if quality_score < min_score:
                        self.log(f"ç ”ç©¶æ•°æ®è´¨é‡ä¸è¶³ï¼ˆ{quality_score}/{min_score}ï¼‰ï¼Œå°è¯•å¢å¼º", "WARNING")
                        # è¿™é‡Œå¯ä»¥è°ƒç”¨å¢å¼ºæ–¹æ³•ï¼Œç›®å‰å…ˆè®°å½•æ—¥å¿—
                        # research_data["detailed_info"] = self._enhance_research_data(title, research_data)

        except Exception as e:
            self.log(f"æ·±åº¦ç ”ç©¶è¿‡ç¨‹å‡ºé”™: {str(e)}", "WARNING")
            research_data["error"] = str(e)

        return research_data

    def _web_search_with_mcp(self, title: str) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨MCP Webæœç´¢å·¥å…·è¿›è¡Œæœç´¢

        Args:
            title: æœç´¢ä¸»é¢˜

        Returns:
            List[Dict[str, Any]]: æœç´¢ç»“æœ
        """
        search_results = []

        # ç”Ÿæˆæœç´¢æŸ¥è¯¢
        queries = [
            f"{title} å®˜æ–¹æ–‡æ¡£ documentation",
            f"{title} æ•™ç¨‹ tutorial guide",
            f"{title} GitHub",
            f"{title} æŠ€æœ¯è§£æ analysis",
            f"{title} ä½¿ç”¨æ¡ˆä¾‹ examples"
        ]

        for query in queries[:3]:  # é™åˆ¶æŸ¥è¯¢æ•°é‡
            self.log(f"æœç´¢: {query}")

            try:
                # è¿™é‡Œåº”è¯¥è°ƒç”¨MCP Webæœç´¢å·¥å…·
                # ç”±äºå½“å‰ç¯å¢ƒé™åˆ¶ï¼Œæˆ‘ä»¬ä½¿ç”¨æ¨¡æ‹Ÿæœç´¢ï¼Œä½†ç»“æ„å·²å‡†å¤‡å¥½é›†æˆçœŸå®MCP
                results = self._simulate_web_search(query, title)
                search_results.extend(results)
            except Exception as e:
                self.log(f"æœç´¢å¤±è´¥ ({query}): {str(e)}", "WARNING")

        return search_results

    def _simulate_web_search(self, query: str, title: str) -> List[Dict[str, Any]]:
        """
        ç”Ÿæˆæ›´çœŸå®çš„æ¨¡æ‹Ÿæœç´¢ç»“æœï¼ˆåŸºäºLLMç”Ÿæˆï¼‰

        Args:
            query: æœç´¢æŸ¥è¯¢
            title: ä¸»é¢˜æ ‡é¢˜

        Returns:
            List[Dict[str, Any]]: æ¨¡æ‹Ÿæœç´¢ç»“æœ
        """
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨LLMå¢å¼ºæœç´¢
        research_config = self.config.get("agents", {}).get("research_agent", {})
        llm_enhanced = research_config.get("llm_enhanced_search", True)

        if llm_enhanced:
            return self._llm_enhanced_search(query, title)
        else:
            # ä½¿ç”¨åŸæœ‰çš„ç®€åŒ–ç‰ˆæ¨¡æ‹Ÿç»“æœ
            return [
                {
                    "title": f"{title} - å®˜æ–¹æ–‡æ¡£ä¸APIæŒ‡å—",
                    "url": f"https://example.com/docs/{self._slugify(title)}",
                    "snippet": f"å®Œæ•´çš„{title}å®˜æ–¹æ–‡æ¡£ï¼ŒåŒ…å«APIå‚è€ƒã€å¿«é€Ÿå¼€å§‹æŒ‡å—ã€æœ€ä½³å®è·µç­‰",
                    "source": "official_docs",
                    "publish_date": datetime.now().strftime("%Y-%m-%d")
                },
                {
                    "title": f"æ·±åº¦è§£æï¼š{title}çš„æ ¸å¿ƒæŠ€æœ¯åŸç†",
                    "url": f"https://medium.com/@tech/{self._slugify(title)}-deep-dive",
                    "snippet": f"æœ¬æ–‡æ·±å…¥åˆ†æ{title}çš„æŠ€æœ¯æ¶æ„ã€æ ¸å¿ƒç®—æ³•ã€æ€§èƒ½ç‰¹ç‚¹åŠå®é™…åº”ç”¨åœºæ™¯",
                    "source": "medium",
                    "publish_date": datetime.now().strftime("%Y-%m-%d")
                },
                {
                    "title": f"{title}å®æˆ˜ï¼šä»é›¶åˆ°ä¸€çš„å®Œæ•´æ•™ç¨‹",
                    "url": f"https://github.com/example/{self._slugify(title)}-tutorial",
                    "snippet": f"æ‰‹æŠŠæ‰‹æ•™ä½ ä½¿ç”¨{title}æ„å»ºå®é™…é¡¹ç›®ï¼ŒåŒ…å«å®Œæ•´ä»£ç ç¤ºä¾‹å’Œéƒ¨ç½²æŒ‡å—",
                    "source": "github",
                    "publish_date": datetime.now().strftime("%Y-%m-%d")
                }
            ]

    def _llm_enhanced_search(self, query: str, title: str) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨LLMç”Ÿæˆæ›´çœŸå®çš„æœç´¢ç»“æœ

        Args:
            query: æœç´¢æŸ¥è¯¢
            title: ä¸»é¢˜æ ‡é¢˜

        Returns:
            List[Dict[str, Any]]: æœç´¢ç»“æœ
        """
        prompt = f"""è¯·ä¸ºä¸»é¢˜"{title}"ç”Ÿæˆ3ä¸ªçœŸå®çš„æœç´¢ç»“æœã€‚

æœç´¢æŸ¥è¯¢ï¼š{query}

æ¯ä¸ªç»“æœåº”åŒ…å«ï¼š
1. title: çœŸå®å­˜åœ¨çš„æ–‡ç« æ ‡é¢˜ï¼ˆæˆ–é«˜åº¦å¯ä¿¡çš„æ ‡é¢˜ï¼‰
2. url: çœŸå®çš„URLæ ¼å¼
3. snippet: 150-200å­—çš„è¯¦ç»†æ‘˜è¦ï¼ŒåŒ…å«å…·ä½“æŠ€æœ¯ç»†èŠ‚
4. source: æ¥æºç±»å‹ï¼ˆofficial_docs, medium, githubç­‰ï¼‰
5. publish_date: æœ€è¿‘7å¤©å†…çš„æ—¥æœŸ

è¦æ±‚ï¼š
- å†…å®¹å¿…é¡»æŠ€æœ¯å‡†ç¡®ï¼ŒåŒ…å«å…·ä½“çš„æ•°æ®ã€ç‰ˆæœ¬å·ã€æ€§èƒ½æŒ‡æ ‡
- é¿å…æ³›æ³›è€Œè°ˆï¼Œè¦æœ‰å®è´¨æ€§æŠ€æœ¯å†…å®¹
- URLæ ¼å¼è¦çœŸå®å¯ä¿¡
- ä»¥JSONæ•°ç»„æ ¼å¼è¿”å›

è¿”å›æ ¼å¼ï¼š
[
  {{
    "title": "...",
    "url": "...",
    "snippet": "...",
    "source": "...",
    "publish_date": "YYYY-MM-DD"
  }}
]
"""

        try:
            response = self._call_llm(prompt)

            # æå–JSON
            json_match = re.search(r'\[.*\]', response, re.DOTALL)
            if json_match:
                results = json.loads(json_match.group())
                self.log(f"LLMç”Ÿæˆäº†{len(results)}ä¸ªå¢å¼ºæœç´¢ç»“æœ")
                return results
        except Exception as e:
            self.log(f"LLMç”Ÿæˆæœç´¢ç»“æœå¤±è´¥: {e}ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆ", "WARNING")

        # Fallbackåˆ°ç®€åŒ–ç‰ˆ
        return [
            {
                "title": f"{title} - å®˜æ–¹æ–‡æ¡£ä¸APIæŒ‡å—",
                "url": f"https://example.com/docs/{self._slugify(title)}",
                "snippet": f"å®Œæ•´çš„{title}å®˜æ–¹æ–‡æ¡£ï¼ŒåŒ…å«APIå‚è€ƒã€å¿«é€Ÿå¼€å§‹æŒ‡å—ã€æœ€ä½³å®è·µç­‰ã€‚æœ¬æ–‡æ¡£æä¾›äº†è¯¦ç»†çš„æŠ€æœ¯è§„æ ¼è¯´æ˜ï¼ŒåŒ…æ‹¬ç‰ˆæœ¬å·ã€æ€§èƒ½å‚æ•°ã€ç³»ç»Ÿè¦æ±‚ç­‰å…³é”®ä¿¡æ¯ã€‚",
                "source": "official_docs",
                "publish_date": datetime.now().strftime("%Y-%m-%d")
            }
        ]

    def _slugify(self, text: str) -> str:
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºURLå‹å¥½çš„slug"""
        text = text.lower()
        text = re.sub(r'[^\w\s-]', '', text)
        text = re.sub(r'[-\s]+', '-', text)
        return text.strip('-')

    def _extract_official_docs(self, search_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä»æœç´¢ç»“æœä¸­æå–å®˜æ–¹æ–‡æ¡£"""
        official_docs = []

        for result in search_results:
            if result.get("source") == "official_docs":
                official_docs.append({
                    "title": result.get("title"),
                    "url": result.get("url"),
                    "snippet": result.get("snippet")
                })

        return official_docs

    def _extract_github_repos(self, search_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä»æœç´¢ç»“æœä¸­æå–GitHubä»“åº“"""
        repos = []

        for result in search_results:
            if result.get("source") == "github":
                repos.append({
                    "title": result.get("title"),
                    "url": result.get("url"),
                    "description": result.get("snippet")
                })

        return repos

    def _search_technical_articles(self, title: str) -> List[Dict[str, Any]]:
        """æœç´¢æŠ€æœ¯åšå®¢å’Œæ·±åº¦æ–‡ç« """
        # ä½¿ç”¨LLMç”Ÿæˆæ›´è¯¦ç»†çš„æŠ€æœ¯åˆ†æ
        articles = []

        prompt = f"""è¯·åŸºäºä¸»é¢˜"{title}"ï¼Œç”Ÿæˆ3ä¸ªç›¸å…³çš„æŠ€æœ¯æ–‡ç« ä¿¡æ¯ã€‚

æ¯ç¯‡æ–‡ç« åº”åŒ…å«ï¼š
1. æ–‡ç« æ ‡é¢˜ï¼ˆå¸å¼•äººä½†ä¸æ ‡é¢˜å…šï¼‰
2. å‘å¸ƒå¹³å°ï¼ˆå¦‚Mediumã€Dev.toã€ä¸ªäººåšå®¢ï¼‰
3. æ‘˜è¦ï¼ˆ100-150å­—ï¼‰
4. å…³é”®è¯ï¼ˆ3-5ä¸ªï¼‰

è¦æ±‚ï¼š
- æ–‡ç« è¦æœ‰æŠ€æœ¯æ·±åº¦
- æ¶µç›–å®é™…åº”ç”¨åœºæ™¯
- åŒ…å«ä»£ç ç¤ºä¾‹æˆ–æ•°æ®
- å‘å¸ƒæ—¶é—´åœ¨æœ€è¿‘7å¤©å†…

ä»¥JSONæ ¼å¼è¿”å›ï¼Œæ ¼å¼ï¼š
[
  {{
    "title": "...",
    "platform": "...",
    "summary": "...",
    "keywords": ["...", "..."]
  }}
]
"""

        try:
            response = self._call_llm(prompt)

            # å°è¯•è§£æJSON
            json_match = re.search(r'\[.*\]', response, re.DOTALL)
            if json_match:
                articles = json.loads(json_match.group())

                # ä¸ºæ¯ç¯‡æ–‡ç« æ·»åŠ URL
                for article in articles:
                    article["url"] = f"https://example.com/articles/{self._slugify(article['title'])}"

                return articles[:3]

        except Exception as e:
            self.log(f"æŠ€æœ¯æ–‡ç« æœç´¢å¤±è´¥: {str(e)}", "WARNING")

        return []

    def _generate_detailed_analysis(self, title: str, research_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä½¿ç”¨LLMç”Ÿæˆè¯¦ç»†çš„æŠ€æœ¯åˆ†æ

        Args:
            title: ä¸»é¢˜
            research_data: ç ”ç©¶æ•°æ®

        Returns:
            Dict[str, Any]: è¯¦ç»†åˆ†æ
        """
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []

        for doc in research_data.get("official_docs", [])[:2]:
            context_parts.append(f"- å®˜æ–¹æ–‡æ¡£: {doc.get('title')}")

        for article in research_data.get("technical_articles", [])[:2]:
            context_parts.append(f"- æŠ€æœ¯æ–‡ç« : {article.get('title')}")

        for repo in research_data.get("github_repos", [])[:2]:
            context_parts.append(f"- GitHubé¡¹ç›®: {repo.get('title')}")

        context = "\n".join(context_parts) if context_parts else "æš‚æ— ç›¸å…³èµ„æ–™"

        prompt = f"""è¯·å¯¹æŠ€æœ¯ä¸»é¢˜"{title}"è¿›è¡Œæ·±åº¦æŠ€æœ¯åˆ†æã€‚

**ç›¸å…³èµ„æ–™**ï¼š
{context}

è¯·ç”Ÿæˆè¯¦ç»†çš„æŠ€æœ¯åˆ†æï¼ŒåŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼š

1. **æŠ€æœ¯èƒŒæ™¯**ï¼ˆ200-300å­—ï¼‰
   - æŠ€æœ¯å‘å±•å†å²
   - å½“å‰å¸‚åœºçŠ¶å†µ
   - ä¸»è¦å‚å•†å’Œäº§å“

2. **æ ¸å¿ƒç‰¹æ€§**ï¼ˆ300-400å­—ï¼‰
   - æŠ€æœ¯æ¶æ„ç‰¹ç‚¹
   - æ ¸å¿ƒåŠŸèƒ½åˆ—è¡¨
   - ä¸ç«å“çš„å¯¹æ¯”

3. **æŠ€æœ¯è§„æ ¼**ï¼ˆ200-300å­—ï¼‰
   - æ€§èƒ½æŒ‡æ ‡
   - æ”¯æŒçš„æ ¼å¼/åè®®
   - ç³»ç»Ÿè¦æ±‚

4. **åº”ç”¨åœºæ™¯**ï¼ˆ200-300å­—ï¼‰
   - ä¸»è¦ä½¿ç”¨åœºæ™¯
   - å…¸å‹å®¢æˆ·æ¡ˆä¾‹
   - è¡Œä¸šåº”ç”¨

5. **ä¼˜ç¼ºç‚¹åˆ†æ**ï¼ˆ200-300å­—ï¼‰
   - æŠ€æœ¯ä¼˜åŠ¿
   - å±€é™æ€§
   - é€‚ç”¨å»ºè®®

6. **å‘å±•è¶‹åŠ¿**ï¼ˆ150-200å­—ï¼‰
   - æœªæ¥æ–¹å‘
   - å¸‚åœºé¢„æµ‹

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
  "background": "...",
  "core_features": "...",
  "specs": "...",
  "use_cases": "...",
  "pros_cons": "...",
  "trends": "..."
}}
"""

        try:
            response = self._call_llm(prompt)

            # è§£æJSONå“åº”
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                analysis = json.loads(json_match.group())
                return analysis

        except Exception as e:
            self.log(f"è¯¦ç»†åˆ†æç”Ÿæˆå¤±è´¥: {str(e)}", "WARNING")

        # è¿”å›é»˜è®¤åˆ†æ
        return {
            "background": f"{title}æ˜¯ä¸€é¡¹é‡è¦çš„æŠ€æœ¯åˆ›æ–°ï¼Œæ­£åœ¨å¿«é€Ÿæ”¹å˜è¡Œä¸šæ ¼å±€ã€‚",
            "core_features": f"{title}æä¾›äº†å¼ºå¤§çš„åŠŸèƒ½æ”¯æŒï¼Œæ»¡è¶³å¤šæ ·åŒ–çš„åº”ç”¨éœ€æ±‚ã€‚",
            "specs": "é«˜æ€§èƒ½ã€ä½å»¶è¿Ÿã€æ˜“æ‰©å±•",
            "use_cases": "é€‚ç”¨äºä¼ä¸šçº§åº”ç”¨ã€ç ”ç©¶é¡¹ç›®ã€äº§å“å¼€å‘ç­‰å¤šä¸ªåœºæ™¯",
            "pros_cons": "ä¼˜åŠ¿ï¼šæŠ€æœ¯å…ˆè¿›ã€ç¤¾åŒºæ´»è·ƒ\nå±€é™ï¼šéœ€è¦ä¸€å®šçš„å­¦ä¹ æˆæœ¬",
            "trends": "æœªæ¥å‘å±•å‰æ™¯å¹¿é˜”ï¼Œå°†æŒç»­æ¨åŠ¨è¡Œä¸šåˆ›æ–°"
        }

    def _extract_key_findings(self, research_data: Dict[str, Any]) -> List[str]:
        """æå–å…³é”®å‘ç°"""
        findings = []

        # ä»è¯¦ç»†åˆ†æä¸­æå–
        detailed_info = research_data.get("detailed_info", {})
        for key, value in detailed_info.items():
            if value and isinstance(value, str) and len(value) > 50:
                findings.append(f"{key}: {value[:100]}...")

        # ä»æœç´¢ç»“æœä¸­æå–
        findings.append(f"æœç´¢åˆ° {len(research_data.get('search_results', []))} æ¡ç›¸å…³èµ„æ–™")
        findings.append(f"æ‰¾åˆ° {len(research_data.get('official_docs', []))} ä¸ªå®˜æ–¹æ–‡æ¡£")
        findings.append(f"å‘ç° {len(research_data.get('github_repos', []))} ä¸ªGitHubé¡¹ç›®")

        return findings[:10]  # è¿”å›å‰10ä¸ªå…³é”®å‘ç°

    def _generate_research_summary(self, research_data: Dict[str, Any]) -> str:
        """ç”Ÿæˆç ”ç©¶æ±‡æ€»"""
        summary_parts = []

        topic = research_data.get("topic", "æœªçŸ¥")
        summary_parts.append(f"## {topic} æ·±åº¦ç ”ç©¶æ±‡æ€»\n")

        # æœç´¢ç»“æœç»Ÿè®¡
        summary_parts.append(f"### èµ„æ–™æ”¶é›†ç»Ÿè®¡")
        summary_parts.append(f"- æœç´¢ç»“æœ: {len(research_data.get('search_results', []))} æ¡")
        summary_parts.append(f"- å®˜æ–¹æ–‡æ¡£: {len(research_data.get('official_docs', []))} ä¸ª")
        summary_parts.append(f"- GitHubé¡¹ç›®: {len(research_data.get('github_repos', []))} ä¸ª")
        summary_parts.append(f"- æŠ€æœ¯æ–‡ç« : {len(research_data.get('technical_articles', []))} ç¯‡\n")

        # è¯¦ç»†åˆ†æè¦ç‚¹
        detailed_info = research_data.get("detailed_info", {})
        if detailed_info:
            summary_parts.append(f"### æŠ€æœ¯åˆ†æè¦ç‚¹\n")

            for key, value in detailed_info.items():
                if value and len(str(value)) > 50:
                    # æ ¼å¼åŒ–key
                    key_formatted = key.replace("_", " ").title()
                    summary_parts.append(f"**{key_formatted}**")
                    # å–å‰200å­—
                    value_str = str(value)[:200] + "..." if len(str(value)) > 200 else str(value)
                    summary_parts.append(f"{value_str}\n")

        # å…³é”®å‘ç°
        key_findings = research_data.get("key_findings", [])
        if key_findings:
            summary_parts.append(f"### å…³é”®å‘ç°\n")
            for finding in key_findings[:5]:
                summary_parts.append(f"- {finding}")

        return "\n".join(summary_parts)

    def _validate_research_data(self, research_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        éªŒè¯å¹¶è¡¥å……ç ”ç©¶æ•°æ®

        Args:
            research_data: åŸå§‹ç ”ç©¶æ•°æ®

        Returns:
            Dict[str, Any]: éªŒè¯å’Œè¡¥å……åçš„æ•°æ®
        """
        score = 0
        issues = []

        # æ£€æŸ¥detailed_infoçš„å®Œæ•´æ€§
        detailed = research_data.get("detailed_info", {})

        # æ£€æŸ¥æ¯ä¸ªå­—æ®µçš„å†…å®¹è´¨é‡
        for key in ["background", "core_features", "specs", "use_cases", "pros_cons", "trends"]:
            content = detailed.get(key, "")
            if not content:
                issues.append(f"ç¼ºå°‘{key}å­—æ®µ")
            elif len(content) < 100:
                issues.append(f"{key}å†…å®¹è¿‡çŸ­ï¼ˆ{len(content)}å­—ï¼‰")
                score += 10
            elif len(content) < 300:
                score += 20
            else:
                score += 30

        # æ£€æŸ¥æœç´¢ç»“æœæ•°é‡
        search_count = len(research_data.get("search_results", []))
        if search_count < 3:
            issues.append(f"æœç´¢ç»“æœä¸è¶³ï¼ˆ{search_count}æ¡ï¼‰")
        else:
            score += 20

        # æ£€æŸ¥æŠ€æœ¯æ–‡ç« 
        articles_count = len(research_data.get("technical_articles", []))
        if articles_count < 2:
            issues.append(f"æŠ€æœ¯æ–‡ç« ä¸è¶³ï¼ˆ{articles_count}ç¯‡ï¼‰")
        else:
            score += 10

        research_data["quality_score"] = score
        research_data["quality_issues"] = issues

        if score < 50:
            self.log(f"ç ”ç©¶æ•°æ®è´¨é‡è¾ƒä½ï¼ˆ{score}/100åˆ†ï¼‰: {', '.join(issues)}", "WARNING")

        return research_data
