"""
World Class AI News Digest Generator v8.0
é¡¶çº§AIæ–°é—»ç®€æŠ¥ç”Ÿæˆå™¨ v8.0 - å¢å¼ºç‰ˆ

v8.0 æ–°ç‰¹æ€§:
- æ•´åˆ copywriting åŸåˆ™ï¼ˆæ¸…æ™°åº¦ã€åˆ©ç›Šå¯¼å‘ã€å…·ä½“æ€§ï¼‰
- æ•´åˆ copy-editing åŸåˆ™ï¼ˆ7æ¬¡æ‰«æä¼˜åŒ–ï¼‰
- å¼ºåŒ–æ ¸å¿ƒæ´å¯Ÿæå–ï¼ˆè¡Œä¸šæ·±åº¦åˆ†æï¼‰
- ä¼˜åŒ–ç¿»è¯‘è´¨é‡ï¼ˆç§‘æŠ€åª’ä½“æ ‡å‡†ï¼‰
- å¢å¼ºå¯è¯»æ€§å’Œå¸å¼•åŠ›
"""

from datetime import datetime
from typing import Dict, Any, List
from loguru import logger
import json
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from collections import Counter


class WorldClassDigestAgentV8:
    """ä¸–ç•Œé¡¶çº§AIæ–°é—»ç®€æŠ¥ç”Ÿæˆå™¨ v8.0"""

    def __init__(self, config: Dict[str, Any], prompts: Dict[str, Any]):
        self.config = config
        self.prompts = prompts
        self.name = "world_class_digest_v8"
        self.llm = self._init_llm()

        # ç¿»è¯‘é…ç½®
        agent_config = config.get("agents", {}).get("world_class_digest", {})
        self.translate_enabled = agent_config.get("translate_enabled", True)
        self.batch_size = agent_config.get("batch_size", 5)

        self.log(f"v8.0åˆå§‹åŒ–å®Œæˆï¼Œæ•´åˆ copywriting + copy-editing åŸåˆ™")

    def _init_llm(self) -> ChatOpenAI:
        """åˆå§‹åŒ–LLM"""
        try:
            import os
            from pathlib import Path
            from dotenv import load_dotenv

            # åŠ è½½.envæ–‡ä»¶
            project_root = Path(__file__).parent.parent.parent
            env_file = project_root / ".env"
            if env_file.exists():
                load_dotenv(env_file)
                self.log(f"å·²åŠ è½½ç¯å¢ƒå˜é‡æ–‡ä»¶: {env_file}")

            llm_config = self.config.get("llm", {})
            provider = llm_config.get("provider", "zhipuai")

            if provider == "zhipuai":
                api_key = os.getenv("ZHIPUAI_API_KEY")
                if not api_key:
                    api_key = self.config.get("api_keys", {}).get("zhipuai")

                if not api_key:
                    self.log("æœªé…ç½®ZHIPUAI_API_KEY", "WARNING")
                    return None

                zhipu_config = llm_config.get("zhipuai", {})
                return ChatOpenAI(
                    model=zhipu_config.get("model", "glm-4-flash"),
                    openai_api_key=api_key,
                    base_url=zhipu_config.get("base_url", "https://open.bigmodel.cn/api/coding/paas/v4/"),
                    temperature=zhipu_config.get("temperature", 0.7),
                    max_tokens=zhipu_config.get("max_tokens", 8000),
                    timeout=zhipu_config.get("timeout", 600)
                )
            return None
        except Exception as e:
            self.log(f"LLMåˆå§‹åŒ–å¤±è´¥: {e}", "WARNING")
            return None

    def log(self, message: str, level: str = "INFO"):
        logger.log(level, f"[WorldClassDigestAgentV8] {message}")

    def execute(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œç®€æŠ¥ç”Ÿæˆ"""
        try:
            self.log("å¼€å§‹ç”Ÿæˆä¸–ç•Œé¡¶çº§AIæ–°é—»ç®€æŠ¥ v8.0...")

            # ä½¿ç”¨ scored_trends
            scored_trends = state.get("scored_trends", {})
            editors_pick = state.get("editors_pick", [])
            source_status = state.get("source_status", {})

            # ç”Ÿæˆç®€æŠ¥
            digest = self._generate_world_class_digest_v8(
                scored_trends,
                editors_pick,
                source_status
            )

            return {
                **state,
                "news_digest": digest,
                "current_step": "digest_generated"
            }

        except Exception as e:
            self.log(f"ç®€æŠ¥ç”Ÿæˆå¤±è´¥: {e}", "ERROR")
            return {
                **state,
                "error_message": str(e),
                "current_step": "world_class_digest_failed"
            }

    def _generate_world_class_digest_v8(
        self,
        scored_trends: Dict[str, Dict],
        editors_pick: List[Dict],
        source_status: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ç”Ÿæˆä¸–ç•Œé¡¶çº§AIæ–°é—»ç®€æŠ¥ v8.0"""

        today = datetime.now()
        issue_number = today.strftime("%Y%m%d")

        # è®¡ç®—æ€»æ•°
        total_count = sum(
            cat_data.get("count", 0)
            for cat_data in scored_trends.values()
        )

        self.log(f"ç”Ÿæˆç®€æŠ¥: {total_count}æ¡ç²¾é€‰çƒ­ç‚¹")

        # ç¬¬1æ­¥ï¼šç¿»è¯‘å’Œå¢å¼ºæ–°é—»ï¼ˆåº”ç”¨ copywriting åŸåˆ™ï¼‰
        enhanced_editors_pick = self._enhance_news_items_v8(editors_pick)
        for cat_name, cat_data in scored_trends.items():
            items = cat_data.get("items", [])
            enhanced_items = self._enhance_news_items_v8(items)
            cat_data["items"] = enhanced_items

        # ç¬¬2æ­¥ï¼šæå–æ ¸å¿ƒæ´å¯Ÿï¼ˆåº”ç”¨ content-research-writer åŸåˆ™ï¼‰
        all_items = []
        for cat_data in scored_trends.values():
            all_items.extend(cat_data.get("items", []))

        core_insights = self._extract_core_insights_v8(all_items)

        # ç¬¬3æ­¥ï¼šè¯†åˆ«çƒ­é—¨è¯é¢˜
        trending_topics = self._identify_trending_topics(all_items)

        # ç¬¬4æ­¥ï¼šç”Ÿæˆæ·±åº¦è§‚å¯Ÿï¼ˆæ–°å¢ï¼‰
        deep_observation = self._generate_deep_observation(all_items, core_insights)

        # ç¬¬5æ­¥ï¼šç”ŸæˆMarkdownå†…å®¹ï¼ˆåº”ç”¨ copy-editing 7æ¬¡æ‰«æï¼‰
        markdown_content = self._generate_markdown_v8(
            scored_trends,
            enhanced_editors_pick,
            core_insights,
            trending_topics,
            deep_observation,
            source_status,
            today,
            issue_number,
            total_count
        )

        # ç¬¬6æ­¥ï¼šç”ŸæˆJSONæ•°æ®
        json_data = self._generate_json_v8(
            scored_trends,
            enhanced_editors_pick,
            core_insights,
            trending_topics,
            deep_observation,
            source_status,
            today,
            issue_number,
            total_count,
            markdown_content
        )

        return json_data

    def _enhance_news_items_v8(self, items: List[Dict]) -> List[Dict]:
        """ä¸ºæ–°é—»æ¡ç›®å¢å¼ºä¿¡æ¯ v8.0ï¼ˆåº”ç”¨ copywriting åŸåˆ™ï¼‰"""
        if not items:
            return items

        # æ‰¹é‡ç¿»è¯‘ï¼ˆä½¿ç”¨æ”¹è¿›çš„ promptï¼‰
        if self.translate_enabled and self.llm:
            items = self._batch_translate_items_v8(items)

        # ä¸ºé‡è¦æ–°é—»ç”Ÿæˆå¢å¼ºåˆ†æ
        for item in items:
            importance = item.get("importance_score", 0)
            if importance >= 75:  # åªä¸ºæœ€é‡è¦æ–°é—»ç”Ÿæˆè¯¦ç»†åˆ†æ
                enhanced = self._generate_enhanced_analysis_v8(item)
                item.update(enhanced)

        return items

    def _batch_translate_items_v8(self, items: List[Dict]) -> List[Dict]:
        """æ‰¹é‡ç¿»è¯‘æ–°é—»æ ‡é¢˜å’Œæ‘˜è¦ v8.0ï¼ˆåº”ç”¨ copywriting åŸåˆ™ï¼‰"""
        if not items or not self.translate_enabled:
            return items

        # æ£€æŸ¥æ˜¯å¦å·²æœ‰ä¸­æ–‡
        if items[0].get("title_cn"):
            return items

        # æ„å»ºç¿»è¯‘æç¤ºï¼ˆæ•´åˆ copywriting åŸåˆ™ï¼‰
        news_items = []
        for i, item in enumerate(items):
            title = item.get("title", "").replace('&amp;', '&').replace('&quot;', '"')
            desc = item.get("description", "").replace('&amp;', '&').replace('&quot;', '"')
            desc = desc.replace('<p>', '').replace('</p>', '').replace('<br>', ' ')[:200]
            news_items.append(f"{i+1}. æ ‡é¢˜: {title}\n   æ‘˜è¦: {desc}")

        prompt = f"""ä½ æ˜¯TechCrunchã€The Vergeã€Wiredç­‰é¡¶çº§ç§‘æŠ€åª’ä½“çš„ä¸­æ–‡ä¸»ç¼–ã€‚

ã€æ ¸å¿ƒç¿»è¯‘åŸåˆ™ - Copywriting Standardsã€‘

1. **æ¸…æ™°åº¦ä¼˜äºèªæ˜** (Clarity Over Cleverness)
   - ç›´æ¥ä¼ è¾¾æ ¸å¿ƒä¿¡æ¯ï¼Œä¸è¦æ•…å¼„ç„è™š
   - ç¤ºä¾‹ï¼š"GPT-5å‘å¸ƒ" ä¼˜äº "GPT-5éœ‡æ’¼ç™»åœº"

2. **åˆ©ç›Šä¼˜äºåŠŸèƒ½** (Benefits Over Features)
   - æ ‡é¢˜è¦çªå‡º"è¿™å¯¹è¯»è€…æ„å‘³ç€ä»€ä¹ˆ"
   - ç¤ºä¾‹ï¼š"æ”¯æŒ100ä¸‡tokensï¼Œå¯å¤„ç†æ•´æœ¬ä¹¦" ä¼˜äº "å…·æœ‰100ä¸‡tokensä¸Šä¸‹æ–‡çª—å£"

3. **å…·ä½“æ€§ä¼˜äºæ¨¡ç³Š** (Specificity Over Vagueness)
   - ä½¿ç”¨å…·ä½“æ•°å­—ï¼Œé¿å…"å¼ºå¤§ã€ä¼˜ç§€ã€å…ˆè¿›"ç­‰è¯
   - ç¤ºä¾‹ï¼š"æ€§èƒ½æå‡300%" ä¼˜äº "æ€§èƒ½å¤§å¹…æå‡"

4. **æ ‡é¢˜å…¬å¼**ï¼ˆä»é¡¶çº§ç§‘æŠ€åª’ä½“å­¦ä¹ ï¼‰
   - "Companyå‘å¸ƒProductï¼Œæ ¸å¿ƒBenefit"
   - "çªç ´æ•°å­—ï¼šå…·ä½“æˆå°±"
   - "Industryå®ç°Milestoneï¼Œæ„å‘³ç€Impact"

5. **ä¸“ä¸šæœ¯è¯­å¤„ç†**
   - ä¿ç•™ä¸ç¿»è¯‘ï¼šLLMã€RAGã€Transformerã€Agentã€GPUã€APIã€SDK
   - ç¿»è¯‘ä½†ä¿ç•™è‹±æ–‡ï¼šäººå·¥æ™ºèƒ½(AI)ã€æœºå™¨å­¦ä¹ (ML)ã€æ·±åº¦å­¦ä¹ (DL)

6. **æ‘˜è¦ç²¾ç‚¼åŸåˆ™**
   - æ§åˆ¶åœ¨60-80å­—
   - çªå‡ºæ ¸å¿ƒä»·å€¼å’Œå½±å“
   - ä½¿ç”¨ä¸»åŠ¨è¯­æ€

ã€ä¼˜ç§€ç¿»è¯‘ç¤ºä¾‹ã€‘

è¾“å…¥: "OpenAI Announces GPT-5 With 1M Context Window, 300% Better Reasoning"
è¾“å‡º: {{"title": "OpenAIå‘å¸ƒGPT-5ï¼šæ”¯æŒ100ä¸‡tokensï¼Œæ¨ç†æå‡300%", "summary": "OpenAIæ¨å‡ºGPT-5ï¼Œä¸Šä¸‹æ–‡çª—å£æ‰©å±•è‡³100ä¸‡tokensï¼ˆå¯å¤„ç†æ•´æœ¬ä¹¦ï¼‰ï¼Œæ¨ç†èƒ½åŠ›æå‡300%"}

è¾“å…¥: "Meta Releases New Open Source LLM to Compete with GPT-4"
è¾“å‡º: {{"title": "Metaå¼€æºæ–°å¤§æ¨¡å‹ï¼Œæ€§èƒ½åª²ç¾GPT-4å¯å…è´¹å•†ç”¨", "summary": "Metaå‘å¸ƒå…¨æ–°å¼€æºLLMï¼Œæ€§èƒ½è¾¾åˆ°GPT-4æ°´å¹³ï¼Œä¼ä¸šå¯å…è´¹ç”¨äºå•†ä¸šäº§å“"}}

è¾“å…¥: "Google DeepMind's AlphaFold 3 Can Now Predict Protein Interactions"
è¾“å‡º: {{"title": "DeepMindæ¨å‡ºAlphaFold 3ï¼šå¯é¢„æµ‹è›‹ç™½è´¨ç›¸äº’ä½œç”¨", "summary": "DeepMindå‘å¸ƒAlphaFold 3ï¼Œçªç ´æ€§å‡çº§å¯é¢„æµ‹è›‹ç™½è´¨é—´ç›¸äº’ä½œç”¨ï¼ŒåŠ é€Ÿæ–°è¯ç ”å‘"}}

ã€è¾“å‡ºæ ¼å¼ã€‘
å¿…é¡»æ˜¯JSONå¯¹è±¡ï¼Œé”®åä¸ºåºå·ï¼ˆ"1", "2", "3"...ï¼‰ï¼š
{{
  "1": {{"title": "ä¸­æ–‡æ ‡é¢˜", "summary": "ä¸­æ–‡æ‘˜è¦ï¼ˆ60-80å­—ï¼‰"}},
  "2": {{"title": "ä¸­æ–‡æ ‡é¢˜", "summary": "ä¸­æ–‡æ‘˜è¦ï¼ˆ60-80å­—ï¼‰"}}
}}

ã€å¾…ç¿»è¯‘æ–°é—»ã€‘
{chr(10).join(news_items)}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸ŠåŸåˆ™ï¼Œç›´æ¥è¾“å‡ºJSONæ ¼å¼ï¼š"""

        try:
            system_msg = """ä½ æ˜¯TechCrunchã€The Vergeã€Wiredç­‰é¡¶çº§ç§‘æŠ€åª’ä½“çš„ä¸­æ–‡ä¸»ç¼–ã€‚

ã€å¿…é¡»éµå®ˆã€‘
1. æ‰€æœ‰æ ‡é¢˜å’Œæ‘˜è¦å¿…é¡»æ˜¯ä¸­æ–‡
2. è¾“å‡ºå¿…é¡»æ˜¯JSONæ ¼å¼ï¼Œé”®åä¸ºåºå·ï¼š"1", "2", "3"...
3. ä¿ç•™ä¸“ä¸šæœ¯è¯­ä¸ç¿»è¯‘ï¼šLLMã€RAGã€Transformerã€Agentã€GPUç­‰
4. çªå‡ºåˆ©ç›Šå’Œä»·å€¼ï¼Œè€Œä¸ä»…ä»…æ˜¯åŠŸèƒ½"""

            response = self.llm.invoke([
                SystemMessage(content=system_msg),
                HumanMessage(content=prompt)
            ])
            result = response.content.strip()

            # æ¸…ç†markdownä»£ç å—
            if result.startswith('```'):
                result = result.split('```', 2)[1] if '```' in result[3:] else result
                result = result.strip()
                if result.startswith('json'):
                    result = result[4:].strip()
                if result.endswith('```'):
                    result = result[:-3].strip()

            # è§£æJSON
            translated_data = json.loads(result)

            # å¤„ç†ä¸åŒæ ¼å¼
            if isinstance(translated_data, list):
                for i, item in enumerate(items):
                    if i < len(translated_data):
                        item["title_cn"] = translated_data[i].get("title", item.get("title", ""))
                        item["summary_cn"] = translated_data[i].get("summary", item.get("description", ""))[:150]
                    else:
                        item["title_cn"] = item.get("title", "")
                        item["summary_cn"] = item.get("description", "")[:150]
            else:
                for i, item in enumerate(items):
                    key = str(i + 1)
                    if key in translated_data:
                        item["title_cn"] = translated_data[key]["title"]
                        item["summary_cn"] = translated_data[key].get("summary", item.get("description", ""))[:150]
                    else:
                        item["title_cn"] = item.get("title", "")
                        item["summary_cn"] = item.get("description", "")[:150]

            self.log(f"æ‰¹é‡ç¿»è¯‘å®Œæˆ: {len(items)}æ¡")
            return items

        except json.JSONDecodeError as e:
            self.log(f"JSONè§£æå¤±è´¥: {e}ï¼ŒLLMè¿”å›: {result[:200]}...", "WARNING")
            for item in items:
                item["title_cn"] = item.get("title", "")
                item["summary_cn"] = item.get("description", "")[:150]
            return items
        except Exception as e:
            self.log(f"æ‰¹é‡ç¿»è¯‘å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹å†…å®¹", "WARNING")
            for item in items:
                item["title_cn"] = item.get("title", "")
                item["summary_cn"] = item.get("description", "")[:150]
            return items

    def _generate_enhanced_analysis_v8(self, item: Dict) -> Dict:
        """ä¸ºå•æ¡æ–°é—»ç”Ÿæˆå¢å¼ºåˆ†æ v8.0ï¼ˆåº”ç”¨ copy-editing åŸåˆ™ï¼‰"""
        if not self.llm:
            return {"background": "", "impact": "", "tags": []}

        title = item.get("title_cn", item.get("title", ""))
        summary = item.get("summary_cn", item.get("description", ""))

        prompt = f"""ä½ æ˜¯èµ„æ·±ç§‘æŠ€è¡Œä¸šåˆ†æå¸ˆï¼ŒåŸºäºä»¥ä¸‹æ–°é—»ç”Ÿæˆæ·±åº¦åˆ†æã€‚

ã€æ–°é—»ã€‘
æ ‡é¢˜: {title}
æ‘˜è¦: {summary}

è¯·ç”Ÿæˆï¼š

1. **background** (120-150å­—): èƒŒæ™¯ä»‹ç»
   - ä½¿ç”¨"æ‰€ä»¥..."ã€"åœ¨æ­¤ä¹‹å‰..."ç­‰è¿æ¥è¯å¢å¼ºå¯è¯»æ€§
   - çªå‡ºæŠ€æœ¯å‘å±•è„‰ç»œ
   - å¸®åŠ©è¯»è€…ç†è§£ä¸Šä¸‹æ–‡

2. **impact** (120-150å­—): è¡Œä¸šå½±å“åˆ†æ
   - ä½¿ç”¨"è¿™æ„å‘³ç€..."ã€"å…·ä½“æ¥è¯´..."ç­‰å…·ä½“åŒ–è¡¨è¾¾
   - é¿å…"é‡å¤§"ã€"æ·±è¿œ"ç­‰ç©ºæ´è¯æ±‡
   - è¯´æ˜å¯¹ä¸åŒç¾¤ä½“çš„å…·ä½“å½±å“

3. **tags** (3-5ä¸ªå…³é”®è¯): ç”¨äºåˆ†ç±»å’Œæ£€ç´¢
   - é€‰æ‹©ç”¨æˆ·ä¼šæœç´¢çš„æœ¯è¯­
   - é¿å…è¿‡äºå®½æ³›çš„è¯

ã€å†™ä½œåŸåˆ™ã€‘
- å…·ä½“æ€§ï¼šä½¿ç”¨å…·ä½“æ•°å­—å’Œä¾‹å­
- åˆ©ç›Šå¯¼å‘ï¼šè¯´æ˜"è¿™å¯¹è¯»è€…æ„å‘³ç€ä»€ä¹ˆ"
- å¯è¯»æ€§ï¼šæ®µè½æ§åˆ¶åœ¨3-4å¥è¯

ç›´æ¥è¾“å‡ºJSONæ ¼å¼ï¼š"""

        try:
            response = self.llm.invoke([HumanMessage(content=prompt)])
            result = response.content.strip()

            if result.startswith('```'):
                result = result.split('```', 2)[1] if '```' in result[3:] else result
                result = result.strip()
                if result.startswith('json'):
                    result = result[4:].strip()
                if result.endswith('```'):
                    result = result[:-3].strip()

            analysis = json.loads(result)
            return {
                "background": analysis.get("background", ""),
                "impact": analysis.get("impact", ""),
                "tags": analysis.get("tags", [])
            }
        except Exception as e:
            self.log(f"å¢å¼ºåˆ†æç”Ÿæˆå¤±è´¥: {e}", "DEBUG")
            return {"background": "", "impact": "", "tags": []}

    def _extract_core_insights_v8(self, items: List[Dict]) -> List[str]:
        """æå–æ ¸å¿ƒæ´å¯Ÿ v8.0ï¼ˆåº”ç”¨ content-research-writer åŸåˆ™ï¼‰"""
        if not self.llm or not items:
            return []

        # é€‰æ‹©æœ€é‡è¦çš„10æ¡æ–°é—»
        top_items = sorted(items, key=lambda x: x.get("importance_score", 0), reverse=True)[:10]

        news_summary = "\n".join([
            f"- {item.get('title_cn', item.get('title', ''))}"
            for item in top_items
        ])

        prompt = f"""ä½ æ˜¯èµ„æ·±ç§‘æŠ€è¡Œä¸šè§‚å¯Ÿå®¶ï¼ŒåŸºäºä»Šæ—¥AIæ–°é—»æå–æ ¸å¿ƒæ´å¯Ÿã€‚

ã€ä»Šæ—¥é‡è¦æ–°é—»ã€‘
{news_summary}

è¯·ç”Ÿæˆ3-5æ¡æ ¸å¿ƒæ´å¯Ÿï¼Œæ¯æ¡40-60å­—ï¼Œè¦æ±‚ï¼š

1. **æ•æ‰è¶‹åŠ¿**ï¼šè¯†åˆ«ä¸åŒæ–°é—»èƒŒåçš„å…±åŒè¶‹åŠ¿
2. **å…³è”åˆ†æ**ï¼šå‘ç°çœ‹ä¼¼æ— å…³äº‹ä»¶ä¹‹é—´çš„è”ç³»
3. **é¢„è§æœªæ¥**ï¼šåŸºäºå½“å‰åŠ¨æ€æ¨æ–­å‘å±•æ–¹å‘
4. **å…·ä½“è¡¨è¾¾**ï¼šé¿å…"é‡è¦"ã€"çªç ´"ç­‰ç©ºæ´è¯æ±‡
5. **åˆ©ç›Šå¯¼å‘**ï¼šè¯´æ˜"è¿™å¯¹è¡Œä¸š/è¯»è€…æ„å‘³ç€ä»€ä¹ˆ"

ã€æ´å¯Ÿç¤ºä¾‹ã€‘
âŒ "AIæŠ€æœ¯æŒç»­å‘å±•ï¼Œå¯¹è¡Œä¸šäº§ç”Ÿé‡å¤§å½±å“"
âœ… "ä»GPT-5åˆ°å¼€æºæ¨¡å‹ç«äº‰ï¼Œå¤§æ¨¡å‹è¿›å…¥'æ€§èƒ½+æˆæœ¬'åŒè½®é©±åŠ¨é˜¶æ®µï¼Œä¼ä¸šé€‰å‹æ›´åŠ¡å®"

ã€è¾“å‡ºæ ¼å¼ã€‘
ç›´æ¥è¾“å‡ºJSONæ•°ç»„æ ¼å¼ï¼Œä¸è¦æœ‰ä»»ä½•é¢å¤–è¯´æ˜ï¼š"""

        try:
            response = self.llm.invoke([HumanMessage(content=prompt)])
            result = response.content.strip()

            if result.startswith('```'):
                result = result.split('```', 2)[1] if '```' in result[3:] else result
                result = result.strip()
                if result.startswith('json'):
                    result = result[4:].strip()
                if result.endswith('```'):
                    result = result[:-3].strip()

            insights = json.loads(result)
            return insights if isinstance(insights, list) else []
        except Exception as e:
            self.log(f"æ ¸å¿ƒæ´å¯Ÿæå–å¤±è´¥: {e}", "DEBUG")
            return []

    def _identify_trending_topics(self, items: List[Dict]) -> List[Dict]:
        """è¯†åˆ«çƒ­é—¨è¯é¢˜"""
        # ä»æ‰€æœ‰æ ‡ç­¾ä¸­ç»Ÿè®¡çƒ­é—¨è¯é¢˜
        all_tags = []
        for item in items:
            tags = item.get("tags", [])
            all_tags.extend(tags)

        # å¦‚æœæ²¡æœ‰æ ‡ç­¾ï¼Œä»æ ‡é¢˜ä¸­æå–å…³é”®è¯
        if not all_tags:
            for item in items:
                title = item.get("title_cn", item.get("title", ""))
                keywords = ["GPT", "LLM", "RAG", "Agent", "Transformer", "AI", "å¤§æ¨¡å‹", "å¼€æº", "å¤šæ¨¡æ€"]
                for kw in keywords:
                    if kw in title:
                        all_tags.append(kw)

        # ç»Ÿè®¡è¯é¢‘
        tag_counts = Counter(all_tags)

        # è½¬æ¢ä¸ºçƒ­é—¨è¯é¢˜åˆ—è¡¨
        trending = []
        for tag, count in tag_counts.most_common(10):
            if count >= 2:
                trending.append({
                    "name": tag,
                    "count": count,
                    "trend": "rising" if count >= 4 else "stable"
                })

        return trending[:5]

    def _generate_deep_observation(self, items: List[Dict], core_insights: List[str]) -> str:
        """ç”Ÿæˆæ·±åº¦è§‚å¯Ÿï¼ˆæ–°å¢éƒ¨åˆ†ï¼‰"""
        if not self.llm or not items:
            return ""

        top_items = sorted(items, key=lambda x: x.get("importance_score", 0), reverse=True)[:5]
        news_list = "\n".join([
            f"- {item.get('title_cn', item.get('title', ''))}"
            for item in top_items
        ])

        prompt = f"""ä½ æ˜¯TechCrunchã€Wiredç­‰é¡¶çº§ç§‘æŠ€åª’ä½“çš„ä¸“æ ä½œå®¶ã€‚

åŸºäºä»Šæ—¥AIæ–°é—»å’Œæ ¸å¿ƒæ´å¯Ÿï¼Œå†™ä¸€ç¯‡350-450å­—çš„æ·±åº¦è§‚å¯Ÿæ–‡ç« ã€‚

ã€å‚è€ƒçš„æ ¸å¿ƒæ´å¯Ÿã€‘
{chr(10).join(f"- {insight}" for insight in core_insights)}

ã€ä»Šæ—¥é‡è¦æ–°é—»ã€‘
{news_list}

ã€å†™ä½œè¦æ±‚ã€‘
1. **å¼ºHookå¼€å¤´**ï¼šç”¨æ•°æ®ã€åå¸¸è¯†è§‚ç‚¹æˆ–å…·ä½“åœºæ™¯å¼€å¤´
2. **å…·ä½“åŒ–è¡¨è¾¾**ï¼šé¿å…"é‡å¤§"ã€"çªç ´"ç­‰ç©ºæ´è¯æ±‡
3. **å…³è”åˆ†æ**ï¼šå‘ç°ä¸åŒæ–°é—»ä¹‹é—´çš„å†…åœ¨è”ç³»
4. **è¡Œä¸šè§†è§’**ï¼šä»äº§ä¸šã€æŠ€æœ¯ã€åº”ç”¨å¤šè§’åº¦åˆ†æ
5. **å¯è¯»æ€§**ï¼šæ®µè½æ§åˆ¶åœ¨3-5å¥è¯ï¼Œä½¿ç”¨è¿æ¥è¯
6. **é•¿åº¦**ï¼š350-450å­—

ã€ä¼˜ç§€å¼€å¤´ç¤ºä¾‹ã€‘
âŒ "ä»Šå¤©AIè¡Œä¸šæœ‰å¾ˆå¤šé‡è¦æ–°é—»..."
âœ… "OpenAIã€Metaã€Googleä¸‰å®¶åœ¨åŒä¸€å¤©å‘å¸ƒæ–°æ¨¡å‹ï¼Œè¿™ä¸æ˜¯å·§åˆï¼Œè€Œæ˜¯å¤§æ¨¡å‹ç«äº‰è¿›å…¥æ–°é˜¶æ®µçš„ä¿¡å·"

ç›´æ¥è¾“å‡ºæ–‡ç« å†…å®¹ï¼ˆä¸è¦æ ‡é¢˜ï¼Œä¸è¦é¢å¤–è¯´æ˜ï¼‰ï¼š"""

        try:
            response = self.llm.invoke([HumanMessage(content=prompt)])
            result = response.content.strip()
            return result
        except Exception as e:
            self.log(f"æ·±åº¦è§‚å¯Ÿç”Ÿæˆå¤±è´¥: {e}", "DEBUG")
            return ""

    def _generate_markdown_v8(
        self,
        scored_trends: Dict[str, Dict],
        editors_pick: List[Dict],
        core_insights: List[str],
        trending_topics: List[Dict],
        deep_observation: str,
        source_status: Dict[str, Any],
        today: datetime,
        issue_number: str,
        total_count: int
    ) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼ç®€æŠ¥ v8.0"""

        parts = []

        # ========== Header ==========
        parts.append(f"# AIæ¯æ—¥çƒ­ç‚¹ Â· {today.strftime('%Yå¹´%mæœˆ%dæ—¥')}\n\n")
        parts.append(f"> **æœŸå·**: #{issue_number}  |  **é˜…è¯»æ—¶é—´**: ~{max(5, total_count * 12 // 60)}åˆ†é’Ÿ  |  **ç²¾é€‰**: {total_count}æ¡\n\n")
        parts.append("---\n\n")

        # ========== æ ¸å¿ƒæ´å¯Ÿ ==========
        if core_insights:
            parts.append("## ğŸ’¡ æ ¸å¿ƒæ´å¯Ÿ\n\n")
            for insight in core_insights:
                parts.append(f"- {insight}\n")
            parts.append("\n---\n\n")

        # ========== æ·±åº¦è§‚å¯Ÿï¼ˆæ–°å¢ï¼‰ ==========
        if deep_observation:
            parts.append("## ğŸ“° æ·±åº¦è§‚å¯Ÿ\n\n")
            parts.append(f"{deep_observation}\n\n")
            parts.append("---\n\n")

        # ========== ç¼–è¾‘ç²¾é€‰ ==========
        if editors_pick:
            parts.append("## â­ ç¼–è¾‘ç²¾é€‰ (Editor's Picks)\n\n")

            for i, item in enumerate(editors_pick, 1):
                title = item.get("title_cn", item.get("title", ""))
                summary = item.get("summary_cn", item.get("description", ""))
                source = item.get("source", "")
                url = item.get("url", "")
                score = item.get("importance_score", 0)
                background = item.get("background", "")
                impact = item.get("impact", "")

                parts.append(f"### {i}. {title}\n\n")
                parts.append(f"> ğŸ“° **{source}**  |  â­ **é‡è¦æ€§**: {int(score)}/100  |  ğŸ”— [åŸæ–‡é“¾æ¥]({url})\n\n")

                if summary:
                    parts.append(f"**æ ¸å¿ƒå†…å®¹**: {summary}\n\n")

                if background:
                    parts.append(f"**èƒŒæ™¯**: {background}\n\n")

                if impact:
                    parts.append(f"**è¡Œä¸šå½±å“**: {impact}\n\n")

                parts.append("---\n\n")

        # ========== çƒ­é—¨è¯é¢˜ ==========
        if trending_topics:
            parts.append("## ğŸ“Š çƒ­é—¨è¯é¢˜\n\n")
            parts.append("| è¯é¢˜ | ç›¸å…³æ–°é—» | è¶‹åŠ¿ |\n")
            parts.append("|------|---------|------|\n")
            for topic in trending_topics:
                trend_icon = "ğŸ“ˆ ä¸Šå‡" if topic.get("trend") == "rising" else "â¡ï¸ ç¨³å®š"
                parts.append(f"| {topic['name']} | {topic['count']}æ¡ | {trend_icon} |\n")
            parts.append("\n---\n\n")

        # ========== åˆ†ç±»çƒ­ç‚¹ ==========
        parts.append("## ğŸ” åˆ†ç±»çƒ­ç‚¹\n\n")

        for cat_name, cat_data in scored_trends.items():
            items = cat_data.get("items", [])
            if not items:
                continue

            icon = cat_data.get("icon", "ğŸ“Œ")
            name = self._get_category_name(cat_name)
            parts.append(f"### {icon} {name} ({len(items)}æ¡)\n\n")

            for i, item in enumerate(items, 1):
                title = item.get("title_cn", item.get("title", ""))
                summary = item.get("summary_cn", item.get("description", ""))
                source = item.get("source", "")
                url = item.get("url", "")
                score = item.get("importance_score", 0)
                background = item.get("background", "")
                impact = item.get("impact", "")

                parts.append(f"#### {i}. {title}\n\n")
                parts.append(f"> ğŸ“° **{source}**  |  â­ **é‡è¦æ€§**: {int(score)}/100  |  ğŸ”— [åŸæ–‡]({url})\n\n")

                if summary:
                    parts.append(f"**æ‘˜è¦**: {summary}\n\n")

                if background:
                    parts.append(f"**èƒŒæ™¯**: {background}\n\n")

                if impact:
                    parts.append(f"**å½±å“**: {impact}\n\n")

                parts.append("---\n\n")

        # ========== æ•°æ®æ¥æº ==========
        parts.append("## ğŸ“š æ•°æ®æ¥æº\n\n")
        success_sources = [s for s, status in source_status.items() if status.get("success", False)]
        for source in success_sources:
            count = source_status[source].get("count", 0)
            parts.append(f"- **{source}**: {count}æ¡\n")
        parts.append("\n---\n\n")

        # ========== Footer ==========
        parts.append("*ğŸ¤– Generated by [ContentForge AI](https://github.com/devfoxaicn/content-forge-ai)*\n")

        return "".join(parts)

    def _generate_json_v8(
        self,
        scored_trends: Dict[str, Dict],
        editors_pick: List[Dict],
        core_insights: List[str],
        trending_topics: List[Dict],
        deep_observation: str,
        source_status: Dict[str, Any],
        today: datetime,
        issue_number: str,
        total_count: int,
        markdown_content: str
    ) -> Dict[str, Any]:
        """ç”ŸæˆJSONæ ¼å¼æ•°æ® v8.0"""

        # æ„å»ºåˆ†ç±»æ•°æ®
        categories = []
        category_id_map = {
            "ğŸ“ˆ è¡Œä¸šåŠ¨æ€": ("industry", "äº§ä¸šåŠ¨æ€", "ğŸ“ˆ"),
            "ğŸ“ å­¦æœ¯å‰æ²¿": ("academic", "å­¦æœ¯å‰æ²¿", "ğŸ“"),
            "ğŸ”¬ æŠ€æœ¯åˆ›æ–°": ("tech", "æŠ€æœ¯åˆ›æ–°", "ğŸ”¬"),
            "ğŸ› ï¸ AIå·¥å…·/äº§å“": ("product", "äº§å“å·¥å…·", "ğŸ› ï¸"),
            "ğŸ’¼ AIåº”ç”¨": ("application", "è¡Œä¸šåº”ç”¨", "ğŸ’¼")
        }

        for cat_name, cat_data in scored_trends.items():
            cat_id, name, icon = category_id_map.get(cat_name, (cat_name, cat_name, "ğŸ“Œ"))
            items = []
            for item in cat_data.get("items", []):
                url_hash = hash(item.get("url", "")) & 0xffffff
                items.append({
                    "id": f"{cat_id}_{url_hash:06x}",
                    "title": item.get("title", ""),
                    "title_cn": item.get("title_cn", ""),
                    "summary": item.get("description", "")[:200],
                    "summary_cn": item.get("summary_cn", "")[:200],
                    "url": item.get("url", ""),
                    "source": item.get("source", ""),
                    "category": name,
                    "importance_score": item.get("importance_score", 0),
                    "published_at": item.get("timestamp", ""),
                    "tags": item.get("tags", []),
                    "background": item.get("background", ""),
                    "impact": item.get("impact", "")
                })

            categories.append({
                "id": cat_id,
                "name": name,
                "icon": icon,
                "count": len(items),
                "items": items
            })

        # æ„å»ºç¼–è¾‘ç²¾é€‰
        editors_pick_data = []
        for item in editors_pick:
            editors_pick_data.append({
                "id": item.get("id", ""),
                "title": item.get("title", ""),
                "title_cn": item.get("title_cn", ""),
                "summary": item.get("description", "")[:200],
                "summary_cn": item.get("summary_cn", "")[:200],
                "url": item.get("url", ""),
                "source": item.get("source", ""),
                "category": self._get_category_name(item.get("category", "")),
                "importance_score": item.get("importance_score", 0),
                "published_at": item.get("timestamp", ""),
                "tags": item.get("tags", []),
                "background": item.get("background", ""),
                "impact": item.get("impact", ""),
                "pick_rank": item.get("pick_rank", 0)
            })

        # æ„å»ºæ•°æ®æ¥æº
        sources = []
        for source, status in source_status.items():
            if status.get("success", False):
                sources.append({
                    "name": source,
                    "count": status.get("count", 0)
                })

        return {
            "metadata": {
                "title": f"AIæ¯æ—¥çƒ­ç‚¹ Â· {today.strftime('%Yå¹´%mæœˆ%dæ—¥')}",
                "issue_number": issue_number,
                "publish_date": today.strftime("%Y-%m-%d"),
                "generated_at": today.isoformat(),
                "word_count": len(markdown_content),
                "reading_time": f"{max(5, total_count * 12 // 60)}åˆ†é’Ÿ",
                "total_items": total_count,
                "version": "v8.0"
            },
            "editors_pick": editors_pick_data,
            "categories": categories,
            "core_insights": core_insights,
            "deep_observation": deep_observation,  # æ–°å¢
            "trending_topics": trending_topics,
            "sources": sources,
            "markdown_content": markdown_content
        }

    def _get_category_name(self, key: str) -> str:
        """è·å–åˆ†ç±»ä¸­æ–‡å"""
        mapping = {
            "ğŸ“ˆ è¡Œä¸šåŠ¨æ€": "äº§ä¸šåŠ¨æ€",
            "ğŸ“ å­¦æœ¯å‰æ²¿": "å­¦æœ¯å‰æ²¿",
            "ğŸ”¬ æŠ€æœ¯åˆ›æ–°": "æŠ€æœ¯åˆ›æ–°",
            "ğŸ› ï¸ AIå·¥å…·/äº§å“": "äº§å“å·¥å…·",
            "ğŸ’¼ AIåº”ç”¨": "è¡Œä¸šåº”ç”¨"
        }
        return mapping.get(key, key)
