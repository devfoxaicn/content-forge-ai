"""
çœŸå®AIçƒ­ç‚¹åˆ†æAgent - å¤šæ•°æ®æºèšåˆç‰ˆæœ¬
æ•´åˆHacker Newsã€arXivã€GitHub Trendingã€Redditç­‰å…è´¹æ•°æ®æº
"""

from typing import Dict, Any, List, Optional
import json
import os
import requests
from datetime import datetime, timedelta
from src.agents.base import BaseAgent
from src.utils.storage import get_storage


class RealAITrendAnalyzerAgent(BaseAgent):
    """çœŸå®çš„AIçƒ­ç‚¹åˆ†æAgent - ä½¿ç”¨å…è´¹API"""

    def __init__(self, config: Dict[str, Any], prompts: Dict[str, Any]):
        super().__init__(config, prompts)
        self.mock_mode = config.get("agents", {}).get("ai_trend_analyzer", {}).get("mock_mode", False)

        # ä½¿ç”¨æ–°çš„å­˜å‚¨ç®¡ç†å™¨
        self.storage = get_storage(config.get("storage", {}).get("base_dir", "data"))

        # æ•°æ®æºé…ç½®
        sources_config = config.get("agents", {}).get("ai_trend_analyzer", {}).get("sources", [])
        self.sources = {
            "hackernews": "hackernews" in sources_config,
            "arxiv": "arxiv" in sources_config,
            "github_trending": "github" in sources_config,
            "reddit": "reddit" in sources_config,
            "huggingface": "huggingface" in sources_config,
            "stackoverflow": "stackoverflow" in sources_config,
            "kaggle": "kaggle" in sources_config,
            "newsapi": "newsapi" in sources_config,
            "devto": "devto" in sources_config,
            "pypi": "pypi" in sources_config,
            "github_topics": "github_topics" in sources_config
        }

        # NewsAPIé…ç½®ï¼ˆéœ€è¦APIå¯†é’¥ï¼‰
        self.newsapi_key = os.getenv("NEWSAPI_KEY", None)

    def execute(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡ŒAIçƒ­ç‚¹åˆ†æ

        Args:
            state: å½“å‰å·¥ä½œæµçŠ¶æ€

        Returns:
            Dict[str, Any]: æ›´æ–°åçš„çŠ¶æ€
        """
        self.log(f"å¼€å§‹åˆ†æAIæŠ€æœ¯çƒ­ç‚¹ï¼Œé¢†åŸŸ: {state['topic']}")

        try:
            topic = state['topic']

            # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨mockæ¨¡å¼
            if self.mock_mode:
                self.log("ä½¿ç”¨Mockæ¨¡å¼ï¼ˆæ¨¡æ‹Ÿæ•°æ®ï¼‰")
                from src.agents.ai_trend_analyzer import AITrendAnalyzerAgent
                mock_agent = AITrendAnalyzerAgent(self.config, self.prompts)
                hot_topics = mock_agent._get_mock_ai_trends(topic)
            else:
                self.log("ä½¿ç”¨çœŸå®APIæ¨¡å¼ï¼ˆå¤šæ•°æ®æºèšåˆï¼‰")
                hot_topics = self._get_real_ai_trends(topic)

            self.log(f"æˆåŠŸåˆ†æå‡º {len(hot_topics)} ä¸ªçƒ­ç‚¹è¯é¢˜")

            # ä¿å­˜çƒ­ç‚¹åˆ†æç»“æœ
            self._save_trends(topic, hot_topics)

            # é€‰æ‹©çƒ­åº¦æœ€é«˜çš„è¯é¢˜
            selected_topic = hot_topics[0]
            self.log(f"é€‰æ‹©çƒ­ç‚¹è¯é¢˜: {selected_topic['title']}")

            return {
                **state,
                "ai_hot_topics": hot_topics,
                "selected_ai_topic": selected_topic,
                "current_step": "ai_trend_analyzer_completed"
            }
        except Exception as e:
            self.log(f"AIçƒ­ç‚¹åˆ†æå¤±è´¥: {str(e)}", "ERROR")
            return {
                **state,
                "error_message": f"AIçƒ­ç‚¹åˆ†æå¤±è´¥: {str(e)}",
                "current_step": "ai_trend_analyzer_failed"
            }

    def _get_real_ai_trends(self, topic: str = None) -> List[Dict[str, Any]]:
        """
        ä»å¤šä¸ªå…è´¹æ•°æ®æºè·å–çœŸå®AIçƒ­ç‚¹ï¼ˆæ— éœ€å…³é”®è¯è¿‡æ»¤ï¼‰

        Args:
            topic: é¢†åŸŸå‚æ•°ï¼ˆå·²å¼ƒç”¨ï¼Œä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼‰

        Returns:
            List[Dict[str, Any]]: çƒ­ç‚¹è¯é¢˜åˆ—è¡¨
        """
        all_trends = []

        # 1. Hacker News
        if self.sources["hackernews"]:
            try:
                hn_trends = self._get_hacker_news_trends()
                all_trends.extend(hn_trends)
                self.log(f"Hacker News: è·å– {len(hn_trends)} æ¡çƒ­ç‚¹")
            except Exception as e:
                self.log(f"Hacker Newsè·å–å¤±è´¥: {e}", "WARNING")

        # 2. arXivè®ºæ–‡
        if self.sources["arxiv"]:
            try:
                arxiv_trends = self._get_arxiv_papers()
                all_trends.extend(arxiv_trends)
                self.log(f"arXiv: è·å– {len(arxiv_trends)} æ¡çƒ­ç‚¹")
            except Exception as e:
                self.log(f"arXivè·å–å¤±è´¥: {e}", "WARNING")

        # 3. GitHub Trending
        if self.sources["github_trending"]:
            try:
                github_trends = self._get_github_trending()
                all_trends.extend(github_trends)
                self.log(f"GitHub Trending: è·å– {len(github_trends)} æ¡çƒ­ç‚¹")
            except Exception as e:
                self.log(f"GitHub Trendingè·å–å¤±è´¥: {e}", "WARNING")

        # 4. Reddit
        if self.sources["reddit"]:
            try:
                reddit_trends = self._get_reddit_trends()
                all_trends.extend(reddit_trends)
                self.log(f"Reddit: è·å– {len(reddit_trends)} æ¡çƒ­ç‚¹")
            except Exception as e:
                self.log(f"Redditè·å–å¤±è´¥: {e}", "WARNING")

        # 5. Hugging Face Trending Models
        if self.sources["huggingface"]:
            try:
                hf_trends = self._get_huggingface_trends()
                all_trends.extend(hf_trends)
                self.log(f"Hugging Face: è·å– {len(hf_trends)} æ¡çƒ­ç‚¹")
            except Exception as e:
                self.log(f"Hugging Faceè·å–å¤±è´¥: {e}", "WARNING")

        # 6. Stack Overflow Hot Questions
        if self.sources["stackoverflow"]:
            try:
                so_trends = self._get_stackoverflow_trends()
                all_trends.extend(so_trends)
                self.log(f"Stack Overflow: è·å– {len(so_trends)} æ¡çƒ­ç‚¹")
            except Exception as e:
                self.log(f"Stack Overflowè·å–å¤±è´¥: {e}", "WARNING")

        # 7. Kaggleç«èµ›å’Œæ•°æ®é›†
        if self.sources["kaggle"]:
            try:
                kaggle_trends = self._get_kaggle_trends()
                all_trends.extend(kaggle_trends)
                self.log(f"Kaggle: è·å– {len(kaggle_trends)} æ¡çƒ­ç‚¹")
            except Exception as e:
                self.log(f"Kaggleè·å–å¤±è´¥: {e}", "WARNING")

        # 8. NewsAPIç§‘æŠ€æ–°é—»
        if self.sources["newsapi"]:
            try:
                news_trends = self._get_newsapi_trends()
                all_trends.extend(news_trends)
                self.log(f"NewsAPI: è·å– {len(news_trends)} æ¡çƒ­ç‚¹")
            except Exception as e:
                self.log(f"NewsAPIè·å–å¤±è´¥: {e}", "WARNING")

        # 9. Dev.toå¼€å‘è€…åšå®¢
        if self.sources["devto"]:
            try:
                devto_trends = self._get_devto_trends()
                all_trends.extend(devto_trends)
                self.log(f"Dev.to: è·å– {len(devto_trends)} æ¡çƒ­ç‚¹")
            except Exception as e:
                self.log(f"Dev.toè·å–å¤±è´¥: {e}", "WARNING")

        # 10. PyPIçƒ­é—¨åŒ…
        if self.sources["pypi"]:
            try:
                pypi_trends = self._get_pypi_trends()
                all_trends.extend(pypi_trends)
                self.log(f"PyPI: è·å– {len(pypi_trends)} æ¡çƒ­ç‚¹")
            except Exception as e:
                self.log(f"PyPIè·å–å¤±è´¥: {e}", "WARNING")

        # 11. GitHub Topicsï¼ˆè¡Œä¸šåº”ç”¨ï¼‰
        if self.sources["github_topics"]:
            try:
                topics_trends = self._get_github_topics_trends()
                all_trends.extend(topics_trends)
                self.log(f"GitHub Topics: è·å– {len(topics_trends)} æ¡çƒ­ç‚¹")
            except Exception as e:
                self.log(f"GitHub Topicsè·å–å¤±è´¥: {e}", "WARNING")

        # æŒ‰ç»¼åˆçƒ­åº¦è¯„åˆ†æ’åº
        all_trends.sort(key=lambda x: x.get("heat_score", 0), reverse=True)

        # å»é‡ï¼ˆåŸºäºæ ‡é¢˜ç›¸ä¼¼åº¦ï¼‰
        all_trends = self._deduplicate_trends(all_trends)

        # è¿”å›Top 10
        return all_trends[:10]

    def _get_hacker_news_trends(self) -> List[Dict[str, Any]]:
        """è·å–Hacker Newsçƒ­é—¨æŠ€æœ¯è¯é¢˜ï¼ˆç›´æ¥è·å–Top 30ï¼‰"""
        try:
            # è·å–çƒ­é—¨æ•…äº‹IDåˆ—è¡¨
            stories_url = "https://hacker-news.firebaseio.com/v0/topstories.json"
            response = requests.get(stories_url, timeout=10)
            story_ids = response.json()[:30]  # å–å‰30ä¸ª

            trends = []

            for story_id in story_ids:
                try:
                    # è·å–æ•…äº‹è¯¦æƒ…
                    item_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
                    item = requests.get(item_url, timeout=5).json()

                    if not item or "url" not in item:
                        continue

                    title = item.get("title", "")

                    # è®¡ç®—çƒ­åº¦è¯„åˆ†
                    score = item.get("score", 0)
                    comments = item.get("descendants", 0)
                    heat_score = score * 2 + comments

                    trends.append({
                        "title": title,
                        "description": item.get("text", title)[:200],
                        "url": item.get("url", ""),
                        "source": "Hacker News",
                        "timestamp": datetime.fromtimestamp(item["time"]).strftime("%Y-%m-%d %H:%M"),
                        "metrics": {
                            "upvotes": score,
                            "comments": comments
                        },
                        "heat_score": heat_score,
                        "tags": ["æŠ€æœ¯æ–°é—»", "HN"]
                    })
                except Exception as e:
                    self.log(f"è·å–HNæ•…äº‹ {story_id} å¤±è´¥: {e}", "WARNING")
                    continue

            return trends
        except Exception as e:
            self.log(f"Hacker News APIè°ƒç”¨å¤±è´¥: {e}", "ERROR")
            return []

    def _get_arxiv_papers(self) -> List[Dict[str, Any]]:
        """è·å–arXivæœ€æ–°AIè®ºæ–‡ï¼ˆç›´æ¥è·å–AIç›¸å…³åˆ†ç±»ï¼‰"""
        try:
            import arxiv

            # æœç´¢AIå’Œè®¡ç®—æœºç§‘å­¦ç›¸å…³åˆ†ç±»
            query = "cat:cs.AI OR cat:cs.CL OR cat:cs.LG OR cat:cs.CV OR cat:cs.NE"

            # æœç´¢æœ€è¿‘7å¤©çš„è®ºæ–‡
            search = arxiv.Search(
                query=query,
                max_results=20,
                sort_by=arxiv.SortCriterion.SubmittedDate,
                sort_order=arxiv.SortOrder.Descending
            )

            trends = []
            cutoff_date = datetime.now() - timedelta(days=7)

            for result in search.results():
                # æ£€æŸ¥è®ºæ–‡å‘å¸ƒæ—¶é—´ï¼ˆæœ€è¿‘7å¤©ï¼‰
                pub_date = result.published.replace(tzinfo=None)
                if pub_date < cutoff_date:
                    continue

                # è®¡ç®—çƒ­åº¦è¯„åˆ†ï¼ˆæ–°è®ºæ–‡åŠ åˆ†ï¼‰
                days_ago = (datetime.now() - pub_date).days
                heat_score = 100 - days_ago * 10  # è¶Šæ–°åˆ†æ•°è¶Šé«˜

                trends.append({
                    "title": result.title,
                    "description": result.summary[:300],
                    "url": result.entry_id,
                    "source": "arXiv",
                    "timestamp": pub_date.strftime("%Y-%m-%d"),
                    "metrics": {
                        "authors": [a.name for a in result.authors[:3]],
                        "categories": result.categories,
                        "days_ago": days_ago
                    },
                    "heat_score": heat_score,
                    "tags": result.categories[:2] + ["è®ºæ–‡", "å­¦æœ¯"]
                })

            return trends
        except ImportError:
            self.log("arXivåº“æœªå®‰è£…ï¼Œè·³è¿‡arXivæ•°æ®æºã€‚è¿è¡Œ: pip install arxiv", "WARNING")
            return []
        except Exception as e:
            self.log(f"arXiv APIè°ƒç”¨å¤±è´¥: {e}", "ERROR")
            return []

    def _get_github_trending(self) -> List[Dict[str, Any]]:
        """è·å–GitHub Trendingçƒ­é—¨é¡¹ç›®ï¼ˆæ‰€æœ‰è¯­è¨€ï¼‰"""
        try:
            # ä½¿ç”¨ç¬¬ä¸‰æ–¹GitHub Trending APIï¼ˆä¸é™åˆ¶è¯­è¨€ï¼‰
            api_url = "https://github-trending-api.now.sh/repositories"
            params = {
                "since": "weekly",
                "spoken_language": "en"
            }

            response = requests.get(api_url, params=params, timeout=10)
            repos = response.json()

            trends = []

            for repo in repos[:15]:
                # è§£æstaræ•°å­—
                stars_str = repo.get("stars", "0")
                stars = self._parse_stars(stars_str)

                # è®¡ç®—çƒ­åº¦è¯„åˆ†
                forks = self._parse_stars(repo.get("forks", "0"))
                heat_score = stars * 0.5 + forks * 0.3

                description = repo.get("description", "")

                trends.append({
                    "title": f"{repo['author']}/{repo['name']}",
                    "description": description or "No description",
                    "url": repo["url"],
                    "source": "GitHub Trending",
                    "timestamp": datetime.now().strftime("%Y-%m-%d"),
                    "metrics": {
                        "stars": stars_str,
                        "forks": repo.get("forks", "0"),
                        "language": repo.get("language", "Unknown")
                    },
                    "heat_score": int(heat_score),
                    "tags": ["å¼€æº", repo.get("language", ""), "GitHub"]
                })

            return trends
        except Exception as e:
            self.log(f"GitHub Trending APIè°ƒç”¨å¤±è´¥: {e}", "ERROR")
            return []

    def _get_reddit_trends(self) -> List[Dict[str, Any]]:
        """è·å–Redditçƒ­é—¨æŠ€æœ¯è®¨è®ºï¼ˆç§‘æŠ€ç›¸å…³Subredditï¼‰"""
        try:
            import praw

            # ä»é…ç½®è¯»å–Reddit APIå‡­è¯
            reddit_config = self.config.get("agents", {}).get("ai_trend_analyzer", {}).get("reddit", {})

            client_id = reddit_config.get("client_id") or os.getenv("REDDIT_CLIENT_ID")
            client_secret = reddit_config.get("client_secret") or os.getenv("REDDIT_CLIENT_SECRET")
            user_agent = reddit_config.get("user_agent", "AI_Trend_Analyzer/1.0")

            if not client_id or not client_secret:
                self.log("Reddit APIå‡­è¯æœªé…ç½®ï¼Œè·³è¿‡Redditæ•°æ®æº", "WARNING")
                return []

            reddit = praw.Reddit(
                client_id=client_id,
                client_secret=client_secret,
                user_agent=user_agent
            )

            # å›ºå®šçš„ç§‘æŠ€ç›¸å…³Subreddit
            subreddits = ["MachineLearning", "artificial", "technology", "programming"]
            trends = []

            for sub_name in subreddits[:2]:  # é™åˆ¶2ä¸ªsubreddit
                try:
                    subreddit = reddit.subreddit(sub_name)
                    for post in subreddit.hot(limit=5):
                        # è¿‡æ»¤ï¼šåªå–æœ€è¿‘3å¤©çš„
                        post_time = datetime.fromtimestamp(post.created_utc)
                        if (datetime.now() - post_time).days > 3:
                            continue

                        # è®¡ç®—çƒ­åº¦è¯„åˆ†
                        upvotes = post.score
                        comments = post.num_comments
                        heat_score = upvotes + comments * 2

                        trends.append({
                            "title": post.title,
                            "description": post.selftext[:200] if hasattr(post, 'selftext') else "",
                            "url": f"https://reddit.com{post.permalink}",
                            "source": f"Reddit r/{sub_name}",
                            "timestamp": post_time.strftime("%Y-%m-%d %H:%M"),
                            "metrics": {
                                "upvotes": upvotes,
                                "comments": comments
                            },
                            "heat_score": heat_score,
                            "tags": ["ç¤¾åŒºè®¨è®º", "Reddit"]
                        })
                except Exception as e:
                    self.log(f"è·å–Reddit r/{sub_name}å¤±è´¥: {e}", "WARNING")
                    continue

            return trends
        except ImportError:
            self.log("PRAWåº“æœªå®‰è£…ï¼Œè·³è¿‡Redditæ•°æ®æºã€‚è¿è¡Œ: pip install praw", "WARNING")
            return []
        except Exception as e:
            self.log(f"Reddit APIè°ƒç”¨å¤±è´¥: {e}", "ERROR")
            return []

    def _parse_stars(self, stars_str: str) -> int:
        """è§£æstaræ•°å­—å­—ç¬¦ä¸²"""
        if isinstance(stars_str, int):
            return stars_str

        stars_str = str(stars_str).replace(",", "").strip()

        if "k" in stars_str.lower():
            return int(float(stars_str.lower().replace("k", "")) * 1000)
        elif "m" in stars_str.lower():
            return int(float(stars_str.lower().replace("m", "")) * 1000000)
        else:
            try:
                return int(stars_str)
            except ValueError:
                return 0

    def _deduplicate_trends(self, trends: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å»é‡ç›¸ä¼¼çš„çƒ­ç‚¹è¯é¢˜"""
        seen_titles = set()
        unique_trends = []

        for trend in trends:
            title = trend.get("title", "").lower()
            # ç®€å•å»é‡ï¼šæ ‡é¢˜å®Œå…¨ç›¸åŒæˆ–åŒ…å«å…³ç³»
            is_duplicate = False
            for seen in seen_titles:
                if title == seen or title in seen or seen in title:
                    is_duplicate = True
                    break

            if not is_duplicate:
                unique_trends.append(trend)
                seen_titles.add(title)

        return unique_trends

    def _save_trends(self, topic: str, trends: List[Dict[str, Any]]):
        """ä¿å­˜çƒ­ç‚¹åˆ†æç»“æœåˆ°rawç›®å½•"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"trends_{topic}_{timestamp}.json"

            output = {
                "topic": topic,
                "timestamp": datetime.now().isoformat(),
                "data_sources": list(self.sources.keys()),
                "total_trends": len(trends),
                "trends": trends
            }

            # ä½¿ç”¨æ–°çš„å­˜å‚¨ç®¡ç†å™¨ï¼Œä¿å­˜åˆ°rawç›®å½•
            filepath = self.storage.save_json("raw", filename, output)

            self.log(f"çƒ­ç‚¹åˆ†æå·²ä¿å­˜: {filepath}")
        except Exception as e:
            self.log(f"ä¿å­˜çƒ­ç‚¹åˆ†æå¤±è´¥: {str(e)}", "WARNING")

    def _get_huggingface_trends(self) -> List[Dict[str, Any]]:
        """è·å–Hugging Faceçƒ­é—¨æ¨¡å‹ï¼ˆç›´æ¥è·å–çƒ­é—¨æ¨¡å‹ï¼‰"""
        try:
            # ä½¿ç”¨Hugging Faceæ¨¡å‹æœç´¢APIï¼ˆæŒ‰likesæ’åºï¼‰
            api_url = "https://huggingface.co/api/models"

            # æœç´¢çƒ­é—¨æ¨¡å‹
            params = {
                "limit": 20,
                "sort": "likes",  # æŒ‰likesæ’åº
                "direction": -1   # é™åº
            }

            response = requests.get(api_url, params=params, timeout=15, headers={
                "User-Agent": "AI-Trend-Analyzer/1.0"
            })

            if response.status_code != 200:
                self.log(f"Hugging Face APIè¿”å›é”™è¯¯: {response.status_code}", "ERROR")
                return []

            # è§£æJSON
            try:
                data = response.json()
            except Exception as e:
                self.log(f"Hugging Face APIè¿”å›æ ¼å¼é”™è¯¯: {e}", "ERROR")
                return []

            # æ£€æŸ¥è¿”å›æ•°æ®æ ¼å¼
            if isinstance(data, dict) and "models" in data:
                models = data["models"]
            elif isinstance(data, list):
                models = data
            else:
                self.log(f"Hugging Face APIè¿”å›æ•°æ®æ ¼å¼å¼‚å¸¸: {type(data)}", "ERROR")
                return []

            if not models or len(models) == 0:
                self.log("Hugging Face APIè¿”å›ç©ºåˆ—è¡¨", "WARNING")
                return []

            trends = []

            for model in models[:15]:
                try:
                    model_id = model.get("id", model.get("modelId", ""))
                    likes = model.get("likes", 0)
                    downloads = model.get("downloads", 0)
                    pipeline = model.get("pipeline", "")

                    if not model_id:
                        continue

                    # è®¡ç®—çƒ­åº¦è¯„åˆ†
                    heat_score = likes * 10 + downloads // 100

                    # æ ¼å¼åŒ–pipeline
                    if pipeline:
                        pipeline_name = pipeline.replace('-', ' ').replace('_', ' ').title()
                    else:
                        pipeline_name = "Model"

                    trends.append({
                        "title": f"ğŸ¤— {model_id}",
                        "description": f"{pipeline_name} | {likes}ğŸ‘ | {downloads}â¬‡ï¸",
                        "url": f"https://huggingface.co/{model_id}",
                        "source": "Hugging Face",
                        "timestamp": datetime.now().strftime("%Y-%m-%d"),
                        "metrics": {
                            "likes": likes,
                            "downloads": downloads,
                            "pipeline": pipeline or "unknown"
                        },
                        "heat_score": heat_score,
                        "tags": ["æ¨¡å‹", "HuggingFace", "AI"]
                    })
                except Exception as e:
                    self.log(f"å¤„ç†Hugging Faceæ¨¡å‹æ•°æ®å¤±è´¥: {e}", "WARNING")
                    continue

            self.log(f"Hugging FaceæˆåŠŸè·å– {len(trends)} ä¸ªæ¨¡å‹")
            return trends
        except Exception as e:
            self.log(f"Hugging Face APIè°ƒç”¨å¤±è´¥: {e}", "ERROR")
            return []

    def _get_stackoverflow_trends(self) -> List[Dict[str, Any]]:
        """è·å–Stack Overflowçƒ­é—¨æŠ€æœ¯é—®é¢˜"""
        try:
            # Stack Exchange API
            api_url = "https://api.stackexchange.com/2.3/questions"

            # è·å–çƒ­é—¨é—®é¢˜
            params = {
                "order": "desc",
                "sort": "hot",  # æŒ‰çƒ­åº¦æ’åº
                "site": "stackoverflow",
                "pagesize": 50  # å¢åŠ æ•°é‡ä»¥è·å¾—æ›´å¤šå€™é€‰
            }

            response = requests.get(api_url, params=params, timeout=10)
            data = response.json()

            trends = []

            if "items" not in data:
                self.log(f"Stack Overflow APIè¿”å›æ ¼å¼å¼‚å¸¸: {data}", "WARNING")
                return []

            for item in data["items"][:30]:
                title = item.get("title", "")
                tags = item.get("tags", [])

                # è®¡ç®—çƒ­åº¦è¯„åˆ†
                score = item.get("score", 0)
                views = item.get("view_count", 0)
                answers = item.get("answer_count", 0)
                heat_score = score * 5 + answers * 15 + views // 200

                # è·å–æ ‡ç­¾å’Œæè¿°
                tags_str = ", ".join(tags[:5])

                # è·å–é—®é¢˜æ­£æ–‡ï¼ˆå»é™¤HTMLæ ‡ç­¾ï¼‰
                body = item.get("body", "")
                if body:
                    # ç®€å•å»é™¤HTMLæ ‡ç­¾
                    import re
                    body_clean = re.sub(r'<[^>]+>', '', body)[:150].replace("\n", " ")
                    description = body_clean if body_clean else f"Tags: {tags_str}"
                else:
                    description = f"Tags: {tags_str}"

                trends.append({
                    "title": title,
                    "description": description,
                    "url": item.get("link", ""),
                    "source": "Stack Overflow",
                    "timestamp": datetime.fromtimestamp(item.get("creation_date", 0)).strftime("%Y-%m-%d"),
                    "metrics": {
                        "score": score,
                        "views": views,
                        "answers": answers,
                        "tags": tags
                    },
                    "heat_score": heat_score,
                    "tags": tags[:3] + ["é—®ç­”", "StackOverflow"]
                })

            self.log(f"Stack OverflowæˆåŠŸè·å– {len(trends)} ä¸ªé—®é¢˜")
            return trends
        except Exception as e:
            self.log(f"Stack Overflow APIè°ƒç”¨å¤±è´¥: {e}", "ERROR")
            return []

    def _get_kaggle_trends(self) -> List[Dict[str, Any]]:
        """è·å–Kaggleç«èµ›å’Œæ•°æ®é›†ï¼ˆAIåº”ç”¨æ¡ˆä¾‹ï¼‰"""
        try:
            # Kaggleä¸æä¾›å®˜æ–¹å…¬å¼€APIï¼Œä½¿ç”¨GitHubæœç´¢æ›¿ä»£
            # æœç´¢æœºå™¨å­¦ä¹ å’Œæ•°æ®ç§‘å­¦ç›¸å…³é¡¹ç›®
            search_query = "machine learning OR data science language:python"
            api_url = "https://api.github.com/search/repositories"
            params = {
                "q": search_query,
                "sort": "stars",
                "order": "desc",
                "per_page": 15
            }

            response = requests.get(api_url, params=params, timeout=15, headers={
                "Accept": "application/vnd.github.v3+json"
            })

            if response.status_code != 200:
                self.log(f"GitHub APIè¿”å›é”™è¯¯: {response.status_code}", "WARNING")
                return []

            data = response.json()

            if "items" not in data:
                return []

            trends = []
            for item in data["items"][:15]:
                # è®¡ç®—çƒ­åº¦è¯„åˆ†
                stars = item.get("stargazers_count", 0)
                forks = item.get("forks_count", 0)
                open_issues = item.get("open_issues_count", 0)
                heat_score = stars * 0.5 + forks * 0.3 + open_issues * 2

                trends.append({
                    "title": item.get("full_name", ""),
                    "description": item.get("description", "æœºå™¨å­¦ä¹ ç›¸å…³é¡¹ç›®")[:200],
                    "url": item.get("html_url", ""),
                    "source": "ML/GitHub",
                    "timestamp": datetime.now().strftime("%Y-%m-%d"),
                    "metrics": {
                        "stars": stars,
                        "forks": forks,
                        "open_issues": open_issues,
                        "language": item.get("language", "Unknown")
                    },
                    "heat_score": heat_score,
                    "tags": ["æ•°æ®ç«èµ›", "AIåº”ç”¨", "å¼€æº"]
                })

            self.log(f"GitHubæœºå™¨å­¦ä¹ é¡¹ç›®æˆåŠŸè·å– {len(trends)} ä¸ªé¡¹ç›®")
            return trends
        except Exception as e:
            self.log(f"Kaggle APIè°ƒç”¨å¤±è´¥: {e}", "ERROR")
            return []

    def _get_newsapi_trends(self) -> List[Dict[str, Any]]:
        """è·å–NewsAPIç§‘æŠ€æ–°é—»ï¼ˆéœ€è¦APIå¯†é’¥ï¼‰"""
        try:
            if not self.newsapi_key:
                self.log("NewsAPIå¯†é’¥æœªé…ç½®ï¼Œè·³è¿‡", "WARNING")
                return []

            # NewsAPIå…è´¹ç‰ˆæ¯å¤©1000æ¬¡è¯·æ±‚
            base_url = "https://newsapi.org/v2/everything"

            # ä½¿ç”¨é€šç”¨çš„AIæŠ€æœ¯å…³é”®è¯
            query = "artificial intelligence OR machine learning OR AI OR LLM OR GPT OR Claude"

            params = {
                "q": query,
                "language": "en",
                "sortBy": "popularity",
                "pageSize": 15,
                "apiKey": self.newsapi_key
            }

            response = requests.get(base_url, params=params, timeout=15)

            if response.status_code == 401:
                self.log("NewsAPIå¯†é’¥æ— æ•ˆ", "WARNING")
                return []
            elif response.status_code == 429:
                self.log("NewsAPIè¯·æ±‚è¶…é™", "WARNING")
                return []
            elif response.status_code != 200:
                self.log(f"NewsAPIè¿”å›é”™è¯¯: {response.status_code}", "WARNING")
                return []

            data = response.json()

            if data.get("status") != "ok":
                return []

            trends = []
            for article in data.get("articles", [])[:15]:
                if not article.get("title") or article.get("title") == "[Removed]":
                    continue

                # è®¡ç®—çƒ­åº¦è¯„åˆ†ï¼ˆåŸºäºæ¥æºå’Œæ—¶é—´ï¼‰
                source_name = article.get("source", {}).get("name", "")
                published_at = article.get("publishedAt", "")

                # ç®€å•çš„çƒ­åº¦è¯„åˆ†
                heat_score = 50  # åŸºç¡€åˆ†

                # æ—¶é—´è¡°å‡
                if published_at:
                    try:
                        pub_date = datetime.fromisoformat(published_at.replace('Z', '+00:00'))
                        days_ago = (datetime.now(pub_date.tzinfo) - pub_date).days
                        heat_score -= days_ago * 5
                    except:
                        pass

                trends.append({
                    "title": article.get("title", ""),
                    "description": article.get("description", article.get("content", ""))[:200],
                    "url": article.get("url", ""),
                    "source": f"NewsAPI ({source_name})",
                    "timestamp": published_at[:10] if published_at else datetime.now().strftime("%Y-%m-%d"),
                    "metrics": {
                        "source": source_name,
                        "published_at": published_at
                    },
                    "heat_score": max(heat_score, 10),
                    "tags": ["æ–°é—»", "AIèµ„è®¯", "è¡Œä¸šåŠ¨æ€"]
                })

            self.log(f"NewsAPIæˆåŠŸè·å– {len(trends)} æ¡æ–°é—»")
            return trends
        except Exception as e:
            self.log(f"NewsAPIè°ƒç”¨å¤±è´¥: {e}", "ERROR")
            return []

    def _get_devto_trends(self) -> List[Dict[str, Any]]:
        """è·å–Dev.toå¼€å‘è€…åšå®¢æ–‡ç« ï¼ˆçƒ­é—¨æŠ€æœ¯æ–‡ç« ï¼‰"""
        try:
            # Dev.toå…¬å¼€APIï¼Œæ— éœ€è®¤è¯
            base_url = "https://dev.to/api/articles"

            # ä½¿ç”¨çƒ­é—¨æŠ€æœ¯æ ‡ç­¾
            tags = ["ai", "machinelearning", "python", "javascript", "webdev"]
            trends = []

            # å¯¹æ¯ä¸ªæ ‡ç­¾è¿›è¡Œæœç´¢
            for tag in tags[:2]:  # åªå–å‰2ä¸ªæ ‡ç­¾é¿å…è¿‡å¤šè¯·æ±‚
                params = {
                    "tag": tag,
                    "top": "7",  # æŒ‰çƒ­åº¦æ’åº
                    "per_page": 10
                }

                response = requests.get(base_url, params=params, timeout=15)

                if response.status_code != 200:
                    continue

                articles = response.json()

                if not isinstance(articles, list):
                    continue

                for article in articles[:7]:
                    # è®¡ç®—çƒ­åº¦è¯„åˆ†
                    comments_count = article.get("comments_count", 0)
                    positive_reactions_count = article.get("positive_reactions_count", 0)
                    heat_score = comments_count * 10 + positive_reactions_count * 2 + 30

                    # è·å–æ ‡ç­¾ï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨æˆ–å­—å…¸åˆ—è¡¨ï¼‰
                    tag_list = article.get("tag_list", [])
                    if tag_list and isinstance(tag_list[0], dict):
                        article_tags = [t.get("name", "") for t in tag_list[:4]]
                    else:
                        article_tags = tag_list[:4] if isinstance(tag_list, list) else []

                    trends.append({
                        "title": article.get("title", ""),
                        "description": article.get("description", "")[:200],
                        "url": article.get("url", ""),
                        "source": "Dev.to",
                        "timestamp": article.get("published_at", "")[:10] if article.get("published_at") else datetime.now().strftime("%Y-%m-%d"),
                        "metrics": {
                            "comments": comments_count,
                            "reactions": positive_reactions_count,
                            "tags": article_tags
                        },
                        "heat_score": heat_score,
                        "tags": article_tags[:3] + ["å¼€å‘è€…åšå®¢", "Dev.to"]
                    })

            # æŒ‰çƒ­åº¦æ’åºå¹¶å»é‡
            trends.sort(key=lambda x: x.get("heat_score", 0), reverse=True)
            seen_titles = set()
            unique_trends = []
            for trend in trends:
                if trend["title"] not in seen_titles:
                    seen_titles.add(trend["title"])
                    unique_trends.append(trend)

            self.log(f"Dev.toæˆåŠŸè·å– {len(unique_trends)} ç¯‡æ–‡ç« ")
            return unique_trends[:15]
        except Exception as e:
            self.log(f"Dev.to APIè°ƒç”¨å¤±è´¥: {e}", "ERROR")
            return []

    def _get_pypi_trends(self) -> List[Dict[str, Any]]:
        """è·å–PyPIçƒ­é—¨PythonåŒ…ï¼ˆçƒ­é—¨AIå’Œæ•°æ®ç§‘å­¦åŒ…ï¼‰"""
        try:
            # ä½¿ç”¨pypistats.org APIï¼ˆå®Œå…¨å…è´¹ï¼‰
            base_url = "https://pypistats.org/api/packages"

            # ä½¿ç”¨çƒ­é—¨AIå’Œæ•°æ®ç§‘å­¦åŒ…åˆ—è¡¨
            packages = [
                "langchain", "openai", "anthropic", "transformers", "torch",
                "tensorflow", "numpy", "pandas", "scikit-learn", "requests",
                "fastapi", "pytest", "matplotlib", "plotly", "gradio"
            ]
            trends = []

            for package in packages[:10]:  # æœ€å¤š10ä¸ªåŒ…
                try:
                    # è·å–æœ€è¿‘30å¤©çš„ä¸‹è½½ç»Ÿè®¡
                    url = f"{base_url}/{package}/recent"
                    response = requests.get(url, timeout=10)

                    if response.status_code != 200:
                        continue

                    data = response.json()

                    # è·å–ä¸‹è½½é‡
                    last_month = data.get("data", {}).get("last_month", 0)
                    last_week = data.get("data", {}).get("last_week", 0)

                    if last_month == 0:
                        continue

                    # è®¡ç®—çƒ­åº¦è¯„åˆ†ï¼ˆä¸‹è½½é‡çš„å¯¹æ•°ï¼‰
                    import math
                    heat_score = math.log10(max(last_month, 1)) * 20

                    # è·å–åŒ…è¯¦æƒ…
                    package_url = f"https://pypi.org/pypi/{package}/json"
                    package_response = requests.get(package_url, timeout=10)

                    description = ""
                    if package_response.status_code == 200:
                        package_info = package_response.json().get("info", {})
                        description = package_info.get("summary", "")[:200]

                    trends.append({
                        "title": f"ğŸ“¦ {package}",
                        "description": description or f"PyPIåŒ… - æœ€è¿‘30å¤©ä¸‹è½½é‡: {last_month:,}",
                        "url": f"https://pypi.org/project/{package}/",
                        "source": "PyPI",
                        "timestamp": datetime.now().strftime("%Y-%m-%d"),
                        "metrics": {
                            "last_month_downloads": last_month,
                            "last_week_downloads": last_week
                        },
                        "heat_score": heat_score,
                        "tags": ["Python", "åŒ…ç®¡ç†", "å·¥å…·"]
                    })
                except Exception as e:
                    self.log(f"è·å–PyPIåŒ… {package} å¤±è´¥: {e}", "WARNING")
                    continue

            # æŒ‰çƒ­åº¦æ’åº
            trends.sort(key=lambda x: x.get("heat_score", 0), reverse=True)

            self.log(f"PyPIæˆåŠŸè·å– {len(trends)} ä¸ªåŒ…")
            return trends[:15]
        except Exception as e:
            self.log(f"PyPI APIè°ƒç”¨å¤±è´¥: {e}", "ERROR")
            return []

    def _get_github_topics_trends(self) -> List[Dict[str, Any]]:
        """è·å–GitHub Topicsï¼ˆAIå’Œç§‘æŠ€çƒ­é—¨ä¸»é¢˜ï¼‰"""
        try:
            # GitHub Topics API - è·å–AIå’Œç§‘æŠ€ç›¸å…³çƒ­é—¨ä¸»é¢˜
            topics = [
                "artificial-intelligence", "machine-learning", "deep-learning",
                "llm", "generative-ai", "automation", "developer-tools"
            ]
            trends = []

            for topic_name in topics[:3]:  # æœ€å¤š3ä¸ªä¸»é¢˜
                try:
                    # æœç´¢è¯¥ä¸»é¢˜ä¸‹çš„çƒ­é—¨ä»“åº“
                    api_url = "https://api.github.com/search/repositories"
                    params = {
                        "q": f"topic:{topic_name}",
                        "sort": "stars",
                        "order": "desc",
                        "per_page": 10
                    }

                    response = requests.get(api_url, params=params, timeout=15, headers={
                        "Accept": "application/vnd.github.v3+json"
                    })

                    if response.status_code != 200:
                        continue

                    data = response.json()

                    if "items" not in data:
                        continue

                    for item in data["items"][:7]:
                        # è®¡ç®—çƒ­åº¦è¯„åˆ†
                        stars = item.get("stargazers_count", 0)
                        forks = item.get("forks_count", 0)
                        heat_score = stars * 0.5 + forks * 0.3

                        # è·å–ä¸»é¢˜æ ‡ç­¾
                        item_topics = item.get("topics", [])[:5]

                        trends.append({
                            "title": item.get("full_name", ""),
                            "description": item.get("description", f"GitHub Topic: {topic_name}")[:200],
                            "url": item.get("html_url", ""),
                            "source": f"GitHub Topics ({topic_name})",
                            "timestamp": datetime.now().strftime("%Y-%m-%d"),
                            "metrics": {
                                "stars": stars,
                                "forks": forks,
                                "topics": item_topics,
                                "language": item.get("language", "Unknown")
                            },
                            "heat_score": heat_score,
                            "tags": item_topics[:3] + ["è¡Œä¸šåº”ç”¨", "å¼€æº"]
                        })

                except Exception as e:
                    self.log(f"è·å–GitHub Topic {topic_name} å¤±è´¥: {e}", "WARNING")
                    continue

            # æŒ‰çƒ­åº¦æ’åºå¹¶å»é‡
            trends.sort(key=lambda x: x.get("heat_score", 0), reverse=True)
            seen_titles = set()
            unique_trends = []
            for trend in trends:
                if trend["title"] not in seen_titles:
                    seen_titles.add(trend["title"])
                    unique_trends.append(trend)

            self.log(f"GitHub TopicsæˆåŠŸè·å– {len(unique_trends)} ä¸ªé¡¹ç›®")
            return unique_trends[:15]
        except Exception as e:
            self.log(f"GitHub Topics APIè°ƒç”¨å¤±è´¥: {e}", "ERROR")
            return []
