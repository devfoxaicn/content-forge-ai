"""
çœŸå®AIçƒ­ç‚¹åˆ†æAgent - äº§å“+æ–°é—»+å­¦æœ¯å¯¼å‘
èšç„¦çƒ­é—¨AIäº§å“ã€åº”ç”¨ã€è¡Œä¸šæ–°é—»å’Œé‡å¤§çªç ´
å—ä¼—ï¼šå¹¿æ³›ï¼ŒéæŠ€æœ¯ç»†èŠ‚
"""

from typing import Dict, Any, List, Optional
import json
import os
import requests
import feedparser
from datetime import datetime, timedelta
from src.agents.base import BaseAgent
from src.utils.storage_v2 import StorageFactory


class RealAITrendAnalyzerAgent(BaseAgent):
    """çœŸå®çš„AIçƒ­ç‚¹åˆ†æAgent - äº§å“æ–°é—»å­¦æœ¯å¯¼å‘"""

    def __init__(self, config: Dict[str, Any], prompts: Dict[str, Any]):
        super().__init__(config, prompts)
        self.mock_mode = config.get("agents", {}).get("ai_trend_analyzer", {}).get("mock_mode", False)

        # ä½¿ç”¨æ–°çš„å­˜å‚¨ç®¡ç†å™¨
        self.storage = StorageFactory.create_daily(
            base_dir=config.get("storage", {}).get("base_dir", "data")
        )

        # æ•°æ®æºé…ç½®
        sources_config = config.get("agents", {}).get("ai_trend_analyzer", {}).get("sources", [])
        self.sources = {
            # äº§å“ç±»
            "producthunt": "producthunt" in sources_config,
            "github_apps": "github" in sources_config,
            # æ–°é—»ç±»
            "techcrunch_ai": "techcrunch_ai" in sources_config,
            "verge_ai": "verge_ai" in sources_config,
            "venturebeat_ai": "venturebeat_ai" in sources_config,
            # å­¦æœ¯ç±»ï¼ˆé‡å¤§æ–°é—»ï¼‰
            "arxiv_news": "arxiv_news" in sources_config,
            # ç§‘æŠ€æ–°é—»ï¼ˆè¿‡æ»¤äº§å“ç±»ï¼‰
            "hackernews": "hackernews" in sources_config,
        }

        # è·å–é…ç½®
        agent_config = config.get("agents", {}).get("ai_trend_analyzer", {})
        self.max_trends = agent_config.get("max_trends", 20)
        self.min_score = agent_config.get("min_heat_score", 60)

        # åˆå§‹åŒ–åˆ†ç±»å…³é”®è¯
        self._init_category_keywords()

    def execute(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡ŒAIçƒ­ç‚¹åˆ†æ

        Args:
            state: å½“å‰å·¥ä½œæµçŠ¶æ€

        Returns:
            Dict[str, Any]: æ›´æ–°åçš„çŠ¶æ€
        """
        # æ£€æµ‹æ˜¯å¦ä¸ºç”¨æˆ·æŒ‡å®šè¯é¢˜æ¨¡å¼
        if state.get("current_step") == "user_topic_set":
            self.log("æ£€æµ‹åˆ°ç”¨æˆ·æŒ‡å®šè¯é¢˜æ¨¡å¼ï¼Œè·³è¿‡AIçƒ­ç‚¹åˆ†æ")
            return state

        self.log(f"å¼€å§‹åˆ†æAIäº§å“ä¸ç§‘æŠ€çƒ­ç‚¹ï¼Œç›®æ ‡: {self.max_trends}ä¸ª")

        try:
            # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨mockæ¨¡å¼
            if self.mock_mode:
                self.log("ä½¿ç”¨Mockæ¨¡å¼ï¼ˆæ¨¡æ‹Ÿæ•°æ®ï¼‰")
                from src.agents.ai_trend_analyzer import AITrendAnalyzerAgent
                mock_agent = AITrendAnalyzerAgent(self.config, self.prompts)
                hot_topics = mock_agent._get_mock_ai_trends(state.get('topic', 'AI'))
            else:
                self.log("ä½¿ç”¨çœŸå®APIæ¨¡å¼ï¼ˆäº§å“+æ–°é—»+å­¦æœ¯ï¼‰")
                hot_topics = self._get_real_ai_trends()

            self.log(f"æˆåŠŸåˆ†æå‡º {len(hot_topics)} ä¸ªçƒ­ç‚¹è¯é¢˜")

            # ä¿å­˜çƒ­ç‚¹åˆ†æç»“æœ
            self._save_trends(hot_topics)

            # é€‰æ‹©çƒ­åº¦æœ€é«˜çš„è¯é¢˜
            if hot_topics:
                selected_topic = hot_topics[0]
                self.log(f"é€‰æ‹©çƒ­ç‚¹è¯é¢˜: {selected_topic['title']}")
            else:
                selected_topic = {
                    "title": "AIæŠ€æœ¯å‘å±•",
                    "description": "äººå·¥æ™ºèƒ½å‰æ²¿åŠ¨æ€",
                    "url": "",
                    "source": "é»˜è®¤"
                }

            return {
                **state,
                "ai_hot_topics": hot_topics,
                "selected_ai_topic": selected_topic,
                "current_step": "ai_trend_analyzer_completed"
            }
        except Exception as e:
            self.log(f"AIçƒ­ç‚¹åˆ†æå¤±è´¥: {str(e)}", "ERROR")
            return {
                **state,
                "error_message": f"AIçƒ­ç‚¹åˆ†æå¤±è´¥: {str(e)}",
                "current_step": "ai_trend_analyzer_failed"
            }

    def _get_real_ai_trends(self) -> List[Dict[str, Any]]:
        """
        ä»å¤šä¸ªæ•°æ®æºè·å–çœŸå®AIçƒ­ç‚¹ï¼ˆäº§å“+æ–°é—»+å­¦æœ¯ï¼‰

        Returns:
            List[Dict[str, Any]]: çƒ­ç‚¹è¯é¢˜åˆ—è¡¨
        """
        all_trends = []

        # ===== äº§å“ç±»æ•°æ®æº =====

        # 1. Product Hunt - çƒ­é—¨AIäº§å“
        if self.sources["producthunt"]:
            try:
                ph_trends = self._get_product_hunt_trends()
                all_trends.extend(ph_trends)
                self.log(f"Product Hunt: è·å– {len(ph_trends)} æ¡çƒ­ç‚¹")
            except Exception as e:
                self.log(f"Product Huntè·å–å¤±è´¥: {e}", "WARNING")

        # 2. GitHub Trending - AIåº”ç”¨é¡¹ç›®
        if self.sources["github_apps"]:
            try:
                gh_trends = self._get_github_ai_apps()
                all_trends.extend(gh_trends)
                self.log(f"GitHub AIåº”ç”¨: è·å– {len(gh_trends)} æ¡çƒ­ç‚¹")
            except Exception as e:
                self.log(f"GitHub AIåº”ç”¨è·å–å¤±è´¥: {e}", "WARNING")

        # ===== æ–°é—»ç±»æ•°æ®æº =====

        # 3. TechCrunch AI
        if self.sources["techcrunch_ai"]:
            try:
                tc_trends = self._get_techcrunch_ai_trends()
                all_trends.extend(tc_trends)
                self.log(f"TechCrunch AI: è·å– {len(tc_trends)} æ¡çƒ­ç‚¹")
            except Exception as e:
                self.log(f"TechCrunch AIè·å–å¤±è´¥: {e}", "WARNING")

        # 4. The Verge AI
        if self.sources["verge_ai"]:
            try:
                verge_trends = self._get_verge_ai_trends()
                all_trends.extend(verge_trends)
                self.log(f"The Verge AI: è·å– {len(verge_trends)} æ¡çƒ­ç‚¹")
            except Exception as e:
                self.log(f"The Verge AIè·å–å¤±è´¥: {e}", "WARNING")

        # 5. VentureBeat AI
        if self.sources["venturebeat_ai"]:
            try:
                vb_trends = self._get_venturebeat_ai_trends()
                all_trends.extend(vb_trends)
                self.log(f"VentureBeat AI: è·å– {len(vb_trends)} æ¡çƒ­ç‚¹")
            except Exception as e:
                self.log(f"VentureBeat AIè·å–å¤±è´¥: {e}", "WARNING")

        # ===== å­¦æœ¯ç±»æ•°æ®æºï¼ˆé‡å¤§æ–°é—»ï¼‰ =====

        # 6. arXivé‡å¤§è®ºæ–‡æ–°é—»
        if self.sources["arxiv_news"]:
            try:
                arxiv_trends = self._get_arxiv_major_news()
                all_trends.extend(arxiv_trends)
                self.log(f"arXivé‡å¤§æ–°é—»: è·å– {len(arxiv_trends)} æ¡çƒ­ç‚¹")
            except Exception as e:
                self.log(f"arXivé‡å¤§æ–°é—»è·å–å¤±è´¥: {e}", "WARNING")

        # ===== ç§‘æŠ€æ–°é—»ï¼ˆè¿‡æ»¤äº§å“ç±»ï¼‰ =====

        # 7. HackerNewsï¼ˆäº§å“ç±»è¿‡æ»¤ï¼‰
        if self.sources["hackernews"]:
            try:
                hn_trends = self._get_hacker_news_products()
                all_trends.extend(hn_trends)
                self.log(f"HackerNewsäº§å“ç±»: è·å– {len(hn_trends)} æ¡çƒ­ç‚¹")
            except Exception as e:
                self.log(f"HackerNewsäº§å“ç±»è·å–å¤±è´¥: {e}", "WARNING")

        # æŒ‰ç»¼åˆçƒ­åº¦è¯„åˆ†æ’åº
        all_trends.sort(key=lambda x: x.get("heat_score", 0), reverse=True)

        # å»é‡ï¼ˆåŸºäºæ ‡é¢˜ç›¸ä¼¼åº¦ï¼‰
        all_trends = self._deduplicate_trends(all_trends)

        # è¿‡æ»¤ä½åˆ†å†…å®¹
        all_trends = [t for t in all_trends if t.get("heat_score", 0) >= self.min_score]

        # å¯¹æ¯ä¸ªçƒ­ç‚¹è¿›è¡Œåˆ†ç±»
        for trend in all_trends:
            classification = self._classify_trend(trend)
            trend["category"] = classification["category"]
            trend["category_icon"] = classification["icon"]
            trend["category_confidence"] = classification["confidence"]
            # æ›´æ–°tagsä»¥åŒ…å«åˆ†ç±»ä¿¡æ¯
            if "tags" not in trend:
                trend["tags"] = []
            trend["tags"].append(classification["category"].replace("ğŸ“ˆ ", "").replace("ğŸ“ ", "").replace("ğŸ”¬ ", "").replace("ğŸ› ï¸ ", "").replace("ğŸ’¼ ", ""))

        # è¿”å›Top N
        return all_trends[:self.max_trends]

    # ==================== äº§å“ç±»æ•°æ®æº ====================

    def _get_product_hunt_trends(self) -> List[Dict[str, Any]]:
        """è·å–Product Huntçƒ­é—¨AIäº§å“ï¼ˆRSSï¼‰"""
        try:
            return self._get_rss_trends(
                rss_url="https://www.producthunt.com/posts/feed",
                source_name="Product Hunt",
                item_type="product",
                max_items=20
            )
        except Exception as e:
            self.log(f"Product Hunt RSSè§£æå¤±è´¥: {e}", "ERROR")
            return []

    def _get_github_ai_apps(self) -> List[Dict[str, Any]]:
        """è·å–GitHub Trending AIåº”ç”¨é¡¹ç›®ï¼ˆéæ¡†æ¶åº“ï¼‰"""
        try:
            api_url = "https://github-trending-api.now.sh/repositories"

            # æœç´¢AIåº”ç”¨ç›¸å…³çš„è¯­è¨€å’Œå…³é”®è¯
            search_terms = [
                ("", "weekly"),  # å…¨å±€çƒ­é—¨
                ("python", "weekly"),
                ("javascript", "weekly"),
                ("typescript", "weekly"),
            ]

            all_repos = []

            for lang, period in search_terms:
                try:
                    params = {
                        "language": lang if lang else None,
                        "since": period
                    }
                    params = {k: v for k, v in params.items() if v is not None}

                    response = requests.get(api_url, params=params, timeout=10)
                    repos = response.json()

                    for repo in repos[:10]:
                        repo["fetched_language"] = lang or "multi"
                        all_repos.append(repo)

                except Exception as e:
                    self.log(f"è·å–GitHub {lang}è¶‹åŠ¿å¤±è´¥: {e}", "WARNING")
                    continue

            trends = []

            for repo in all_repos[:50]:  # å–å‰50ä¸ªå€™é€‰
                # è¿‡æ»¤ï¼šä¿ç•™AIåº”ç”¨ç±»é¡¹ç›®
                name = repo.get("name", "").lower()
                description = repo.get("description", "").lower()
                combined = f"{name} {description}"

                # è¿‡æ»¤æ‰çº¯æŠ€æœ¯æ¡†æ¶/åº“
                skip_keywords = [
                    "framework", "library", "sdk", "api", "toolkit",
                    "boilerplate", "template", "wrapper", "binding"
                ]

                if any(kw in combined for kw in skip_keywords):
                    continue

                # ä¼˜å…ˆä¿ç•™AIåº”ç”¨ç±»é¡¹ç›®
                ai_keywords = [
                    "ai", "gpt", "chatbot", "agent", "assistant", "automation",
                    "copilot", "llm", "openai", "claude", "gemini", "stable diffusion",
                    "image", "video", "audio", "text", "code", "generation"
                ]

                if not any(kw in combined for kw in ai_keywords):
                    # éAIé¡¹ç›®é™ä½ä¼˜å…ˆçº§
                    continue

                stars_str = repo.get("stars", "0")
                stars = self._parse_stars(stars_str)
                forks = self._parse_stars(repo.get("forks", "0"))

                # è®¡ç®—çƒ­åº¦è¯„åˆ†
                heat_score = stars * 0.5 + forks * 0.3 + 50  # åŸºç¡€åˆ†50

                description = repo.get("description", "") or "AIåº”ç”¨é¡¹ç›®"
                lang = repo.get("fetched_language", repo.get("language", "Unknown"))

                trends.append({
                    "title": f"{repo['author']}/{repo['name']}",
                    "description": description[:200],
                    "url": repo["url"],
                    "source": f"GitHub ({lang})",
                    "timestamp": datetime.now().strftime("%Y-%m-%d"),
                    "metrics": {
                        "stars": stars_str,
                        "forks": repo.get("forks", "0"),
                        "language": lang
                    },
                    "heat_score": int(heat_score),
                    "tags": ["AIåº”ç”¨", "å¼€æº", lang]
                })

            return trends[:30]  # è¿”å›å‰30ä¸ª
        except Exception as e:
            self.log(f"GitHub AIåº”ç”¨è·å–å¤±è´¥: {e}", "ERROR")
            return []

    # ==================== æ–°é—»ç±»æ•°æ®æº ====================

    def _get_techcrunch_ai_trends(self) -> List[Dict[str, Any]]:
        """è·å–TechCrunch AIæ–°é—»ï¼ˆRSSï¼‰"""
        try:
            return self._get_rss_trends(
                rss_url="https://techcrunch.com/category/artificial-intelligence/feed/",
                source_name="TechCrunch AI",
                item_type="news",
                max_items=15
            )
        except Exception as e:
            self.log(f"TechCrunch AI RSSè§£æå¤±è´¥: {e}", "ERROR")
            return []

    def _get_verge_ai_trends(self) -> List[Dict[str, Any]]:
        """è·å–The Verge AIæ–°é—»ï¼ˆRSSï¼‰"""
        try:
            return self._get_rss_trends(
                rss_url="https://www.theverge.com/ai-artificial-intelligence/rss/index.xml",
                source_name="The Verge AI",
                item_type="news",
                max_items=15
            )
        except Exception as e:
            self.log(f"The Verge AI RSSè§£æå¤±è´¥: {e}", "ERROR")
            return []

    def _get_venturebeat_ai_trends(self) -> List[Dict[str, Any]]:
        """è·å–VentureBeat AIæ–°é—»ï¼ˆRSSï¼‰"""
        try:
            return self._get_rss_trends(
                rss_url="https://venturebeat.com/ai/feed/",
                source_name="VentureBeat AI",
                item_type="news",
                max_items=10
            )
        except Exception as e:
            self.log(f"VentureBeat AI RSSè§£æå¤±è´¥: {e}", "ERROR")
            return []

    def _get_rss_trends(self, rss_url: str, source_name: str, item_type: str, max_items: int = 15) -> List[Dict[str, Any]]:
        """é€šç”¨RSSè·å–æ–¹æ³•"""
        try:
            feed = feedparser.parse(rss_url)

            if feed.bozo:
                self.log(f"{source_name} RSSè§£æè­¦å‘Š: {feed.bozo}", "WARNING")

            trends = []

            for entry in feed.entries[:max_items]:
                try:
                    title = entry.get("title", "")
                    description = entry.get("description", "")

                    # æ¸…ç†HTMLæ ‡ç­¾
                    if description:
                        import re
                        description = re.sub(r'<[^>]+>', '', description)
                        description = description.strip()[:300]

                    url = entry.get("link", "")
                    published = entry.get("published", "")

                    # è®¡ç®—çƒ­åº¦è¯„åˆ†
                    heat_score = 60  # RSSæºåŸºç¡€åˆ†

                    # æ ¹æ®ç±»å‹è°ƒæ•´
                    if item_type == "product":
                        heat_score += 20
                    elif item_type == "news":
                        # æ–°é—»ç±»ï¼šå…³é”®è¯åŠ åˆ†
                        news_keywords = ["å‘å¸ƒ", "æ¨å‡º", "èèµ„", "æ”¶è´­", "çªç ´", "å‘å¸ƒ", "launch", "raises", "acquisition"]
                        if any(kw.lower() in title.lower() for kw in news_keywords):
                            heat_score += 15

                        # çŸ¥åå…¬å¸åŠ åˆ†
                        companies = ["OpenAI", "Google", "Meta", "Microsoft", "Anthropic", "Apple", "Amazon"]
                        if any(company.lower() in title.lower() for company in companies):
                            heat_score += 10

                    trends.append({
                        "title": title,
                        "description": description or title[:200],
                        "url": url,
                        "source": source_name,
                        "timestamp": published[:10] if published else datetime.now().strftime("%Y-%m-%d"),
                        "metrics": {
                            "published": published,
                            "type": item_type
                        },
                        "heat_score": heat_score,
                        "tags": ["æ–°é—»", "AIèµ„è®¯"] if item_type == "news" else ["äº§å“", "AIå·¥å…·"]
                    })

                except Exception as e:
                    self.log(f"å¤„ç†{source_name}æ¡ç›®å¤±è´¥: {e}", "WARNING")
                    continue

            return trends
        except Exception as e:
            self.log(f"{source_name} RSSè·å–å¤±è´¥: {e}", "ERROR")
            return []

    # ==================== å­¦æœ¯ç±»æ•°æ®æºï¼ˆé‡å¤§æ–°é—»ï¼‰ ====================

    def _get_arxiv_major_news(self) -> List[Dict[str, Any]]:
        """è·å–arXivé‡å¤§è®ºæ–‡æ–°é—»ï¼ˆä»…é‡å¤§çªç ´ï¼‰"""
        try:
            import arxiv

            # æœç´¢AIç›¸å…³åˆ†ç±»
            query = "cat:cs.AI OR cat:cs.CL OR cat:cs.LG OR cat:cs.CV"

            search = arxiv.Search(
                query=query,
                max_results=50,  # è·å–æ›´å¤šå€™é€‰
                sort_by=arxiv.SortCriterion.SubmittedDate,
                sort_order=arxiv.SortOrder.Descending
            )

            trends = []
            cutoff_date = datetime.now() - timedelta(days=30)  # æ‰©å±•åˆ°30å¤©

            # çŸ¥åæœºæ„åˆ—è¡¨
            major_institutions = [
                "openai", "google", "deepmind", "meta", "anthropic",
                "microsoft", "stanford", "mit", "berkeley", "carnegie",
                "nvidia", "amazon", "apple"
            ]

            # é‡å¤§çªç ´å…³é”®è¯
            breakthrough_keywords = [
                "gpt", "claude", "gemini", "llama", "diffusion",
                "breakthrough", "sota", "record", "human-level",
                "reasoning", "agent", "multimodal", "vision"
            ]

            for result in search.results():
                pub_date = result.published.replace(tzinfo=None)
                if pub_date < cutoff_date:
                    continue

                title = result.title.lower()
                authors = [a.name.lower() for a in result.authors]

                # è¿‡æ»¤ï¼šå¿…é¡»æ˜¯çŸ¥åæœºæ„æˆ–é‡å¤§çªç ´
                is_major = False

                # æ£€æŸ¥ä½œè€…æ˜¯å¦æ¥è‡ªçŸ¥åæœºæ„
                for author in authors[:5]:
                    if any(inst in author for inst in major_institutions):
                        is_major = True
                        break

                # æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åŒ…å«é‡å¤§çªç ´å…³é”®è¯
                if not is_major:
                    if any(kw in title for kw in breakthrough_keywords):
                        is_major = True

                if not is_major:
                    continue  # è·³è¿‡æ™®é€šè®ºæ–‡

                # è®¡ç®—çƒ­åº¦è¯„åˆ†
                days_ago = (datetime.now() - pub_date).days
                heat_score = 80 - days_ago * 2  # åŸºç¡€åˆ†æ›´é«˜

                # é‡å¤§å…³é”®è¯åŠ åˆ†
                if any(kw in title for kw in breakthrough_keywords):
                    heat_score += 10

                trends.append({
                    "title": result.title,
                    "description": result.summary[:300],
                    "url": result.entry_id,
                    "source": "arXiv",
                    "timestamp": pub_date.strftime("%Y-%m-%d"),
                    "metrics": {
                        "authors": [a.name for a in result.authors[:3]],
                        "days_ago": days_ago
                    },
                    "heat_score": heat_score,
                    "tags": ["è®ºæ–‡", "å­¦æœ¯", "é‡å¤§çªç ´"]
                })

                if len(trends) >= 20:
                    break

            return trends
        except ImportError:
            self.log("arXivåº“æœªå®‰è£…ï¼Œè·³è¿‡ã€‚è¿è¡Œ: pip install arxiv", "WARNING")
            return []
        except Exception as e:
            self.log(f"arXivé‡å¤§æ–°é—»è·å–å¤±è´¥: {e}", "ERROR")
            return []

    # ==================== ç§‘æŠ€æ–°é—»ï¼ˆè¿‡æ»¤äº§å“ç±»ï¼‰ ====================

    def _get_hacker_news_products(self) -> List[Dict[str, Any]]:
        """è·å–HackerNewsäº§å“ç±»è¯é¢˜ï¼ˆè¿‡æ»¤æŠ€æœ¯ç»†èŠ‚ï¼‰"""
        try:
            stories_url = "https://hacker-news.firebaseio.com/v0/topstories.json"
            response = requests.get(stories_url, timeout=10)
            story_ids = response.json()[:50]

            trends = []

            # ä¿ç•™çš„å…³é”®è¯
            keep_keywords = [
                "show hn:", "launch", "release", "ai", "gpt", "openai",
                "product", "startup", "company", "raises", "funding",
                "acquired", "microsoft", "google", "apple", "meta"
            ]

            # è¿‡æ»¤çš„å…³é”®è¯ï¼ˆæŠ€æœ¯ç»†èŠ‚ï¼‰
            skip_keywords = [
                "tutorial", "how to", "guide", "tips", "best practices",
                "programming", "coding", "debug", "framework", "library"
            ]

            for story_id in story_ids:
                try:
                    item_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
                    item = requests.get(item_url, timeout=5).json()

                    if not item or "url" not in item:
                        continue

                    title = item.get("title", "").lower()

                    # è¿‡æ»¤ï¼šè·³è¿‡æŠ€æœ¯ç»†èŠ‚ç±»
                    if any(kw in title for kw in skip_keywords):
                        continue

                    # ä¼˜å…ˆä¿ç•™äº§å“ç±»
                    if not any(kw in title for kw in keep_keywords):
                        # éäº§å“ç±»é™ä½ä¼˜å…ˆçº§
                        continue

                    score = item.get("score", 0)
                    comments = item.get("descendants", 0)
                    heat_score = score * 2 + comments + 40  # åŸºç¡€åˆ†40

                    trends.append({
                        "title": item.get("title", ""),
                        "description": item.get("text", item.get("title", ""))[:200],
                        "url": item.get("url", ""),
                        "source": "Hacker News",
                        "timestamp": datetime.fromtimestamp(item["time"]).strftime("%Y-%m-%d %H:%M"),
                        "metrics": {
                            "upvotes": score,
                            "comments": comments
                        },
                        "heat_score": heat_score,
                        "tags": ["ç§‘æŠ€æ–°é—»", "äº§å“"]
                    })

                except Exception as e:
                    self.log(f"è·å–HNæ•…äº‹ {story_id} å¤±è´¥: {e}", "WARNING")
                    continue

            return trends[:30]
        except Exception as e:
            self.log(f"HackerNewsäº§å“ç±»è·å–å¤±è´¥: {e}", "ERROR")
            return []

    # ==================== è¾…åŠ©æ–¹æ³• ====================

    def _init_category_keywords(self):
        """åˆå§‹åŒ–åˆ†ç±»å…³é”®è¯"""
        # æŒ‰ä¼˜å…ˆçº§æ’åºçš„5å¤§åˆ†ç±»
        self.category_keywords = {
            "ğŸ“ˆ è¡Œä¸šåŠ¨æ€": {
                "keywords": [
                    "raises", "funding", "investment", "acquisition", "acquired", "merger",
                    "ipo", "valuation", "revenue", "strategy", "partnership", "collaboration",
                    "ceo", "founder", "startup", "company", "corporation", "launch", "release",
                    "business", "commercial", "enterprise", "deal"
                ],
                "icon": "ğŸ“ˆ",
                "priority": 1
            },
            "ğŸ“ å­¦æœ¯çªç ´": {
                "keywords": [
                    "paper", "research", "study", "arxiv", "publication", "publish",
                    "university", "institute", "lab", "professor", "scientist", "researcher",
                    "conference", "journal", "peer-reviewed", "dataset", "breakthrough",
                    "novel", "state-of-the-art", "sota"
                ],
                "icon": "ğŸ“",
                "priority": 2
            },
            "ğŸ”¬ æŠ€æœ¯åˆ›æ–°": {
                "keywords": [
                    "model", "algorithm", "architecture", "gpt", "claude", "gemini", "llama",
                    "diffusion", "transformer", "neural", "network", "training", "inference",
                    "framework", "engine", "system", "upgrade", "advance", "breakthrough",
                    "sota", "record", "human-level", "reasoning", "multimodal"
                ],
                "icon": "ğŸ”¬",
                "priority": 3
            },
            "ğŸ› ï¸ AIå·¥å…·/äº§å“": {
                "keywords": [
                    "tool", "platform", "service", "app", "software", "application",
                    "product", "saas", "solution", "assistant", "copilot", "chatbot",
                    "generator", "creator", "editor", "plugin", "extension", "integration",
                    "api", "sdk", "library", "package", "release", "launch", "update"
                ],
                "icon": "ğŸ› ï¸",
                "priority": 4
            },
            "ğŸ’¼ AIåº”ç”¨": {
                "keywords": [
                    "use case", "industry", "business", "workflow", "automation",
                    "implementation", "deployment", "integration", "solution", "case study",
                    "application", "enterprise", "organization", "company", "sector"
                ],
                "icon": "ğŸ’¼",
                "priority": 5
            }
        }

        # æ•°æ®æºåˆ°åˆ†ç±»çš„æ˜ å°„ï¼ˆç”¨äºåˆæ­¥åˆ†ç±»ï¼‰
        self.source_category_map = {
            "Product Hunt": "ğŸ› ï¸ AIå·¥å…·/äº§å“",
            "GitHub": "ğŸ’¼ AIåº”ç”¨",
            "TechCrunch AI": "ğŸ“ˆ è¡Œä¸šåŠ¨æ€",
            "The Verge AI": "ğŸ”¬ æŠ€æœ¯åˆ›æ–°",
            "VentureBeat AI": "ğŸ“ˆ è¡Œä¸šåŠ¨æ€",
            "arXiv": "ğŸ“ å­¦æœ¯çªç ´",
            "Hacker News": None  # HNéœ€è¦æ ¹æ®å†…å®¹åˆ¤æ–­
        }

    def _classify_trend(self, trend: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ™ºèƒ½åˆ†ç±»çƒ­ç‚¹è¯é¢˜

        Args:
            trend: çƒ­ç‚¹æ•°æ®

        Returns:
            åˆ†ç±»ä¿¡æ¯å­—å…¸
        """
        title = trend.get("title", "").lower()
        description = trend.get("description", "").lower()
        text = f"{title} {description}"

        # æ­¥éª¤1ï¼šåŸºäºæ•°æ®æºçš„åˆæ­¥åˆ†ç±»
        source = trend.get("source", "")
        base_category = self.source_category_map.get(source)

        # æ­¥éª¤2ï¼šåŸºäºå…³é”®è¯è®¡ç®—æ¯ä¸ªç±»åˆ«çš„åŒ¹é…åº¦
        category_scores = {}

        for category, config in self.category_keywords.items():
            keywords = config["keywords"]

            # è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°
            score = 0
            matched_keywords = []

            for keyword in keywords:
                if keyword.lower() in text:
                    score += 1
                    matched_keywords.append(keyword)

            # å¦‚æœæœ‰åŸºç¡€åˆ†ç±»ä¸”åŒ¹é…ï¼ŒåŠ åˆ†
            if base_category == category:
                score += 2

            category_scores[category] = {
                "score": score,
                "matched_keywords": matched_keywords
            }

        # æ­¥éª¤3ï¼šé€‰æ‹©æœ€é«˜åˆ†ç±»
        best_category = max(category_scores.items(), key=lambda x: x[1]["score"])
        category_name = best_category[0]
        category_info = self.category_keywords[category_name]

        # æ­¥éª¤4ï¼šåˆ¤æ–­æ˜¯å¦æ˜¯æœ‰æ•ˆåˆ†ç±»
        if best_category[1]["score"] == 0:
            # æ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•å…³é”®è¯ï¼Œæ ¹æ®æ•°æ®æºåˆ†é…é»˜è®¤åˆ†ç±»
            if base_category:
                category_name = base_category
                category_info = self.category_keywords[base_category]
            else:
                # å…œåº•åˆ†ç±»
                category_name = "ğŸ”¬ æŠ€æœ¯åˆ›æ–°"
                category_info = self.category_keywords[category_name]

        return {
            "category": category_name,
            "icon": category_info["icon"],
            "confidence": best_category[1]["score"],
            "matched_keywords": best_category[1]["matched_keywords"]
        }

    def _parse_stars(self, stars_str: str) -> int:
        """è§£æstaræ•°å­—å­—ç¬¦ä¸²"""
        if isinstance(stars_str, int):
            return stars_str

        stars_str = str(stars_str).replace(",", "").strip()

        if "k" in stars_str.lower():
            return int(float(stars_str.lower().replace("k", "")) * 1000)
        elif "m" in stars_str.lower():
            return int(float(stars_str.lower().replace("m", "")) * 1000000)
        else:
            try:
                return int(stars_str)
            except ValueError:
                return 0

    def _deduplicate_trends(self, trends: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å»é‡ç›¸ä¼¼çš„çƒ­ç‚¹è¯é¢˜"""
        seen_titles = set()
        unique_trends = []

        for trend in trends:
            title = trend.get("title", "").lower()
            # ç®€å•å»é‡ï¼šæ ‡é¢˜å®Œå…¨ç›¸åŒæˆ–åŒ…å«å…³ç³»
            is_duplicate = False
            for seen in seen_titles:
                if title == seen or title in seen or seen in title:
                    is_duplicate = True
                    break

            if not is_duplicate:
                unique_trends.append(trend)
                seen_titles.add(title)

        return unique_trends

    def _save_trends(self, trends: List[Dict[str, Any]]):
        """ä¿å­˜çƒ­ç‚¹åˆ†æç»“æœåˆ°rawç›®å½•"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"trends_ai_{timestamp}.json"

            output = {
                "timestamp": datetime.now().isoformat(),
                "total_trends": len(trends),
                "data_sources": list(self.sources.keys()),
                "trends": trends
            }

            # ä½¿ç”¨æ–°çš„å­˜å‚¨ç®¡ç†å™¨ï¼Œä¿å­˜åˆ°rawç›®å½•
            filepath = self.storage.save_json("raw", filename, output)

            self.log(f"çƒ­ç‚¹åˆ†æå·²ä¿å­˜: {filepath}")
        except Exception as e:
            self.log(f"ä¿å­˜çƒ­ç‚¹åˆ†æå¤±è´¥: {str(e)}", "WARNING")
