"""
çœŸå®AIçƒ­ç‚¹åˆ†æAgent - äº§å“+æ–°é—»+å­¦æœ¯å¯¼å‘
èšç„¦çƒ­é—¨AIäº§å“ã€åº”ç”¨ã€è¡Œä¸šæ–°é—»å’Œé‡å¤§çªç ´
å—ä¼—ï¼šå¹¿æ³›ï¼ŒéæŠ€æœ¯ç»†èŠ‚
"""

from typing import Dict, Any, List, Optional
import json
import os
import requests
import feedparser
from datetime import datetime, timedelta
from src.agents.base import BaseAgent
from src.utils.storage_v2 import StorageFactory


class RealAITrendAnalyzerAgent(BaseAgent):
    """çœŸå®çš„AIçƒ­ç‚¹åˆ†æAgent - äº§å“æ–°é—»å­¦æœ¯å¯¼å‘"""

    def __init__(self, config: Dict[str, Any], prompts: Dict[str, Any]):
        super().__init__(config, prompts)
        self.mock_mode = config.get("agents", {}).get("ai_trend_analyzer", {}).get("mock_mode", False)

        # ä½¿ç”¨æ–°çš„å­˜å‚¨ç®¡ç†å™¨
        self.storage = StorageFactory.create_daily(
            base_dir=config.get("storage", {}).get("base_dir", "data")
        )

        # æ•°æ®æºé…ç½®
        sources_config = config.get("agents", {}).get("ai_trend_analyzer", {}).get("sources", [])
        self.sources = {
            # äº§å“ç±»
            "producthunt": "producthunt" in sources_config,
            "github_apps": "github" in sources_config,
            # æ–°é—»ç±»
            "techcrunch_ai": "techcrunch_ai" in sources_config,
            "verge_ai": "verge_ai" in sources_config,
            "venturebeat_ai": "venturebeat_ai" in sources_config,
            "newsapi": "newsapi" in sources_config,
            # å­¦æœ¯ç±»ï¼ˆé‡å¤§æ–°é—»ï¼‰
            "arxiv_news": "arxiv_news" in sources_config,
            # ç§‘æŠ€æ–°é—»ï¼ˆè¿‡æ»¤äº§å“ç±»ï¼‰
            "hackernews": "hackernews" in sources_config,
        }

        # è·å–é…ç½®
        agent_config = config.get("agents", {}).get("ai_trend_analyzer", {})
        self.max_trends = agent_config.get("max_trends", 20)
        self.min_score = agent_config.get("min_heat_score", 60)

        # åˆå§‹åŒ–åˆ†ç±»å…³é”®è¯
        self._init_category_keywords()

    def execute(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡ŒAIçƒ­ç‚¹åˆ†æ

        Args:
            state: å½“å‰å·¥ä½œæµçŠ¶æ€

        Returns:
            Dict[str, Any]: æ›´æ–°åçš„çŠ¶æ€
        """
        # æ£€æµ‹æ˜¯å¦ä¸ºç”¨æˆ·æŒ‡å®šè¯é¢˜æ¨¡å¼
        if state.get("current_step") == "user_topic_set":
            self.log("æ£€æµ‹åˆ°ç”¨æˆ·æŒ‡å®šè¯é¢˜æ¨¡å¼ï¼Œè·³è¿‡AIçƒ­ç‚¹åˆ†æ")
            return state

        self.log(f"å¼€å§‹åˆ†æAIäº§å“ä¸ç§‘æŠ€çƒ­ç‚¹ï¼Œç›®æ ‡: {self.max_trends}ä¸ª")

        try:
            # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨mockæ¨¡å¼
            if self.mock_mode:
                self.log("ä½¿ç”¨Mockæ¨¡å¼ï¼ˆæ¨¡æ‹Ÿæ•°æ®ï¼‰")
                from src.agents.ai_trend_analyzer import AITrendAnalyzerAgent
                mock_agent = AITrendAnalyzerAgent(self.config, self.prompts)
                hot_topics = mock_agent._get_mock_ai_trends(state.get('topic', 'AI'))
            else:
                self.log("ä½¿ç”¨çœŸå®APIæ¨¡å¼ï¼ˆäº§å“+æ–°é—»+å­¦æœ¯ï¼‰")
                hot_topics = self._get_real_ai_trends()

            self.log(f"æˆåŠŸåˆ†æå‡º {len(hot_topics)} ä¸ªçƒ­ç‚¹è¯é¢˜")

            # ä¿å­˜çƒ­ç‚¹åˆ†æç»“æœ
            self._save_trends(hot_topics)

            # è®¡ç®—æ€»æ•°é‡
            total_count = sum(len(trends) for trends in hot_topics.values())

            # é€‰æ‹©çƒ­åº¦æœ€é«˜çš„è¯é¢˜ï¼ˆç”¨äºå…¼å®¹æ—§ä»£ç ï¼‰
            all_trends_flat = []
            for trends in hot_topics.values():
                all_trends_flat.extend(trends)

            if all_trends_flat:
                # æŒ‰çƒ­åº¦æ’åº
                all_trends_flat.sort(key=lambda x: x.get("heat_score", 0), reverse=True)
                selected_topic = all_trends_flat[0]
                self.log(f"é€‰æ‹©çƒ­ç‚¹è¯é¢˜: {selected_topic['title']}")
            else:
                selected_topic = {
                    "title": "AIæŠ€æœ¯å‘å±•",
                    "description": "äººå·¥æ™ºèƒ½å‰æ²¿åŠ¨æ€",
                    "url": "",
                    "source": "é»˜è®¤"
                }

            return {
                **state,
                "trends_by_source": hot_topics,
                "total_trends_count": total_count,
                "ai_hot_topics": all_trends_flat[:20],  # ä¿ç•™æ—§å­—æ®µå…¼å®¹
                "selected_ai_topic": selected_topic,
                "current_step": "ai_trend_analyzer_completed"
            }
        except Exception as e:
            self.log(f"AIçƒ­ç‚¹åˆ†æå¤±è´¥: {str(e)}", "ERROR")
            return {
                **state,
                "error_message": f"AIçƒ­ç‚¹åˆ†æå¤±è´¥: {str(e)}",
                "current_step": "ai_trend_analyzer_failed"
            }

    def _get_real_ai_trends(self) -> List[Dict[str, Any]]:
        """
        ä»å¤šä¸ªæ•°æ®æºè·å–çœŸå®AIçƒ­ç‚¹ï¼ˆäº§å“+æ–°é—»+å­¦æœ¯ï¼‰

        Returns:
            List[Dict[str, Any]]: çƒ­ç‚¹è¯é¢˜åˆ—è¡¨
        """
        all_trends = []

        # ===== äº§å“ç±»æ•°æ®æº =====

        # 1. Product Hunt - çƒ­é—¨AIäº§å“
        if self.sources["producthunt"]:
            try:
                ph_trends = self._get_product_hunt_trends()
                all_trends.extend(ph_trends)
                self.log(f"Product Hunt: è·å– {len(ph_trends)} æ¡çƒ­ç‚¹")
            except Exception as e:
                self.log(f"Product Huntè·å–å¤±è´¥: {e}", "WARNING")

        # 2. GitHub Trending - AIåº”ç”¨é¡¹ç›®
        if self.sources["github_apps"]:
            try:
                gh_trends = self._get_github_ai_apps()
                all_trends.extend(gh_trends)
                self.log(f"GitHub AIåº”ç”¨: è·å– {len(gh_trends)} æ¡çƒ­ç‚¹")
            except Exception as e:
                self.log(f"GitHub AIåº”ç”¨è·å–å¤±è´¥: {e}", "WARNING")

        # ===== æ–°é—»ç±»æ•°æ®æº =====

        # 3. TechCrunch AI
        if self.sources["techcrunch_ai"]:
            try:
                tc_trends = self._get_techcrunch_ai_trends()
                all_trends.extend(tc_trends)
                self.log(f"TechCrunch AI: è·å– {len(tc_trends)} æ¡çƒ­ç‚¹")
            except Exception as e:
                self.log(f"TechCrunch AIè·å–å¤±è´¥: {e}", "WARNING")

        # 4. The Verge AI
        if self.sources["verge_ai"]:
            try:
                verge_trends = self._get_verge_ai_trends()
                all_trends.extend(verge_trends)
                self.log(f"The Verge AI: è·å– {len(verge_trends)} æ¡çƒ­ç‚¹")
            except Exception as e:
                self.log(f"The Verge AIè·å–å¤±è´¥: {e}", "WARNING")

        # 5. VentureBeat AI
        if self.sources["venturebeat_ai"]:
            try:
                vb_trends = self._get_venturebeat_ai_trends()
                all_trends.extend(vb_trends)
                self.log(f"VentureBeat AI: è·å– {len(vb_trends)} æ¡çƒ­ç‚¹")
            except Exception as e:
                self.log(f"VentureBeat AIè·å–å¤±è´¥: {e}", "WARNING")

        # 6. NewsAPI.org
        if self.sources["newsapi"]:
            try:
                newsapi_trends = self._get_newsapi_trends()
                all_trends.extend(newsapi_trends)
                self.log(f"NewsAPI: è·å– {len(newsapi_trends)} æ¡çƒ­ç‚¹")
            except Exception as e:
                self.log(f"NewsAPIè·å–å¤±è´¥: {e}", "WARNING")

        # ===== å­¦æœ¯ç±»æ•°æ®æºï¼ˆé‡å¤§æ–°é—»ï¼‰ =====

        # 6. arXivé‡å¤§è®ºæ–‡æ–°é—»
        if self.sources["arxiv_news"]:
            try:
                arxiv_trends = self._get_arxiv_major_news()
                all_trends.extend(arxiv_trends)
                self.log(f"arXivé‡å¤§æ–°é—»: è·å– {len(arxiv_trends)} æ¡çƒ­ç‚¹")
            except Exception as e:
                self.log(f"arXivé‡å¤§æ–°é—»è·å–å¤±è´¥: {e}", "WARNING")

        # ===== ç§‘æŠ€æ–°é—»ï¼ˆè¿‡æ»¤äº§å“ç±»ï¼‰ =====

        # 7. HackerNewsï¼ˆäº§å“ç±»è¿‡æ»¤ï¼‰
        if self.sources["hackernews"]:
            try:
                hn_trends = self._get_hacker_news_products()
                all_trends.extend(hn_trends)
                self.log(f"HackerNewsäº§å“ç±»: è·å– {len(hn_trends)} æ¡çƒ­ç‚¹")
            except Exception as e:
                self.log(f"HackerNewsäº§å“ç±»è·å–å¤±è´¥: {e}", "WARNING")

        # ä¸å†æ’åºã€å»é‡ã€è¿‡æ»¤ï¼Œä¿ç•™æ‰€æœ‰æ•°æ®æºçš„å®Œæ•´å†…å®¹
        # æŒ‰æ•°æ®æºç»„ç»‡è¿”å›
        trends_by_source = {
            "Product Hunt": [],
            "GitHub": [],
            "TechCrunch AI": [],
            "The Verge AI": [],
            "VentureBeat AI": [],
            "NewsAPI": [],
            "arXiv": [],
            "Hacker News": []
        }

        # å°†çƒ­ç‚¹æŒ‰æ•°æ®æºåˆ†ç±»
        for trend in all_trends:
            source = trend.get("source", "")
            # ç¡®å®šæ•°æ®æºåˆ†ç±»
            if "Product Hunt" in source:
                trends_by_source["Product Hunt"].append(trend)
            elif "GitHub" in source:
                trends_by_source["GitHub"].append(trend)
            elif "TechCrunch" in source:
                trends_by_source["TechCrunch AI"].append(trend)
            elif "Verge" in source:
                trends_by_source["The Verge AI"].append(trend)
            elif "VentureBeat" in source:
                trends_by_source["VentureBeat AI"].append(trend)
            elif "NewsAPI" in source:
                trends_by_source["NewsAPI"].append(trend)
            elif "arXiv" in source:
                trends_by_source["arXiv"].append(trend)
            elif "Hacker" in source:
                trends_by_source["Hacker News"].append(trend)

        total_count = sum(len(trends) for trends in trends_by_source.values())

        self.log(f"æ•°æ®æºæ±‡æ€»å®Œæˆ: å…±{total_count}æ¡çƒ­ç‚¹")
        for source, trends in trends_by_source.items():
            if trends:
                self.log(f"  {source}: {len(trends)}æ¡")

        return trends_by_source

    # ==================== äº§å“ç±»æ•°æ®æº ====================

    def _get_product_hunt_trends(self) -> List[Dict[str, Any]]:
        """è·å–Product Huntçƒ­é—¨AIäº§å“ï¼ˆRSSï¼‰"""
        try:
            return self._get_rss_trends(
                rss_url="https://www.producthunt.com/posts/feed",
                source_name="Product Hunt",
                item_type="product",
                max_items=20
            )
        except Exception as e:
            self.log(f"Product Hunt RSSè§£æå¤±è´¥: {e}", "ERROR")
            return []

    def _get_github_ai_apps(self) -> List[Dict[str, Any]]:
        """è·å–GitHub Trending AIåº”ç”¨é¡¹ç›®ï¼ˆéæ¡†æ¶åº“ï¼‰"""
        try:
            api_url = "https://github-trending-api.now.sh/repositories"

            # æœç´¢AIåº”ç”¨ç›¸å…³çš„è¯­è¨€å’Œå…³é”®è¯
            search_terms = [
                ("", "weekly"),  # å…¨å±€çƒ­é—¨
                ("python", "weekly"),
                ("javascript", "weekly"),
                ("typescript", "weekly"),
            ]

            all_repos = []

            for lang, period in search_terms:
                try:
                    params = {
                        "language": lang if lang else None,
                        "since": period
                    }
                    params = {k: v for k, v in params.items() if v is not None}

                    response = requests.get(api_url, params=params, timeout=10)
                    repos = response.json()

                    for repo in repos[:10]:
                        repo["fetched_language"] = lang or "multi"
                        all_repos.append(repo)

                except Exception as e:
                    self.log(f"è·å–GitHub {lang}è¶‹åŠ¿å¤±è´¥: {e}", "WARNING")
                    continue

            trends = []

            for repo in all_repos[:50]:  # å–å‰50ä¸ªå€™é€‰
                # è¿‡æ»¤ï¼šä¿ç•™AIåº”ç”¨ç±»é¡¹ç›®
                name = repo.get("name", "").lower()
                description = repo.get("description", "").lower()
                combined = f"{name} {description}"

                # è¿‡æ»¤æ‰çº¯æŠ€æœ¯æ¡†æ¶/åº“
                skip_keywords = [
                    "framework", "library", "sdk", "api", "toolkit",
                    "boilerplate", "template", "wrapper", "binding"
                ]

                if any(kw in combined for kw in skip_keywords):
                    continue

                # ä¼˜å…ˆä¿ç•™AIåº”ç”¨ç±»é¡¹ç›®
                ai_keywords = [
                    "ai", "gpt", "chatbot", "agent", "assistant", "automation",
                    "copilot", "llm", "openai", "claude", "gemini", "stable diffusion",
                    "image", "video", "audio", "text", "code", "generation"
                ]

                if not any(kw in combined for kw in ai_keywords):
                    # éAIé¡¹ç›®é™ä½ä¼˜å…ˆçº§
                    continue

                stars_str = repo.get("stars", "0")
                stars = self._parse_stars(stars_str)
                forks = self._parse_stars(repo.get("forks", "0"))

                # è®¡ç®—çƒ­åº¦è¯„åˆ†
                heat_score = stars * 0.5 + forks * 0.3 + 50  # åŸºç¡€åˆ†50

                description = repo.get("description", "") or "AIåº”ç”¨é¡¹ç›®"
                lang = repo.get("fetched_language", repo.get("language", "Unknown"))

                trends.append({
                    "title": f"{repo['author']}/{repo['name']}",
                    "description": description[:200],
                    "url": repo["url"],
                    "source": f"GitHub ({lang})",
                    "timestamp": datetime.now().strftime("%Y-%m-%d"),
                    "metrics": {
                        "stars": stars_str,
                        "forks": repo.get("forks", "0"),
                        "language": lang
                    },
                    "heat_score": int(heat_score),
                    "tags": ["AIåº”ç”¨", "å¼€æº", lang]
                })

            return trends[:30]  # è¿”å›å‰30ä¸ª
        except Exception as e:
            self.log(f"GitHub AIåº”ç”¨è·å–å¤±è´¥: {e}", "ERROR")
            return []

    # ==================== æ–°é—»ç±»æ•°æ®æº ====================

    def _get_techcrunch_ai_trends(self) -> List[Dict[str, Any]]:
        """è·å–TechCrunch AIæ–°é—»ï¼ˆRSSï¼‰"""
        try:
            return self._get_rss_trends(
                rss_url="https://techcrunch.com/category/artificial-intelligence/feed/",
                source_name="TechCrunch AI",
                item_type="news",
                max_items=15
            )
        except Exception as e:
            self.log(f"TechCrunch AI RSSè§£æå¤±è´¥: {e}", "ERROR")
            return []

    def _get_verge_ai_trends(self) -> List[Dict[str, Any]]:
        """è·å–The Verge AIæ–°é—»ï¼ˆRSSï¼‰"""
        try:
            return self._get_rss_trends(
                rss_url="https://www.theverge.com/ai-artificial-intelligence/rss/index.xml",
                source_name="The Verge AI",
                item_type="news",
                max_items=15
            )
        except Exception as e:
            self.log(f"The Verge AI RSSè§£æå¤±è´¥: {e}", "ERROR")
            return []

    def _get_venturebeat_ai_trends(self) -> List[Dict[str, Any]]:
        """è·å–VentureBeat AIæ–°é—»ï¼ˆRSSï¼‰"""
        try:
            return self._get_rss_trends(
                rss_url="https://venturebeat.com/ai/feed/",
                source_name="VentureBeat AI",
                item_type="news",
                max_items=10
            )
        except Exception as e:
            self.log(f"VentureBeat AI RSSè§£æå¤±è´¥: {e}", "ERROR")
            return []

    def _get_newsapi_trends(self) -> List[Dict[str, Any]]:
        """è·å–NewsAPI.org AIæ–°é—»"""
        try:
            from src.utils.api_config import APIConfigManager

            api_config = APIConfigManager()
            api_key = api_config.get_api_key("newsapi")

            # NewsAPI endpoint for everything
            url = "https://newsapi.org/v2/everything"

            # AIç›¸å…³å…³é”®è¯æœç´¢
            ai_keywords = [
                "artificial intelligence",
                "machine learning",
                "deep learning",
                "AI model",
                "ChatGPT",
                "GPT-4",
                "LLM",
                "large language model",
                "OpenAI",
                "Anthropic",
                "Google Gemini",
                "Claude",
                "neural network",
                "computer vision",
                "NLP"
            ]

            # å°†å…³é”®è¯ç”¨ OR è¿æ¥
            query = " OR ".join(ai_keywords[:10])  # é™åˆ¶å…³é”®è¯æ•°é‡

            params = {
                "q": query,
                "apiKey": api_key,
                "language": "en",
                "sortBy": "publishedAt",
                "pageSize": 20,
                "searchIn": "title,description"
            }

            response = requests.get(url, params=params, timeout=15)
            response.raise_for_status()

            data = response.json()

            if data.get("status") != "ok":
                self.log(f"NewsAPIè¿”å›é”™è¯¯: {data.get('message', 'Unknown error')}", "ERROR")
                return []

            articles = data.get("articles", [])
            trends = []

            for article in articles:
                try:
                    title = article.get("title", "")
                    description = article.get("description", "")
                    url = article.get("url", "")
                    published = article.get("publishedAt", "")
                    source_name = article.get("source", {}).get("name", "NewsAPI")

                    # è¿‡æ»¤æ‰æ— æ ‡é¢˜æˆ–æ— URLçš„æ–‡ç« 
                    if not title or not url or title == "[Removed]":
                        continue

                    # æ¸…ç†æè¿°
                    if description:
                        import re
                        description = re.sub(r'<[^>]+>', '', description)
                        description = description.strip()[:300]

                    # è®¡ç®—çƒ­åº¦è¯„åˆ†
                    heat_score = 65  # NewsAPIåŸºç¡€åˆ†

                    # æ ¹æ®å…³é”®è¯åŠ åˆ†
                    title_lower = title.lower()
                    high_value_keywords = [
                        "breakthrough", "launch", "release", "announce", "unveil",
                        "openai", "gpt-4", "claude", "gemini", "llama", "mistral",
                        "billion", "funding", "investment", "acquisition"
                    ]

                    for keyword in high_value_keywords:
                        if keyword.lower() in title_lower:
                            heat_score += 5
                            break

                    trends.append({
                        "title": title,
                        "description": description or title[:200],
                        "url": url,
                        "source": f"NewsAPI ({source_name})",
                        "timestamp": published[:10] if published else datetime.now().strftime("%Y-%m-%d"),
                        "metrics": {
                            "published": published,
                            "source": source_name,
                            "type": "news"
                        },
                        "heat_score": heat_score,
                        "tags": ["AIæ–°é—»", "è¡Œä¸šèµ„è®¯"]
                    })

                except Exception as e:
                    self.log(f"å¤„ç†NewsAPIæ–‡ç« å¤±è´¥: {e}", "WARNING")
                    continue

            return trends[:20]  # è¿”å›å‰20æ¡

        except ValueError as e:
            # APIå¯†é’¥æœªé…ç½®
            self.log(f"NewsAPIå¯†é’¥æœªé…ç½®: {e}", "WARNING")
            return []
        except requests.exceptions.RequestException as e:
            self.log(f"NewsAPIè¯·æ±‚å¤±è´¥: {e}", "ERROR")
            return []
        except Exception as e:
            self.log(f"NewsAIè·å–å¤±è´¥: {e}", "ERROR")
            return []

    def _get_rss_trends(self, rss_url: str, source_name: str, item_type: str, max_items: int = 15) -> List[Dict[str, Any]]:
        """é€šç”¨RSSè·å–æ–¹æ³•"""
        try:
            feed = feedparser.parse(rss_url)

            if feed.bozo:
                self.log(f"{source_name} RSSè§£æè­¦å‘Š: {feed.bozo}", "WARNING")

            trends = []

            for entry in feed.entries[:max_items]:
                try:
                    title = entry.get("title", "")
                    description = entry.get("description", "")

                    # æ¸…ç†HTMLæ ‡ç­¾
                    if description:
                        import re
                        description = re.sub(r'<[^>]+>', '', description)
                        description = description.strip()[:300]

                    url = entry.get("link", "")
                    published = entry.get("published", "")

                    # è®¡ç®—çƒ­åº¦è¯„åˆ†
                    heat_score = 60  # RSSæºåŸºç¡€åˆ†

                    # æ ¹æ®ç±»å‹è°ƒæ•´
                    if item_type == "product":
                        heat_score += 20
                    elif item_type == "news":
                        # æ–°é—»ç±»ï¼šå…³é”®è¯åŠ åˆ†
                        news_keywords = ["å‘å¸ƒ", "æ¨å‡º", "èèµ„", "æ”¶è´­", "çªç ´", "å‘å¸ƒ", "launch", "raises", "acquisition"]
                        if any(kw.lower() in title.lower() for kw in news_keywords):
                            heat_score += 15

                        # çŸ¥åå…¬å¸åŠ åˆ†
                        companies = ["OpenAI", "Google", "Meta", "Microsoft", "Anthropic", "Apple", "Amazon"]
                        if any(company.lower() in title.lower() for company in companies):
                            heat_score += 10

                    trends.append({
                        "title": title,
                        "description": description or title[:200],
                        "url": url,
                        "source": source_name,
                        "timestamp": published[:10] if published else datetime.now().strftime("%Y-%m-%d"),
                        "metrics": {
                            "published": published,
                            "type": item_type
                        },
                        "heat_score": heat_score,
                        "tags": ["æ–°é—»", "AIèµ„è®¯"] if item_type == "news" else ["äº§å“", "AIå·¥å…·"]
                    })

                except Exception as e:
                    self.log(f"å¤„ç†{source_name}æ¡ç›®å¤±è´¥: {e}", "WARNING")
                    continue

            return trends
        except Exception as e:
            self.log(f"{source_name} RSSè·å–å¤±è´¥: {e}", "ERROR")
            return []

    # ==================== å­¦æœ¯ç±»æ•°æ®æºï¼ˆé‡å¤§æ–°é—»ï¼‰ ====================

    def _get_arxiv_major_news(self) -> List[Dict[str, Any]]:
        """è·å–arXivé‡å¤§è®ºæ–‡æ–°é—»ï¼ˆä»…é‡å¤§çªç ´ï¼‰"""
        try:
            import arxiv

            # æœç´¢AIç›¸å…³åˆ†ç±»
            query = "cat:cs.AI OR cat:cs.CL OR cat:cs.LG OR cat:cs.CV"

            search = arxiv.Search(
                query=query,
                max_results=50,  # è·å–æ›´å¤šå€™é€‰
                sort_by=arxiv.SortCriterion.SubmittedDate,
                sort_order=arxiv.SortOrder.Descending
            )

            trends = []
            cutoff_date = datetime.now() - timedelta(days=30)  # æ‰©å±•åˆ°30å¤©

            # çŸ¥åæœºæ„åˆ—è¡¨
            major_institutions = [
                "openai", "google", "deepmind", "meta", "anthropic",
                "microsoft", "stanford", "mit", "berkeley", "carnegie",
                "nvidia", "amazon", "apple"
            ]

            # é‡å¤§çªç ´å…³é”®è¯
            breakthrough_keywords = [
                "gpt", "claude", "gemini", "llama", "diffusion",
                "breakthrough", "sota", "record", "human-level",
                "reasoning", "agent", "multimodal", "vision"
            ]

            for result in search.results():
                pub_date = result.published.replace(tzinfo=None)
                if pub_date < cutoff_date:
                    continue

                title = result.title.lower()
                authors = [a.name.lower() for a in result.authors]

                # è¿‡æ»¤ï¼šå¿…é¡»æ˜¯çŸ¥åæœºæ„æˆ–é‡å¤§çªç ´
                is_major = False

                # æ£€æŸ¥ä½œè€…æ˜¯å¦æ¥è‡ªçŸ¥åæœºæ„
                for author in authors[:5]:
                    if any(inst in author for inst in major_institutions):
                        is_major = True
                        break

                # æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åŒ…å«é‡å¤§çªç ´å…³é”®è¯
                if not is_major:
                    if any(kw in title for kw in breakthrough_keywords):
                        is_major = True

                if not is_major:
                    continue  # è·³è¿‡æ™®é€šè®ºæ–‡

                # è®¡ç®—çƒ­åº¦è¯„åˆ†
                days_ago = (datetime.now() - pub_date).days
                heat_score = 80 - days_ago * 2  # åŸºç¡€åˆ†æ›´é«˜

                # é‡å¤§å…³é”®è¯åŠ åˆ†
                if any(kw in title for kw in breakthrough_keywords):
                    heat_score += 10

                trends.append({
                    "title": result.title,
                    "description": result.summary[:300],
                    "url": result.entry_id,
                    "source": "arXiv",
                    "timestamp": pub_date.strftime("%Y-%m-%d"),
                    "metrics": {
                        "authors": [a.name for a in result.authors[:3]],
                        "days_ago": days_ago
                    },
                    "heat_score": heat_score,
                    "tags": ["è®ºæ–‡", "å­¦æœ¯", "é‡å¤§çªç ´"]
                })

                if len(trends) >= 20:
                    break

            return trends
        except ImportError:
            self.log("arXivåº“æœªå®‰è£…ï¼Œè·³è¿‡ã€‚è¿è¡Œ: pip install arxiv", "WARNING")
            return []
        except Exception as e:
            self.log(f"arXivé‡å¤§æ–°é—»è·å–å¤±è´¥: {e}", "ERROR")
            return []

    # ==================== ç§‘æŠ€æ–°é—»ï¼ˆè¿‡æ»¤äº§å“ç±»ï¼‰ ====================

    def _get_hacker_news_products(self) -> List[Dict[str, Any]]:
        """è·å–HackerNewsäº§å“ç±»è¯é¢˜ï¼ˆè¿‡æ»¤æŠ€æœ¯ç»†èŠ‚ï¼‰"""
        try:
            stories_url = "https://hacker-news.firebaseio.com/v0/topstories.json"
            response = requests.get(stories_url, timeout=10)
            story_ids = response.json()[:50]

            trends = []

            # ä¿ç•™çš„å…³é”®è¯
            keep_keywords = [
                "show hn:", "launch", "release", "ai", "gpt", "openai",
                "product", "startup", "company", "raises", "funding",
                "acquired", "microsoft", "google", "apple", "meta"
            ]

            # è¿‡æ»¤çš„å…³é”®è¯ï¼ˆæŠ€æœ¯ç»†èŠ‚ï¼‰
            skip_keywords = [
                "tutorial", "how to", "guide", "tips", "best practices",
                "programming", "coding", "debug", "framework", "library"
            ]

            for story_id in story_ids:
                try:
                    item_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
                    item = requests.get(item_url, timeout=5).json()

                    if not item or "url" not in item:
                        continue

                    title = item.get("title", "").lower()

                    # è¿‡æ»¤ï¼šè·³è¿‡æŠ€æœ¯ç»†èŠ‚ç±»
                    if any(kw in title for kw in skip_keywords):
                        continue

                    # ä¼˜å…ˆä¿ç•™äº§å“ç±»
                    if not any(kw in title for kw in keep_keywords):
                        # éäº§å“ç±»é™ä½ä¼˜å…ˆçº§
                        continue

                    score = item.get("score", 0)
                    comments = item.get("descendants", 0)
                    heat_score = score * 2 + comments + 40  # åŸºç¡€åˆ†40

                    trends.append({
                        "title": item.get("title", ""),
                        "description": item.get("text", item.get("title", ""))[:200],
                        "url": item.get("url", ""),
                        "source": "Hacker News",
                        "timestamp": datetime.fromtimestamp(item["time"]).strftime("%Y-%m-%d %H:%M"),
                        "metrics": {
                            "upvotes": score,
                            "comments": comments
                        },
                        "heat_score": heat_score,
                        "tags": ["ç§‘æŠ€æ–°é—»", "äº§å“"]
                    })

                except Exception as e:
                    self.log(f"è·å–HNæ•…äº‹ {story_id} å¤±è´¥: {e}", "WARNING")
                    continue

            return trends[:30]
        except Exception as e:
            self.log(f"HackerNewsäº§å“ç±»è·å–å¤±è´¥: {e}", "ERROR")
            return []

    # ==================== è¾…åŠ©æ–¹æ³• ====================

    def _init_category_keywords(self):
        """åˆå§‹åŒ–åˆ†ç±»å…³é”®è¯"""
        # æŒ‰ä¼˜å…ˆçº§æ’åºçš„5å¤§åˆ†ç±»
        self.category_keywords = {
            "ğŸ“ˆ è¡Œä¸šåŠ¨æ€": {
                "keywords": [
                    "raises", "funding", "investment", "acquisition", "acquired", "merger",
                    "ipo", "valuation", "revenue", "strategy", "partnership", "collaboration",
                    "ceo", "founder", "startup", "company", "corporation", "launch", "release",
                    "business", "commercial", "enterprise", "deal"
                ],
                "icon": "ğŸ“ˆ",
                "priority": 1
            },
            "ğŸ“ å­¦æœ¯çªç ´": {
                "keywords": [
                    "paper", "research", "study", "arxiv", "publication", "publish",
                    "university", "institute", "lab", "professor", "scientist", "researcher",
                    "conference", "journal", "peer-reviewed", "dataset", "breakthrough",
                    "novel", "state-of-the-art", "sota"
                ],
                "icon": "ğŸ“",
                "priority": 2
            },
            "ğŸ”¬ æŠ€æœ¯åˆ›æ–°": {
                "keywords": [
                    "model", "algorithm", "architecture", "gpt", "claude", "gemini", "llama",
                    "diffusion", "transformer", "neural", "network", "training", "inference",
                    "framework", "engine", "system", "upgrade", "advance", "breakthrough",
                    "sota", "record", "human-level", "reasoning", "multimodal"
                ],
                "icon": "ğŸ”¬",
                "priority": 3
            },
            "ğŸ› ï¸ AIå·¥å…·/äº§å“": {
                "keywords": [
                    "tool", "platform", "service", "app", "software", "application",
                    "product", "saas", "solution", "assistant", "copilot", "chatbot",
                    "generator", "creator", "editor", "plugin", "extension", "integration",
                    "api", "sdk", "library", "package", "release", "launch", "update"
                ],
                "icon": "ğŸ› ï¸",
                "priority": 4
            },
            "ğŸ’¼ AIåº”ç”¨": {
                "keywords": [
                    "use case", "industry", "business", "workflow", "automation",
                    "implementation", "deployment", "integration", "solution", "case study",
                    "application", "enterprise", "organization", "company", "sector"
                ],
                "icon": "ğŸ’¼",
                "priority": 5
            }
        }

        # æ•°æ®æºåˆ°åˆ†ç±»çš„æ˜ å°„ï¼ˆç”¨äºåˆæ­¥åˆ†ç±»ï¼‰
        self.source_category_map = {
            "Product Hunt": "ğŸ› ï¸ AIå·¥å…·/äº§å“",
            "GitHub": "ğŸ’¼ AIåº”ç”¨",
            "TechCrunch AI": "ğŸ“ˆ è¡Œä¸šåŠ¨æ€",
            "The Verge AI": "ğŸ”¬ æŠ€æœ¯åˆ›æ–°",
            "VentureBeat AI": "ğŸ“ˆ è¡Œä¸šåŠ¨æ€",
            "arXiv": "ğŸ“ å­¦æœ¯çªç ´",
            "Hacker News": None  # HNéœ€è¦æ ¹æ®å†…å®¹åˆ¤æ–­
        }

    def _classify_trend(self, trend: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ™ºèƒ½åˆ†ç±»çƒ­ç‚¹è¯é¢˜

        Args:
            trend: çƒ­ç‚¹æ•°æ®

        Returns:
            åˆ†ç±»ä¿¡æ¯å­—å…¸
        """
        title = trend.get("title", "").lower()
        description = trend.get("description", "").lower()
        text = f"{title} {description}"

        # æ­¥éª¤1ï¼šåŸºäºæ•°æ®æºçš„åˆæ­¥åˆ†ç±»
        source = trend.get("source", "")
        base_category = self.source_category_map.get(source)

        # æ­¥éª¤2ï¼šåŸºäºå…³é”®è¯è®¡ç®—æ¯ä¸ªç±»åˆ«çš„åŒ¹é…åº¦
        category_scores = {}

        for category, config in self.category_keywords.items():
            keywords = config["keywords"]

            # è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°
            score = 0
            matched_keywords = []

            for keyword in keywords:
                if keyword.lower() in text:
                    score += 1
                    matched_keywords.append(keyword)

            # å¦‚æœæœ‰åŸºç¡€åˆ†ç±»ä¸”åŒ¹é…ï¼ŒåŠ åˆ†
            if base_category == category:
                score += 2

            category_scores[category] = {
                "score": score,
                "matched_keywords": matched_keywords
            }

        # æ­¥éª¤3ï¼šé€‰æ‹©æœ€é«˜åˆ†ç±»
        best_category = max(category_scores.items(), key=lambda x: x[1]["score"])
        category_name = best_category[0]
        category_info = self.category_keywords[category_name]

        # æ­¥éª¤4ï¼šåˆ¤æ–­æ˜¯å¦æ˜¯æœ‰æ•ˆåˆ†ç±»
        if best_category[1]["score"] == 0:
            # æ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•å…³é”®è¯ï¼Œæ ¹æ®æ•°æ®æºåˆ†é…é»˜è®¤åˆ†ç±»
            if base_category:
                category_name = base_category
                category_info = self.category_keywords[base_category]
            else:
                # å…œåº•åˆ†ç±»
                category_name = "ğŸ”¬ æŠ€æœ¯åˆ›æ–°"
                category_info = self.category_keywords[category_name]

        return {
            "category": category_name,
            "icon": category_info["icon"],
            "confidence": best_category[1]["score"],
            "matched_keywords": best_category[1]["matched_keywords"]
        }

    def _parse_stars(self, stars_str: str) -> int:
        """è§£æstaræ•°å­—å­—ç¬¦ä¸²"""
        if isinstance(stars_str, int):
            return stars_str

        stars_str = str(stars_str).replace(",", "").strip()

        if "k" in stars_str.lower():
            return int(float(stars_str.lower().replace("k", "")) * 1000)
        elif "m" in stars_str.lower():
            return int(float(stars_str.lower().replace("m", "")) * 1000000)
        else:
            try:
                return int(stars_str)
            except ValueError:
                return 0

    def _deduplicate_trends(self, trends: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å»é‡ç›¸ä¼¼çš„çƒ­ç‚¹è¯é¢˜"""
        seen_titles = set()
        unique_trends = []

        for trend in trends:
            title = trend.get("title", "").lower()
            # ç®€å•å»é‡ï¼šæ ‡é¢˜å®Œå…¨ç›¸åŒæˆ–åŒ…å«å…³ç³»
            is_duplicate = False
            for seen in seen_titles:
                if title == seen or title in seen or seen in title:
                    is_duplicate = True
                    break

            if not is_duplicate:
                unique_trends.append(trend)
                seen_titles.add(title)

        return unique_trends

    def _save_trends(self, trends_by_source: Dict[str, List[Dict[str, Any]]]):
        """ä¿å­˜çƒ­ç‚¹åˆ†æç»“æœåˆ°rawç›®å½•"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"trends_ai_{timestamp}.json"

            total_count = sum(len(trends) for trends in trends_by_source.values())

            output = {
                "timestamp": datetime.now().isoformat(),
                "total_trends": total_count,
                "data_sources": list(trends_by_source.keys()),
                "trends_by_source": trends_by_source,
                # ä¿ç•™æ—§æ ¼å¼å…¼å®¹
                "trends": []
            }

            # å±•å¹³æ‰€æœ‰è¶‹åŠ¿åˆ°æ—§æ ¼å¼
            for source, trends in trends_by_source.items():
                output["trends"].extend(trends)

            # ä½¿ç”¨æ–°çš„å­˜å‚¨ç®¡ç†å™¨ï¼Œä¿å­˜åˆ°rawç›®å½•
            filepath = self.storage.save_json("raw", filename, output)

            self.log(f"çƒ­ç‚¹åˆ†æå·²ä¿å­˜: {filepath}")
        except Exception as e:
            self.log(f"ä¿å­˜çƒ­ç‚¹åˆ†æå¤±è´¥: {str(e)}", "WARNING")
