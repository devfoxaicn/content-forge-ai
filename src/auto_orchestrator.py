"""
è‡ªåŠ¨åŒ–å†…å®¹ç”Ÿæˆå·¥ä½œæµ
æ•´åˆAIçƒ­ç‚¹åˆ†æã€é•¿æ–‡æœ¬ç”Ÿæˆã€å°çº¢ä¹¦ç²¾ç‚¼å’Œå‘å¸ƒçš„å®Œæ•´æµç¨‹
"""

import os
import sys
import yaml
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List

# LangGraph imports
from langgraph.graph import StateGraph, END

# æœ¬åœ°imports
from src.state import create_initial_state, update_state, add_agent_to_order, calculate_execution_time
from src.agents.ai_trend_analyzer_real import RealAITrendAnalyzerAgent
from src.agents.concurrent_fetch_agent import ConcurrentFetchAgent  # v11.0: å¹¶å‘æ•°æ®è·å–
from src.agents.time_weight_agent import TimeWeightAgent  # v11.0: æ—¶æ•ˆæ€§åŠ æƒ
from src.agents.auto_fact_check_agent import AutoFactCheckAgent  # v11.0: è½»é‡çº§äº‹å®æ ¸æŸ¥
from src.agents.content_enhancer_agent import ContentEnhancerAgent  # v11.0: å†…å®¹å¢å¼º
from src.agents.translation_refiner_agent import TranslationRefinerAgent  # v11.0: ç¿»è¯‘ç²¾ç‚¼
from src.agents.trend_categorizer_agent import TrendCategorizerAgent
from src.agents.news_scoring_agent import NewsScoringAgent
from src.agents.world_class_digest_agent_v8 import WorldClassDigestAgentV9  # v9.0: 6åˆ†ç±»ç³»ç»Ÿ
from src.utils.storage_v2 import StorageFactory
from src.utils.github_publisher import GitHubPublisher

# æ—¥å¿—é…ç½®
from loguru import logger


class AutoContentOrchestrator:
    """è‡ªåŠ¨åŒ–å†…å®¹ç”Ÿæˆåè°ƒå™¨ - æ–°å·¥ä½œæµ"""

    def __init__(self, config_path: str = "config/config.yaml"):
        """
        åˆå§‹åŒ–è‡ªåŠ¨åŒ–åè°ƒå™¨ï¼ˆAutoæ¨¡å¼ï¼šæ¯æ—¥çƒ­ç‚¹ç®€æŠ¥ï¼‰

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = self._load_config(config_path)
        self.prompts = self._load_prompts()
        self._setup_logging()

        # åˆå§‹åŒ–å­˜å‚¨ç®¡ç†å™¨ï¼ˆAutoæ¨¡å¼ï¼šæ¯æ—¥çƒ­ç‚¹ï¼‰
        self.storage = StorageFactory.create_daily(
            base_dir=self.config.get("storage", {}).get("base_dir", "data")
        )

        # åˆå§‹åŒ–Agent
        self.agents = self._init_agents()

        # æ„å»ºå·¥ä½œæµ
        self.workflow = self._build_workflow()

        logger.info("è‡ªåŠ¨åŒ–å†…å®¹ç”Ÿæˆåè°ƒå™¨åˆå§‹åŒ–å®Œæˆï¼ˆAutoæ¨¡å¼ï¼šæ¯æ—¥çƒ­ç‚¹ç®€æŠ¥ï¼‰")
        logger.info(f"æ•°æ®å­˜å‚¨ç›®å½•: {self.storage.get_root_dir()}")

    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            logger.info(f"é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_path}")
            return config
        except FileNotFoundError:
            logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            sys.exit(1)
        except yaml.YAMLError as e:
            logger.error(f"é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
            sys.exit(1)

    def _load_prompts(self) -> Dict[str, Any]:
        """åŠ è½½æç¤ºè¯é…ç½®"""
        prompts_file = self.config.get("prompts", {}).get("template_file", "config/prompts.yaml")
        try:
            with open(prompts_file, 'r', encoding='utf-8') as f:
                prompts = yaml.safe_load(f)
            logger.info(f"æç¤ºè¯æ–‡ä»¶åŠ è½½æˆåŠŸ: {prompts_file}")
            return {"prompts": prompts}
        except FileNotFoundError:
            logger.warning(f"æç¤ºè¯æ–‡ä»¶ä¸å­˜åœ¨: {prompts_file}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return {"prompts": {}}
        except yaml.YAMLError as e:
            logger.warning(f"æç¤ºè¯æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return {"prompts": {}}

    def _setup_logging(self):
        """é…ç½®æ—¥å¿—ï¼ˆæŒ‰æ—¥æœŸåˆ†å±‚å­˜å‚¨ï¼‰"""
        log_config = self.config.get("logging", {})

        # æ—¥å¿—çº§åˆ«
        level = log_config.get("level", "INFO")
        logger.remove()
        logger.add(sys.stderr, level=level)

        # æ–‡ä»¶æ—¥å¿— - ä½¿ç”¨æ—¥æœŸç›®å½•
        if log_config.get("file", {}).get("enabled", True):
            from datetime import datetime
            date_str = datetime.now().strftime("%Y%m%d")
            log_dir = f"logs/{date_str}"
            os.makedirs(log_dir, exist_ok=True)

            log_file = os.path.join(log_dir, "app.log")
            logger.add(
                log_file,
                rotation=log_config.get("file", {}).get("rotation", "100 MB"),
                retention=log_config.get("file", {}).get("retention", "30 days"),
                level=level
            )

    def _init_agents(self) -> Dict[str, Any]:
        """åˆå§‹åŒ–æ‰€æœ‰Agentï¼ˆAutoæ¨¡å¼ v11.0ï¼šå¹¶å‘æ•°æ®è·å– + æ—¶æ•ˆæ€§åŠ æƒ + 6åˆ†ç±»ç³»ç»Ÿ + è´¨é‡ä¿è¯ï¼‰"""
        agents = {}
        agents_config = self.config.get("agents", {})

        # ========== æ•°æ®è·å–å±‚ ==========
        # v11.0: ä¼˜å…ˆä½¿ç”¨å¹¶å‘æ•°æ®è·å–Agent
        if agents_config.get("concurrent_fetch", {}).get("enabled", False):
            agents["concurrent_fetch"] = ConcurrentFetchAgent(self.config, self.prompts)
            logger.info("ä½¿ç”¨ ConcurrentFetchAgentï¼ˆå¹¶å‘æ¨¡å¼ï¼‰")
        # é™çº§åˆ°åŒæ­¥æ¨¡å¼ï¼ˆå‘åå…¼å®¹ï¼‰
        elif agents_config.get("ai_trend_analyzer", {}).get("enabled", True):
            agents["ai_trend_analyzer"] = RealAITrendAnalyzerAgent(self.config, self.prompts)
            logger.info("ä½¿ç”¨ RealAITrendAnalyzerAgentï¼ˆåŒæ­¥æ¨¡å¼ï¼‰")

        # v11.0: æ—¶æ•ˆæ€§æ™ºèƒ½åŠ æƒAgent
        if agents_config.get("time_weight", {}).get("enabled", False):
            agents["time_weight"] = TimeWeightAgent(self.config, self.prompts)
            logger.info("ä½¿ç”¨ TimeWeightAgentï¼ˆæ—¶æ•ˆæ€§åŠ æƒï¼‰")

        # ========== åˆ†ç±»è¯„åˆ†å±‚ ==========
        # 2. çƒ­ç‚¹åˆ†ç±»Agent
        if agents_config.get("trends_digest", {}).get("enabled", True):
            agents["trend_categorizer"] = TrendCategorizerAgent(self.config, self.prompts)

        # 3. æ–°é—»è¯„åˆ†Agent
        if agents_config.get("news_scoring", {}).get("enabled", True):
            agents["news_scoring"] = NewsScoringAgent(self.config, self.prompts)

        # ========== è´¨é‡ä¿è¯å±‚ ==========
        # v11.0: è½»é‡çº§äº‹å®æ ¸æŸ¥Agent
        if agents_config.get("auto_fact_check", {}).get("enabled", False):
            agents["auto_fact_check"] = AutoFactCheckAgent(self.config, self.prompts)
            logger.info("ä½¿ç”¨ AutoFactCheckAgentï¼ˆäº‹å®æ ¸æŸ¥ï¼‰")

        # v11.0: å†…å®¹å¢å¼ºAgent
        if agents_config.get("content_enhancer", {}).get("enabled", False):
            agents["content_enhancer"] = ContentEnhancerAgent(self.config, self.prompts)
            logger.info("ä½¿ç”¨ ContentEnhancerAgentï¼ˆå†…å®¹å¢å¼ºï¼‰")

        # v11.0: ç¿»è¯‘ç²¾ç‚¼Agent
        if agents_config.get("translation_refiner", {}).get("enabled", False):
            agents["translation_refiner"] = TranslationRefinerAgent(self.config, self.prompts)
            logger.info("ä½¿ç”¨ TranslationRefinerAgentï¼ˆç¿»è¯‘ç²¾ç‚¼ï¼‰")

        # ========== è¾“å‡ºç”Ÿæˆå±‚ ==========
        # 4. ä¸–ç•Œé¡¶çº§ä¸­æ–‡ç®€æŠ¥Agent v9.0ï¼ˆ6åˆ†ç±»ç³»ç»Ÿ + 30æ•°æ®æº + Top5æˆªå–ï¼‰
        if agents_config.get("world_class_digest", {}).get("enabled", True):
            agents["world_class_digest"] = WorldClassDigestAgentV9(self.config, self.prompts)

        # æ³¨æ„ï¼šAutoæ¨¡å¼ä¸‹ä¸åˆå§‹åŒ–é•¿æ–‡æœ¬ã€å°çº¢ä¹¦ã€Twitterç­‰Agent
        # å¦‚éœ€ç”Ÿæˆå®Œæ•´å†…å®¹ï¼Œè¯·ä½¿ç”¨ Series æ¨¡å¼

        logger.info(f"Autoæ¨¡å¼ v11.0 å·²åˆå§‹åŒ– {len(agents)} ä¸ªAgent: {list(agents.keys())}")
        return agents

    def _build_workflow(self) -> StateGraph:
        """æ„å»ºè‡ªåŠ¨åŒ–å·¥ä½œæµï¼ˆAutoæ¨¡å¼ v11.0ï¼šå®Œæ•´è´¨é‡ä¿è¯æµç¨‹ï¼‰"""
        workflow = StateGraph(dict)

        # æ·»åŠ AgentèŠ‚ç‚¹
        for agent_name, agent in self.agents.items():
            workflow.add_node(agent_name, self._create_agent_node(agent))

        # v11.0: å®šä¹‰æ‰§è¡Œé¡ºåºï¼ˆå®Œæ•´å·¥ä½œæµï¼‰
        # concurrent_fetch â†’ time_weight â†’ trend_categorizer â†’ news_scoring
        # â†’ auto_fact_check â†’ content_enhancer â†’ translation_refiner â†’ world_class_digest â†’ END

        # ç¡®å®šå…¥å£ç‚¹ï¼ˆå¹¶å‘æˆ–åŒæ­¥ï¼‰
        if "concurrent_fetch" in self.agents:
            workflow.set_entry_point("concurrent_fetch")
            last_node = "concurrent_fetch"
            logger.info("å·¥ä½œæµå…¥å£: concurrent_fetch (å¹¶å‘æ¨¡å¼)")
        elif "ai_trend_analyzer" in self.agents:
            workflow.set_entry_point("ai_trend_analyzer")
            last_node = "ai_trend_analyzer"
            logger.info("å·¥ä½œæµå…¥å£: ai_trend_analyzer (åŒæ­¥æ¨¡å¼)")
        else:
            # å¦‚æœæ²¡æœ‰æ•°æ®è·å–Agentï¼Œç›´æ¥ä»åç»­æµç¨‹å¼€å§‹
            if "time_weight" in self.agents:
                workflow.set_entry_point("time_weight")
                last_node = "time_weight"
            elif "trend_categorizer" in self.agents:
                workflow.set_entry_point("trend_categorizer")
                last_node = "trend_categorizer"
            else:
                logger.warning("æ²¡æœ‰å¯ç”¨çš„æ•°æ®è·å–æˆ–åˆ†ç±»Agent")
                return workflow.compile()

        # æ—¶æ•ˆæ€§æ™ºèƒ½åŠ æƒAgent
        if "time_weight" in self.agents:
            workflow.add_edge(last_node, "time_weight")
            last_node = "time_weight"
            logger.info("å·¥ä½œæµ: æ·»åŠ æ—¶æ•ˆæ€§åŠ æƒ")

        # çƒ­ç‚¹åˆ†ç±»Agent
        if "trend_categorizer" in self.agents:
            workflow.add_edge(last_node, "trend_categorizer")
            last_node = "trend_categorizer"

        # æ–°é—»è¯„åˆ†Agent
        if "news_scoring" in self.agents:
            workflow.add_edge(last_node, "news_scoring")
            last_node = "news_scoring"

        # v11.0: è´¨é‡ä¿è¯å±‚
        # è½»é‡çº§äº‹å®æ ¸æŸ¥Agent
        if "auto_fact_check" in self.agents:
            workflow.add_edge(last_node, "auto_fact_check")
            last_node = "auto_fact_check"
            logger.info("å·¥ä½œæµ: æ·»åŠ äº‹å®æ ¸æŸ¥")

        # å†…å®¹å¢å¼ºAgent
        if "content_enhancer" in self.agents:
            workflow.add_edge(last_node, "content_enhancer")
            last_node = "content_enhancer"
            logger.info("å·¥ä½œæµ: æ·»åŠ å†…å®¹å¢å¼º")

        # ç¿»è¯‘ç²¾ç‚¼Agent
        if "translation_refiner" in self.agents:
            workflow.add_edge(last_node, "translation_refiner")
            last_node = "translation_refiner"
            logger.info("å·¥ä½œæµ: æ·»åŠ ç¿»è¯‘ç²¾ç‚¼")

        # ä¸–ç•Œé¡¶çº§ä¸­æ–‡ç®€æŠ¥Agent
        if "world_class_digest" in self.agents:
            workflow.add_edge(last_node, "world_class_digest")
            last_node = "world_class_digest"

        workflow.add_edge(last_node, END)

        return workflow.compile()

    def _create_agent_node(self, agent):
        """åˆ›å»ºAgentèŠ‚ç‚¹å‡½æ•°"""
        def node_function(state):
            logger.info(f"æ‰§è¡ŒAgent: {agent.name}")
            try:
                result = agent.execute(state)
                return add_agent_to_order(result, agent.name)
            except Exception as e:
                logger.error(f"Agent {agent.name} æ‰§è¡Œå¤±è´¥: {e}")
                return update_state(state, {
                    "error_message": str(e),
                    "current_step": f"{agent.name}_failed"
                })
        return node_function

    def run(self, topic: str = None, target_audience: str = "æŠ€æœ¯ä»ä¸šè€…",
            content_type: str = "å¹²è´§åˆ†äº«", keywords: list = None,
            user_provided_topic: dict = None) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´è‡ªåŠ¨åŒ–å·¥ä½œæµ

        Args:
            topic: å†…å®¹ä¸»é¢˜æ ‡è¯†ï¼ˆå¯é€‰ï¼Œç”¨äºæ–‡ä»¶å‘½åï¼Œç•™ç©ºåˆ™åŸºäºå®æ—¶çƒ­ç‚¹è‡ªåŠ¨ç”Ÿæˆï¼‰
            target_audience: ç›®æ ‡å—ä¼—
            content_type: å†…å®¹ç±»å‹
            keywords: å…³é”®è¯åˆ—è¡¨
            user_provided_topic: ç”¨æˆ·æŒ‡å®šçš„å®Œæ•´è¯é¢˜æ•°æ®ï¼ˆåŒ…å«title, description, keywordsç­‰ï¼‰ï¼Œ
                               å¦‚æœæä¾›åˆ™è·³è¿‡AIçƒ­ç‚¹åˆ†æï¼Œç›´æ¥ä½¿ç”¨è¯¥è¯é¢˜

        Returns:
            Dict[str, Any]: æœ€ç»ˆè¾“å‡º
        """
        # åˆ¤æ–­æ˜¯å¦ä¸ºç”¨æˆ·æŒ‡å®šè¯é¢˜æ¨¡å¼
        is_user_topic_mode = user_provided_topic is not None

        if is_user_topic_mode:
            # ç”¨æˆ·æŒ‡å®šè¯é¢˜æ¨¡å¼
            topic = topic or user_provided_topic.get("title", "user_topic")
            logger.info(f"ğŸ¯ ç”¨æˆ·æŒ‡å®šè¯é¢˜æ¨¡å¼: {topic}")
        elif topic is None:
            topic = "auto"
            logger.info("ğŸ“¡ å¼€å§‹æ‰§è¡Œè‡ªåŠ¨åŒ–å†…å®¹ç”Ÿäº§æµç¨‹ï¼ˆåŸºäºå®æ—¶çƒ­ç‚¹ï¼‰")
        else:
            logger.info(f"å¼€å§‹æ‰§è¡Œè‡ªåŠ¨åŒ–å†…å®¹ç”Ÿäº§æµç¨‹: {topic}")

        # åˆ›å»ºåˆå§‹çŠ¶æ€
        state = create_initial_state(
            topic=topic,
            target_audience=target_audience,
            content_type=content_type,
            keywords=keywords,
            config=self.config
        )

        # å¦‚æœæ˜¯ç”¨æˆ·æŒ‡å®šè¯é¢˜æ¨¡å¼ï¼Œè®¾ç½®é€‰ä¸­çš„è¯é¢˜ï¼Œè·³è¿‡AIçƒ­ç‚¹åˆ†æ
        if is_user_topic_mode:
            state["selected_ai_topic"] = {
                "title": user_provided_topic.get("title", topic),
                "description": user_provided_topic.get("description", ""),
                "source": "user_provided",
                "url": "",
                "tags": user_provided_topic.get("keywords", []),
                "key_points": [user_provided_topic.get("description", "")]
            }
            state["ai_hot_topics"] = [state["selected_ai_topic"]]
            state["current_step"] = "user_topic_set"
            logger.info(f"âœ… å·²è®¾ç½®ç”¨æˆ·æŒ‡å®šè¯é¢˜ï¼Œè·³è¿‡AIçƒ­ç‚¹åˆ†æ")

        # æ‰§è¡Œå·¥ä½œæµ
        try:
            result = self.workflow.invoke(state)
            result = calculate_execution_time(result)

            # ä¿å­˜è¾“å‡º
            self._save_output(result)

            # æ‰“å°ç»“æœæ‘˜è¦
            self._print_summary(result)

            logger.success(f"è‡ªåŠ¨åŒ–å†…å®¹ç”Ÿäº§å®Œæˆï¼è€—æ—¶: {result.get('execution_time', 0):.2f}ç§’")
            return result

        except Exception as e:
            logger.error(f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
            raise

    def _save_output(self, state: Dict[str, Any]):
        """ä¿å­˜è¾“å‡ºç»“æœåˆ°æŒ‰æ—¥æœŸåˆ†å±‚çš„ç›®å½•ï¼ˆAutoæ¨¡å¼ v3.0ï¼šåŸå§‹æ•°æ®+åˆ†ç±»ç®€æŠ¥ï¼‰"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # 1. ä¿å­˜AIçƒ­ç‚¹åŸå§‹æ•°æ®ï¼ˆJSONæ ¼å¼ï¼‰- åŒ…å«trends_by_source
        if state.get("trends_by_source"):
            trends_by_source = state["trends_by_source"]
            raw_filename = f"raw_topics_{timestamp}.json"
            raw_data = {
                "fetched_at": datetime.now().isoformat(),
                "total_topics": state.get("total_trends_count", 0),
                "data_sources": list(trends_by_source.keys()),
                "trends_by_source": trends_by_source,
                # å…¼å®¹æ—§æ ¼å¼
                "topics": state.get("ai_hot_topics", [])
            }
            raw_file = self.storage.save_json("raw", raw_filename, raw_data)
            logger.info(f"AIçƒ­ç‚¹åŸå§‹æ•°æ®å·²ä¿å­˜: {raw_file}")

        # 2. ä¿å­˜çƒ­ç‚¹ç®€æŠ¥ï¼ˆå…¼å®¹æ–°æ—§å­—æ®µï¼‰
        digest = state.get("trends_digest") or state.get("news_digest")
        if digest:
            self._save_digest(state, digest)

        logger.success(f"Autoæ¨¡å¼å†…å®¹å·²ä¿å­˜åˆ°: {self.storage.get_date_dir()}")

    def _format_twitter_thread(self, tweets: list) -> str:
        """æ ¼å¼åŒ–Twitter threadä¸ºMarkdown"""
        formatted_tweets = []
        for i, tweet in enumerate(tweets, 1):
            formatted_tweets.append(f"### Tweet {i}\n\n{tweet}\n")
        return "\n".join(formatted_tweets)

    def _save_digest(self, state: Dict[str, Any], digest: Dict[str, Any]):
        """ä¿å­˜çƒ­ç‚¹ç®€æŠ¥åˆ°digestç›®å½•ï¼ˆv7.0ï¼šæ”¯æŒå¢å¼ºJSONæ ¼å¼ï¼‰"""
        try:
            if not digest:
                return

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            issue_number = digest.get('issue_number', timestamp)
            base_filename = f"digest_{issue_number}"

            # å…¼å®¹ v7.0 æ–°æ ¼å¼å’Œæ—§æ ¼å¼
            markdown_content = digest.get('markdown_content') or digest.get('full_content', '')

            # ä¿å­˜Markdownæ ¼å¼ï¼ˆä¸»è¦æ ¼å¼ï¼Œç¬¦åˆaibookè¦æ±‚ï¼‰
            md_filename = f"{base_filename}.md"
            md_file = self.storage.save_markdown("digest", md_filename, markdown_content)
            logger.info(f"çƒ­ç‚¹ç®€æŠ¥Markdownå·²ä¿å­˜: {md_file}")

            # ä¿å­˜JSONæ ¼å¼ï¼ˆç”¨äºç½‘ç«™APIï¼‰
            json_filename = f"{base_filename}.json"

            # v7.0 æ ¼å¼ï¼šdigest æœ¬èº«å°±æ˜¯å®Œæ•´çš„JSONæ•°æ®
            # åªéœ€è¦ç¡®ä¿æ²¡æœ‰ markdown_content å­—æ®µçš„é‡å¤ï¼ˆå·²ç»åœ¨å¤–å±‚äº†ï¼‰
            json_data_to_save = dict(digest)

            # å¦‚æœæ˜¯æ—§æ ¼å¼ï¼Œè½¬æ¢ä¸ºæ–°æ ¼å¼
            if "editors_pick" not in json_data_to_save and "categories" not in json_data_to_save:
                # æ—§æ ¼å¼è½¬æ¢
                json_data_to_save = {
                    "metadata": {
                        "title": digest.get("title"),
                        "subtitle": digest.get("subtitle"),
                        "issue_number": issue_number,
                        "publish_date": digest.get("publish_date"),
                        "generated_at": datetime.now().isoformat(),
                        "word_count": digest.get("word_count"),
                        "reading_time": digest.get("reading_time"),
                        "total_topics": digest.get("total_topics"),
                        "version": digest.get("version", "v3.0")
                    },
                    "editors_pick": [],
                    "categories": [],
                    "core_insights": [],
                    "trending_topics": [],
                    "sources": digest.get("sources", []),
                    "topics": digest.get("topics", []),
                    "summary_analysis": digest.get("summary_analysis"),
                    "markdown_content": markdown_content
                }

            json_file = self.storage.save_json("digest", json_filename, json_data_to_save)
            logger.success(f"çƒ­ç‚¹ç®€æŠ¥å·²ä¿å­˜: {md_file} (MD) + {json_file} (JSON)")

            # ========== GitHubè‡ªåŠ¨å‘å¸ƒ ==========
            self._publish_to_github(md_file, json_file, digest)

        except Exception as e:
            logger.error(f"ä¿å­˜çƒ­ç‚¹ç®€æŠ¥å¤±è´¥: {e}")

    def _publish_to_github(self, md_file: str, json_file: str, digest: Dict[str, Any]):
        """å‘å¸ƒç®€æŠ¥åˆ°GitHub"""
        try:
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨GitHubå‘å¸ƒ
            enable_github_publish = self.config.get("agents", {}).get("ai_trend_analyzer", {}).get("github_publish", False)
            if not enable_github_publish:
                logger.info("GitHubå‘å¸ƒåŠŸèƒ½æœªå¯ç”¨ï¼Œè·³è¿‡è‡ªåŠ¨æ¨é€")
                return

            logger.info("å¼€å§‹å‘å¸ƒç®€æŠ¥åˆ°GitHub...")

            try:
                publisher = GitHubPublisher()

                # æ£€æŸ¥GitçŠ¶æ€
                status = publisher.check_git_status()
                logger.info(f"å½“å‰åˆ†æ”¯: {status['branch']}, æœ‰æ›´æ”¹: {status['has_changes']}")

                # å‘å¸ƒç®€æŠ¥
                success = publisher.publish_daily_digest(
                    digest_file=md_file,
                    json_file=json_file
                )

                if success:
                    logger.success(f"âœ… ç®€æŠ¥å·²æˆåŠŸæ¨é€åˆ°GitHub: {digest.get('title')}")
                else:
                    logger.warning("âš ï¸ GitHubæ¨é€å¤±è´¥ï¼Œä½†ç®€æŠ¥å·²ä¿å­˜åˆ°æœ¬åœ°")

            except Exception as e:
                logger.warning(f"GitHubå‘å¸ƒåŠŸèƒ½ä¸å¯ç”¨æˆ–å¤±è´¥: {e}")
                logger.info("ç®€æŠ¥å·²ä¿å­˜åˆ°æœ¬åœ°ï¼Œå¯æ‰‹åŠ¨æäº¤åˆ°GitHub")

        except Exception as e:
            logger.error(f"GitHubå‘å¸ƒå¤±è´¥: {e}")

    def _print_summary(self, state: Dict[str, Any]):
        """æ‰“å°ç»“æœæ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“ Autoæ¨¡å¼ v8.0 - ä¸–ç•Œé¡¶çº§AIæ–°é—»ç®€æŠ¥ç”Ÿæˆå®Œæˆ")
        print("="*60)

        # çƒ­ç‚¹ç®€æŠ¥ä¿¡æ¯
        digest = state.get('news_digest')
        if digest:
            print(f"\nğŸ“° çƒ­ç‚¹ç®€æŠ¥: {digest.get('title', 'N/A')}")
            print(f"   æœŸå·: #{digest.get('issue_number', 'N/A')}")
            print(f"   çƒ­ç‚¹æ•°é‡: {digest.get('total_topics', 0)} ä¸ª")
            print(f"   ç‰ˆæœ¬: {digest.get('version', 'v4.0')}")
            print(f"   å­—æ•°: {digest.get('word_count', 0)} å­—")
            print(f"   é˜…è¯»æ—¶é—´: {digest.get('reading_time', 'N/A')}")

        # AIçƒ­ç‚¹ä¿¡æ¯
        total_count = state.get('total_trends_count', 0)
        if total_count > 0:
            print(f"\nğŸ”¥ è·å–åˆ° {total_count} ä¸ªAIçƒ­ç‚¹ï¼ˆæŒ‰æ•°æ®æºæ±‡æ€»ï¼‰")

            # æ‰“å°æ•°æ®æºç»Ÿè®¡
            trends_by_source = state.get('trends_by_source', {})
            if trends_by_source:
                print("\nğŸ“Š æ•°æ®æºç»Ÿè®¡:")
                for source, items in trends_by_source.items():
                    if items:
                        print(f"   {source}: {len(items)} æ¡")

        print(f"\nâ±ï¸  æ‰§è¡Œè€—æ—¶: {state.get('execution_time', 0):.2f}ç§’")
        print(f"ğŸ“ å­˜å‚¨ä½ç½®: {self.storage.get_date_dir()}")
        print("\nğŸ’¡ æç¤ºï¼šå¦‚éœ€ç”Ÿæˆå®Œæ•´å†…å®¹ï¼Œè¯·ä½¿ç”¨ Customã€Refine æˆ– Series æ¨¡å¼")
        print("="*60 + "\n")
