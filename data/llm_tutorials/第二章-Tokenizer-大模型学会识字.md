# 第2章：Tokenizer——大模型学会"识字"

**阅读时间**：20分钟
**难度等级**：⭐

---

## 开篇故事

想象一下，如果你从来没学过中文，现在给你一本书，你会怎么做？

你可能会：
1. 盯着字看，觉得像一幅幅画
2. 不知道哪里是一个字的结束，哪里是下一个字的开始
3. 更不知道这些"画"是什么意思

**这就像大模型面对原始文本时的状态。**

在ChatGPT能理解你说的"今天天气怎么样"之前，它首先需要解决一个基础问题：

**如何把人类能看懂的"文字"，变成计算机能处理的"数字"？**

这个问题的答案，就是**Tokenizer（分词器）**。

[插图1：左边是人类文字"你好世界"，右边是数字序列[15496, 197, 7062, 17786]，中间是Tokenizer在转换]

---

## 2.1 为什么计算机不认识汉字？

### 计算机只认识0和1

这是计算机最本质的特点：
- 计算机内部只有两个状态：通电（1）和断电（0）
- 所有的数据，最终都要变成0和1的组合
- 用比喻：计算机像一个只有"开"和"关"两种状态的电灯开关

**那文字怎么办？**

我们需要一个"翻译器"，把文字变成计算机能理解的数字。

**这个过程叫"编码"（Encoding）。**

[插图2：文字"你好" → 编码器 → 数字"你好" → 计算机处理]

### 怎样把文字变成数字？

**方法1：给每个字一个编号**

就像给每个人发身份证：
- "你" = 12345
- "好" = 12346
- "世" = 12347
- "界" = 12348

用比喻：像给每个汉字编一个"身份证号"。

**问题1：中文字太多了！**
- 中文常用字：3000-5000个
- 中文总字数：超过6万个
- 如果每个字一个编号，需要6万个编号
- 用比喻：像要给6万人每个人都发身份证，管理起来很麻烦

**问题2：生僻字怎么办？**
- 有些字很罕见，但偶尔会用到
- 如果不给它编号，就无法处理
- 如果给所有可能的字都编号，编号系统会太大
- 用比喻：像字典太厚了，查起来很慢

**方法2：按笔画拆分**

把汉字拆成基本笔画：
- "你" = 亻（单人旁）+ 尔
- "好" = 女 + 子
- 用比喻：像拼图，把复杂的图案拆成简单的零件

**问题：**
- 笔画顺序不固定
- 同一个字有不同写法
- 处理起来很复杂

**有没有更好的方法？**

答案是：**不是按"字"分，而是按"词块"（Token）分。**

[插图3：对比图——左边是按字分（"你/好/世/界"），右边是按词块分（"你好/世界"）]

---

## 2.2 字符、词、字：不同的"切法"

### 想象切蛋糕

假设你有一个长条形的蛋糕，要切成小块，怎么切？

**方法1：切成很小的碎块**
- 每次切1厘米
- 结果：太多碎片，拼起来麻烦
- 用比喻：像按"字符"切（"你/好/世/界"）

**方法2：切成很大的块**
- 整个蛋糕不切
- 结果：太大一口吃不下
- 用比喻：像按"整个句子"切（"你好世界"）

**方法3：切成合适的大小**
- 根据蛋糕的纹理切
- 每块大小适中，方便食用
- 用比喻：像按"词块"切（"你好/世界"）

这就是Tokenizer的核心思想：**找到最合适的"切法"**。

[插图4：蛋糕切法示意图——碎块（字符）、整块（句子）、适中块（词块）]

### 方案对比表

| 切分方式 | 例子 | 优点 | 缺点 |
|---------|------|------|------|
| **按字符** | 你/好/世/界 | 简单，词表小 | 太碎，语义不完整 |
| **按词** | 你好/世界 | 语义完整 | 词语边界难判断 |
| **按子词** | 你/好/世/界 | 平衡，灵活 | 需要训练 |

### 为什么"按词"很难？

**中文没有空格！**

英文有天然的边界：
- "Hello world" → 空格分隔 → "Hello" 和 "world"

中文没有：
- "你好世界" → 哪里是词的边界？
- 是"你好/世界"？还是"你/好/世/界"？

**用比喻：**
- 英文像用积木搭的房子，积木之间有缝隙
- 中文像用泥巴捏的雕塑，没有明显的分界线

**问题来了：**
- "北京大学生" → "北京/大学生" 还是 "北京大学/生"？
- "乒乓球拍卖完了" → "乒乓球/拍卖/完了" 还是 "乒乓/球拍/卖/完了"？

这叫做"分词歧义"问题。

[插图5：分词歧义示意图——"北京大学生"可以有两种分法，产生不同含义]

### 为什么"按字符"不够好？

**太碎了！**

例子：**"artificial"**（人工的）

按字符分：
- a r t i f i c i a l
- 10个字符

按子词分：
- artific ial（或者：art ificial）
- 2个词块

**问题在哪里？**

1. **语义不完整**
   - "a" 什么都不是
   - "art" 有意义（艺术）
   - 用比喻：像把"苹果"切成"苹"和"果"，"苹"单独看没意义

2. **效率低**
   - 需要处理更多"块"
   - 计算量大
   - 用比喻：像搬砖，一次搬一块 vs 一次搬十块

3. **学习能力差**
   - 看不到"整体"模式
   - 难以学习词之间的规律
   - 用比喻：只见过树木，没见过森林

[插图6：字符 vs 子词对比——左边是零散的字符，右边是有意义的词块]

---

## 2.3 BPE算法：从简单到复杂的拼图

### 什么是"字节对编码"（BPE）？

**BPE = Byte Pair Encoding（字节对编码）**

核心思想：**从最简单的块开始，慢慢合并成复杂的块。**

**用比喻：乐高积木**

- 基础块：小方块（像字母）
- 组合：两个方块拼在一起（像字母组合）
- 复杂：多个方块拼成汽车（像单词）

**BPE的训练过程**：

假设我们要训练一个Tokenizer，文本是：
```
hug hug pug hug pug
```

**步骤1：统计字符频率**
```
h: 3次
u: 3次
g: 3次
p: 2次
(空格): 4次
```

**步骤2：找到最常见的相邻字符对**
- "ug" 出现3次（最多）
- 合并！把"ug"当作一个整体

新的词表：
```
h, u, g, p, (空格), ug
```

**步骤3：重新统计**
- 现在 "h" + "ug" = "hug" 出现3次（最多）
- 合并！把"hug"当作一个整体

新的词表：
```
h, u, g, p, (空格), ug, hug
```

**步骤4：继续**
- "p" + "ug" = "pug" 出现2次
- 合并！

最终词表：
```
h, u, g, p, (空格), ug, hug, pug
```

**用比喻：**
- 像教婴儿学说话
- 先学单个音（啊、哦）
- 再学音节（妈妈、爸爸）
- 最后学词语（妈妈抱）

[插图7：BPE合并过程——从单个字母到完整单词的演化]

### BPE的神奇之处

**它能自动发现"有意义的单元"！**

例子：英文单词

**阶段1（初始）**：所有字母
- a, b, c, d, e, f, g, h, i, ...

**阶段2（初步合并）**：常见组合
- th, ch, sh, ing, tion, ...

**阶段3（进一步合并）**：常见词根/词缀
- un, dis, re, able, ment, ...

**阶段4（最终）**：完整单词
- unhappy, disagreement, ...

**用比喻：**
- 像拼图
- 从最小的拼图块开始
- 逐步拼出完整的图案
- 最后得到完整的图景

[插图8：BPE层次结构——字母→字母组合→词根/词缀→完整单词]

### BPE对中文的处理

**中文BPE更复杂！**

因为：
- 中文没有空格
- 一个字本身就是一个语义单元
- 但有些词是多个字组成的

**例子："北京大学"**

**按字**：
- 北、京、大、学、4个字

**按词**：
- 北京、大学、2个词

**BPE可能学到**：
- 北、京、大、学（基础字）
- 北京、大学（常见词）
- 北京大学（如果经常一起出现）

**用比喻：**
- 像搭积木
- 基础块：单字
- 组合：常见的词
- 复杂组合：专有名词

[插图9：中文BPE示例——"北京大学"的分解]

---

## 2.4 分词器的"词库"有多大？

### 词表大小对比

不同模型的词表大小：

**GPT-2**：约5万个词块
**GPT-3**：约5万个词块
**GPT-4**：约10万个词块
**LLaMA（Meta）**：约3.2万个词块
**中文模型（如ChatGLM）**：约6-13万词块

**用比喻：**
- 人类词汇量：
  - 小学毕业：约3500字
  - 初中毕业：约5000字
  - 高中毕业：约7000字
  - 大学毕业：约10000字
  - 教授/作家：约20000字

**GPT的词表相当于"研究生"水平！**

[插图10：词表大小对比——从小学到博士的词汇量]

### 为什么中文词表更大？

**原因1：汉字多**
- 英文26个字母
- 中文6万个汉字（常用3000-5000个）

**原因2：编码方式**
- 英文：一个字母 = 1个字节
- 中文：一个汉字 = 3个字节（UTF-8编码）

**结果：**
- 同样的内容，中文需要更多的"块"（Token）
- 用比喻：中文"密度"更高，信息更浓缩

**例子：**
- 英文："Hello world" = 2个token（大约）
- 中文："你好世界" = 可能3-4个token

**为什么？**
- "Hello" = 1个token（常见词）
- "你好" = 可能是2个token（"你"+"好"）

**用比喻：**
- 英文像用乐高大块拼图（块少）
- 中文像用乐高小块拼图（块多，但更灵活）

[插图11：中英文Token对比——相同含义的文本，中文需要更多token]

---

## 2.5 特殊标记：大模型的"标点符号"

除了正常的词，Tokenizer还有特殊的"标记"（Tokens）。

这些标记有特殊的用途，就像文章的标点符号。

### 常见的特殊标记

**1. `<|startoftext|>` 或 `<BOS>`（Beginning of Sequence）**
- 含义：序列开始
- 作用：告诉大模型"我要开始说话了"
- 用比喻：像文章的标题，或者演讲的开场白"各位好"

**2. `<|endoftext|>` 或 `<EOS>`（End of Sequence）**
- 含义：序列结束
- 作用：告诉大模型"我说完了"
- 用比喻：像文章的句号，或者演讲的结束语"谢谢大家"

**3. `<|padding|>` 或 `<PAD>`**
- 含义：填充
- 作用：把短文本补长，让所有文本一样长
- 用比喻：像考试时，答案不够长，用空格凑字数

**4. `<|unk|>` 或 `<UNK>`（Unknown）**
- 含义：未知
- 作用：遇到词库里没有的词时用
- 用比喻：像字典里查不到的字，标注"生词"

**5. `<|sep|>` 或 `<SEP>`（Separator）**
- 含义：分隔符
- 作用：分隔不同的内容
- 用比喻：像文章的分段符号

[插图12：特殊标记示意图——展示各个特殊标记的用途]

### 特殊标记的实际应用

**例子：对话**

```
<BOS> 用户：你好 <SEP>
助手：你好！有什么可以帮助您的吗？ <EOS>
```

**例子：问答**

```
<BOS> 问题：什么是AI？ <SEP>
回答：人工智能是... <EOS>
```

**例子：文本补全**

```
<BOS> 床前明月 <PAD> <PAD>
（让模型预测下一个字）
```

**用比喻：**
- 这些特殊标记像"交通标志"
- 告诉大模型"哪里开始"、"哪里结束"、"哪里转弯"

[插图13：带特殊标记的文本流示意图]

---

## 2.6 Tokenizer的"小问题"

### 问题1：为什么有时候会"乱码"？

**现象：**
- 你输入："今天天气真好"
- 大模型输出：有时候会生成奇怪的字符

**原因：**
- Tokenizer不完美
- 有些词拆分得很奇怪
- 用比喻：像一句话被切成很碎的片段，拼起来意思变了

**例子：**
- "artificial" 可能被分成：art ificial 或 artific ial
- 取决于训练时怎么学

**用比喻：**
- 像"北京大学生"的分词歧义
- 不同分法，意思完全不同

[插图14：分词错误导致的乱码示意图]

### 问题2：为什么中文输入字数限制更严格？

**现象：**
- ChatGPT：英文可以输入更多字
- 同样的意思，中文更快达到"token上限"

**原因：**
- 中文"密度"高
- 一个中文字可能 = 多个英文单词的token

**例子：**
```
英文：I love artificial intelligence (4个token)
中文：我爱人工智能 (可能是6-8个token)
```

**用比喻：**
- 英文像用"大卡车"运货（一次运很多）
- 中文像用"小推车"运货（需要多次）

**实际影响：**
- ChatGPT的token限制：约4096个token（GPT-3.5）
- 英文：大约3000个单词
- 中文：大约2000-2500个字

[插图15：中英文token占用对比图]

### 问题3：如何解决这些问题？

**方法1：使用更大的词表**
- 增加词块数量
- 更好地覆盖常用词
- 用比喻：字典越厚，能查到的词越多

**方法2：针对中文优化**
- 单独训练中文Tokenizer
- 考虑中文特点
- 用比喻：专门编中文字典，而不是翻译英文字典

**方法3：智能分段**
- 不是固定切分
- 根据语义动态调整
- 用比喻：像说话时的停顿，根据意思换气

[插图16：优化方案对比图]

---

## 本章小结

### Tokenizer的核心作用

**一句话总结：**
> Tokenizer是大模型学会"识字"的第一步，它把人类能理解的"文字"，变成计算机能处理的"数字"。

**它的作用：**
1. **分词**：把文本切成合适的"块"（Token）
2. **编码**：给每个块一个数字编号
3. **特殊标记**：添加开始、结束、分隔等标记

**像什么？**
- 像字典：把词和数字对应起来
- 像翻译器：把文字翻译成数字
- 像切蛋糕：把大文本切成合适的块

### 为什么Tokenizer很重要？

**原因1：影响理解能力**
- 切得好 → 模型容易理解语义
- 切得不好 → 模型理解困难
- 用比喻：像读书，知道哪里该停顿很重要

**原因2：影响效率**
- 词表太大 → 搜索慢
- 词表太小 → 需要更多token
- 用比喻：字典太厚查得慢，太薄查不到

**原因3：决定性能上限**
- Tokenizer是第一道关卡
- 它的好坏影响后续所有环节
- 用比喻：像地基，地基不好，房子盖不高

### 三种主流分词方法

| 方法 | 原理 | 优点 | 缺点 | 代表 |
|------|------|------|------|------|
| **BPE** | 从简单到复杂合并 | 灵活，不需要标注 | 可能切分不完美 | GPT系列 |
| **WordPiece** | 选择使语言模型概率最大的合并 | 适合预训练 | 需要大量数据 | BERT |
| **Unigram** | 从大词表逐步删除 | 性能好 | 计算复杂 | T5, ALBERT |

### 下一步

现在大模型已经学会了"识字"（Tokenizer）。

接下来要问：
- 它是怎么"学习"这些字的含义和关系的？
- 如何从海量文本中学习规律？

这就是第3章要讲的：**Pretraining（预训练）**。

**就像婴儿学会了认字，接下来要开始读书了。**

---

**本章思考题**：

1. 为什么不能简单地按"字"或"词"来分词？BPE算法的优势在哪里？
2. 为什么中文的Tokenizer通常比英文的大？
3. 如果让你设计一个Tokenizer，你会如何处理"北京大学生"这样的歧义？

---

*"学会识字，是理解世界的第一步。对于人类如此，对于AI也是如此。"*
