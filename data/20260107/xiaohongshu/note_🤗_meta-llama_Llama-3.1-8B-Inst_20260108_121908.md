# åŠ è½½ Tokenizer

tokenizer = AutoTokenizer.from_pretrained(model_id)

# åŠ è½½æ¨¡å‹ (torch_dtype=torch.bfloat16 é€‚åˆ Ampere æ¶æ„åŠä»¥ä¸Šæ˜¾å¡)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    device_map="auto",  # è‡ªåŠ¨åˆ†é…è®¾å¤‡
)

# æ„é€  Prompt (Llama 3.1 å®˜æ–¹æ¨èæ¨¡æ¿)
messages = [
    {"role": "system", "content": "You are a professional coding assistant."},
    {"role": "user", "content": "Write a Python function to calculate Fibonacci numbers."}
]

input_ids = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_tensors="pt"
).to(model.device)

# æ¨ç†é…ç½®
terminators = [
    tokenizer.eos_token_id,
    tokenizer.convert_tokens_to_ids("<|eot_id|>")
]

with torch.no_grad():
    outputs = model.generate(
        input_ids,
        max_new_tokens=512,
        eos_token_id=terminators,
        do_sample=True,
        temperature=0.6,
        top_p=0.9,
    )
    
response = outputs[0][input_ids.shape[-1]:]
print(tokenizer.decode(response, skip_special_tokens=True))
```

---

### âš ï¸ é¿å‘æŒ‡å—ä¸æœ€ä½³å®è·µ

åœ¨è½åœ° Llama 3.1-8B æ—¶ï¼Œè¯·åŠ¡å¿…æ³¨æ„ä»¥ä¸‹å¸¸è§é—®é¢˜ï¼š

1.  **æ˜¾å­˜ç“¶é¢ˆä¸é‡åŒ–**
    *   **å‘ç‚¹**ï¼šFP16 ç²¾åº¦ä¸‹æ¨¡å‹æƒé‡çº¦éœ€ 16GB æ˜¾å­˜ï¼ŒåŠ ä¸Š KV Cache å®¹æ˜“ OOM (Out of Memory)ã€‚
    *   **è§£æ³•**ï¼šå¯¹äº 8GB-12GB æ˜¾å­˜çš„ç”¨æˆ·ï¼Œæ¨èä½¿ç”¨ **AWQ** æˆ– **GPTQ** 4-bit é‡åŒ–ç‰ˆæœ¬ (å¦‚ `TheBloke/Llama-3.1-8B-Instruct-AWQ`)ï¼Œèƒ½åœ¨æŸå¤±æå°ç²¾åº¦çš„æƒ…å†µä¸‹å¤§å¹…é™ä½æ˜¾å­˜éœ€æ±‚ã€‚

2.  **é•¿æ–‡æœ¬çš„â€œè¿·å¤±â€**
    *   **å‘ç‚¹**ï¼šè™½ç„¶æ”¯æŒ 128K ä¸Šä¸‹æ–‡ï¼Œä½†åœ¨å¤„ç†è¶…è¿‡ 32K çš„æ–‡æœ¬æ—¶ï¼Œæ¨¡å‹å¯èƒ½ä¼šå‡ºç°â€œä¸­é—´è¿·å¤±â€ ç°è±¡ï¼Œå³é—å¿˜ä¸Šä¸‹æ–‡ä¸­é—´çš„ç»†èŠ‚ã€‚
    *   **è§£æ³•**ï¼šåœ¨ RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ) åº”ç”¨ä¸­ï¼Œå°½é‡é€šè¿‡ Chunking å’Œ Re-ranking å°†è¾“å…¥ Token æ§åˆ¶åœ¨ 8K-16K ä»¥å†…ï¼Œä»¥ä¿è¯æ¨ç†è´¨é‡ã€‚

3.  **System Prompt éµå¾ªåº¦**
    *   **å‘ç‚¹**ï¼šéƒ¨åˆ†å¾®è°ƒç‰ˆæœ¬å¯¹ System Prompt çš„éµå¾ªèƒ½åŠ›è¾ƒå¼±ã€‚
    *   **è§£æ³•**ï¼šå®˜æ–¹ Instruct ç‰ˆæœ¬ç»è¿‡ RLHF å¼ºåŒ–ï¼Œå»ºè®®ç›´æ¥ä½¿ç”¨å®˜æ–¹åŸºåº§ï¼Œå¹¶åœ¨ System Prompt ä¸­æ˜ç¡®æŒ‡ä»¤è¾¹ç•Œã€‚

4.  **Function Calling æ ¼å¼**
    *   **æ³¨æ„**ï¼šLlama 3.1 åŸç”Ÿæ”¯æŒå·¥å…·è°ƒç”¨ï¼Œä½†è¾“å‡ºæ ¼å¼éœ€ä¸¥æ ¼éµå¾ª `<|python_tag|>` æˆ–ç‰¹å®š JSON ç»“æ„ï¼Œè§£æè¾“å‡ºæ—¶éœ€åšå¥½å¼‚å¸¸å¤„ç†ã€‚

---

**æ€»ç»“**ï¼šLlama 3.1-8B-Instruct å‡­å€Ÿ **GQA æ¶æ„**å¸¦æ¥çš„æ¨ç†çº¢åˆ©å’Œ **128K é•¿æ–‡æœ¬**èƒ½åŠ›ï¼Œæ­£å¼å®£å‘Šäº†ç«¯ä¾§é«˜æ€§èƒ½ AI æ—¶ä»£çš„åˆ°æ¥ã€‚å¯¹äºå¼€å‘è€…è€Œè¨€ï¼Œç°åœ¨æ˜¯å°†å…¶æ¥å…¥ç”Ÿäº§ç¯å¢ƒçš„æœ€ä½³æ—¶æœºã€‚ğŸš€



### ğŸ’¡ æ ¸å¿ƒæ´å¯Ÿ

Meta å‘å¸ƒçš„ **Llama 3.1-8B-Instruct** æ­£åœ¨å¼•å‘å¼€æºç¤¾åŒºçš„æµ·å•¸ã€‚å‡­å€Ÿ **1200ä¸‡+** çš„ä¸‹è½½é‡å’Œè¡Œä¸šé¢†å…ˆçš„æ€§èƒ½ï¼Œå®ƒè¿…é€Ÿæˆä¸º 8B å‚æ•°é‡çº§çš„ **SOTA (State of the Art)** æ¨¡å‹ã€‚

ä¸ä¸Šä¸€ä»£ç›¸æ¯”ï¼ŒLlama 3.1 ä¸ä»…ä»…æ˜¯å¾®è°ƒï¼Œè€Œæ˜¯æ¶æ„å±‚é¢çš„è¿›åŒ–ã€‚å®ƒä¸ä»…åœ¨é€šç”¨èƒ½åŠ›ä¸Šé€¼è¿‘ GPT-4ï¼Œæ›´åœ¨ **128K ä¸Šä¸‹æ–‡çª—å£** å’Œ **æ¨ç†æ•ˆç‡** ä¸Šå®ç°äº†è´¨çš„é£è·ƒï¼Œæˆä¸ºæ¶ˆè´¹çº§æ˜¾å¡æœ¬åœ°éƒ¨ç½²çš„é¦–é€‰åŸºåº§ã€‚

---

---
**æ ‡ç­¾**: #HuggingFace #æ¨¡å‹ #AI
**å­—æ•°**: 3556
**å‹ç¼©ç‡**: 66.6%
