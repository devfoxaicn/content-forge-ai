# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

**Current Version**: v7.0 (Latest: v7.0 with 6-dimensional news scoring and Chinese digest)

**IMPORTANT**: Always run commands with PYTHONPATH set to the project root directory.

**Project Root**: `/Users/z/Documents/work/content-forge-ai` (adjust if different)

## Quick Reference

**Essential Commands**:
```bash
# Set PYTHONPATH (required for all commands - adjust path to your project root)
export PYTHONPATH=/Users/z/Documents/work/content-forge-ai

# ========== Auto Mode v7.0 (Chinese AI News Digest with Scoring) ==========
# Run once (recommended for daily AI news digest)
PYTHONPATH=/Users/z/Documents/work/content-forge-ai python src/main.py --mode auto --once

# ========== Series Mode (100-episode blog series) ==========
# View progress
PYTHONPATH=/Users/z/Documents/work/content-forge-ai python src/main.py --mode series --progress
# Generate single episode
PYTHONPATH=/Users/z/Documents/work/content-forge-ai python src/main.py --mode series --episode 1
# Generate range
PYTHONPATH=/Users/z/Documents/work/content-forge-ai python src/main.py --mode series --all --start 1 --end 10

# ========== Custom Mode (user-defined topics) ==========
PYTHONPATH=/Users/z/Documents/work/content-forge-ai python src/main.py --mode custom --topic "RAGæŠ€æœ¯åŽŸç†ä¸Žå®žæˆ˜"

# ========== Refine Mode (multi-platform content refining) ==========
PYTHONPATH=/Users/z/Documents/work/content-forge-ai python src/main.py --mode refine --input article.md

# ========== Tests ==========
cd test
PYTHONPATH=/Users/z/Documents/work/content-forge-ai python test_ai_trends.py --source hackernews
PYTHONPATH=/Users/z/Documents/work/content-forge-ai python test_storage.py
```

**Core Files**:
- `src/main.py` - Unified entry point (use `--mode` to switch)
- `config/config.yaml` - Main config (LLM, agents, 15+ data sources)
- `config/blog_topics_100_complete.json` - 100-episode content plan
- `config/prompts.yaml` - Agent system prompt templates
- `src/utils/storage_v2.py` - Unified storage (StorageFactory)
- `src/utils/api_config.py` - API configuration manager

**Key Architecture Points**:
1. **Four-Mode Architecture**: Auto (v7.0 Chinese digest with scoring), Series (100 topics), Custom (user-defined), Refine (multi-platform)
2. **Auto Mode v7.0**: 15+ data sources â†’ åˆ†ç±»ç»„ç»‡ â†’ è¯„åˆ†ç­›é€‰ â†’ å…¨ä¸­æ–‡ç®€æŠ¥
3. **DailyStorage**: Only creates `raw/` and `digest/` directories
4. **Immutable State Updates**: Use `{**state, **updates}` pattern

## Project Overview

ContentForge AI v7.0 is a LangChain/LangGraph-based automated content production system that generates AI-focused content in Chinese and English across multiple formats.

**Auto Mode v7.0** (Latest):
- **14 Data Sources**: TechCrunch AI, NewsAPI.org, Hacker News, MIT Tech Review, OpenAI Blog, BAIR Blog, Microsoft Research, arXiv, MarkTechPost, KDnuggets, AI Business, The Gradient, InfoQ AI, Hugging Face Blog
- **4 Agents**: AI Trend Analyzer â†’ Trend Categorizer â†’ News Scoring â†’ World Class Digest (å…¨ä¸­æ–‡)
- **5 Categories**: äº§ä¸šåŠ¨æ€, å­¦æœ¯å‰æ²¿, æŠ€æœ¯åˆ›æ–°, äº§å“å·¥å…·, è¡Œä¸šåº”ç”¨
- **Scoring System**: 6-dimensional scoring (source_authority 30%, engagement 20%, freshness 15%, category_balance 15%, content_quality 10%, diversity 10%)
- **Output**: `data/daily/YYYYMMDD/digest/digest_YYYYMMDD.md` (å…¨ä¸­æ–‡, with structured JSON)

**Other Modes**:
- **Series Mode**: 100-episode technical blog series (planned, episodes 1-100)
- **Custom Mode**: User-defined topic content generation
- **Refine Mode**: Multi-platform content adaptation (WeChat/Xiaohongshu/Twitter)

## Environment Setup

**Required API Keys** (`.env`):
- `ZHIPUAI_API_KEY` - Primary LLM provider (https://open.bigmodel.cn/)

**Optional Keys**:
- `TAVILY_API_KEY` - Web search (for ResearchAgent)
- `NEWSAPI_KEY` - NewsAPI.org data source
- `OPENAI_API_KEY` - Backup LLM

**Dependencies**:
```bash
pip install langgraph langchain langchain-openai loguru pyyaml python-dotenv arxiv praw
```

## Auto Mode v7.0 Architecture

**Workflow**:
```
1. RealAITrendAnalyzerAgent
   - ä»Ž15+ä¸ªæ•°æ®æºèŽ·å–çƒ­ç‚¹
   - ä¿ç•™æ‰€æœ‰å†…å®¹ï¼ˆä¸åŽ»é‡ã€ä¸æŽ’åºï¼‰
   - è¾“å‡º: trends_by_source

2. TrendCategorizerAgent
   - æŒ‰åˆ†ç±»ç»„ç»‡çƒ­ç‚¹
   - 5å¤§åˆ†ç±»ï¼šäº§ä¸šåŠ¨æ€ã€å­¦æœ¯å‰æ²¿ã€æŠ€æœ¯åˆ›æ–°ã€äº§å“å·¥å…·ã€è¡Œä¸šåº”ç”¨
   - è¾“å‡º: categorized_trends

3. NewsScoringAgent (v7.0æ–°å¢ž)
   - å¯¹æ–°é—»è¿›è¡Œ6ç»´åº¦è¯„åˆ†
   - æ™ºèƒ½ç­›é€‰ï¼Œä¿ç•™é«˜ä»·å€¼å†…å®¹
   - è¾“å‡º: scored_trends

4. WorldClassDigestAgent
   - ç”Ÿæˆå…¨ä¸­æ–‡ä¸–ç•Œé¡¶çº§æ–°é—»ç®€æŠ¥
   - ç¿»è¯‘æ‰€æœ‰æ ‡é¢˜ã€æè¿°
   - ç”Ÿæˆæ ¸å¿ƒæ´žå¯Ÿå’Œæ·±åº¦è§‚å¯Ÿ
   - è¾“å‡º: news_digest (å…¨ä¸­æ–‡ + ç»“æž„åŒ–JSON)
```

**Data Sources** (14 enabled sources):
| æ•°æ®æº | ç±»åž‹ | å†…å®¹ |
|--------|------|------|
| TechCrunch AI | æ–°é—» | AIè¡Œä¸šæ–°é—»RSS |
| NewsAPI.org | æ–°é—» | å…¨çƒAIæ–°é—»èšåˆï¼ˆéœ€API keyï¼‰ |
| Hacker News | ç¤¾åŒº | ç§‘æŠ€çƒ­ç‚¹è®¨è®ºAPI |
| MIT Tech Review | æ–°é—» | MITæŠ€æœ¯è¯„è®ºRSS |
| OpenAI Blog | å®˜æ–¹ | OpenAIå®˜æ–¹åŠ¨æ€RSS |
| BAIR Blog | å­¦æœ¯ | UC Berkeley AIç ”ç©¶RSS |
| Microsoft Research | å­¦æœ¯ | å¾®è½¯ç ”ç©¶é™¢åšå®¢RSS |
| arXiv | å­¦æœ¯ | AIé‡å¤§è®ºæ–‡API |
| MarkTechPost | æ–°é—» | AIç ”ç©¶æ–°é—»RSS |
| KDnuggets | æ–°é—» | æ•°æ®ç§‘å­¦æƒå¨RSS |
| AI Business | æ–°é—» | AIè¡Œä¸šæ–°é—»RSS |
| The Gradient | æœŸåˆŠ | AIç ”ç©¶æœŸåˆŠRSS |
| InfoQ AI | æŠ€æœ¯ | æŠ€æœ¯åª’ä½“RSS |
| Hugging Face | å®˜æ–¹ | Hugging Faceå®˜æ–¹åšå®¢RSS |

**Scoring System** (NewsScoringAgent):
- `source_authority` (30%): æ¥æºæƒå¨åº¦ï¼ŒåŸºäºŽé¢„å®šä¹‰è¯„åˆ†è¡¨
- `engagement` (20%): äº’åŠ¨æ•°æ®ï¼ˆç‚¹èµžã€è¯„è®ºã€åˆ†äº«ï¼‰
- `freshness` (15%): æ—¶æ•ˆæ€§ï¼ˆ24å°æ—¶å†…å‘å¸ƒåŠ åˆ†ï¼‰
- `category_balance` (15%): ç¡®ä¿å„åˆ†ç±»å¹³è¡¡
- `content_quality` (10%): æ ‡é¢˜è´¨é‡ã€å†…å®¹å®Œæ•´æ€§
- `diversity` (10%): ç¡®ä¿æ¥æºå¤šæ ·æ€§

**Output Format**:
```markdown
# AIæ¯æ—¥çƒ­ç‚¹ Â· 2026å¹´01æœˆ22æ—¥

## ðŸ’¡ æ ¸å¿ƒæ´žå¯Ÿ
- å¤šæ™ºèƒ½ä½“åä½œèŒƒå¼ç¡®ç«‹...

## ðŸ“° æ·±åº¦è§‚å¯Ÿ
**AIäº§ä¸šè§‚å¯Ÿï¼šä»Žäº‘ç«¯ç«žé€åˆ°ç«¯ä¾§é‡æž„çš„èŒƒå¼è½¬ç§»**

## ðŸ” æœ¬æœŸçƒ­ç‚¹
### ðŸ“ˆ äº§ä¸šåŠ¨æ€ï¼ˆ15æ¡ï¼Œå·²ç­›é€‰ï¼‰
#### [æ®æŠ¥Appleç ”å‘AIå¯ç©¿æˆ´è®¾å¤‡](é“¾æŽ¥)
**æ¥æº**ï¼šTechCrunch AI  Â·  **çƒ­åº¦**ï¼š70  Â·  **è¯„åˆ†**ï¼š82
...
```

## Command Reference

**Auto Mode**:
- `--mode auto --once` - ç”Ÿæˆä¸€æ¬¡ç®€æŠ¥
- `--topic STR` - æ–‡ä»¶å‘½åï¼ˆä¸å½±å“å†…å®¹ï¼‰

**Series Mode**:
- `--mode series --progress` - æŸ¥çœ‹è¿›åº¦
- `--mode series --episode INT` - ç”ŸæˆæŒ‡å®šé›†
- `--mode series --all` - ç”Ÿæˆå…¨éƒ¨

**Custom Mode**:
- `--mode custom --topic STR` - æŒ‡å®šä¸»é¢˜
- `--prompt STR` - è¯¦ç»†è¦æ±‚

**Refine Mode**:
- `--mode refine --input PATH` - è¾“å…¥æ–‡ä»¶
- `--platforms LIST` - ç›®æ ‡å¹³å°
- `--start INT` - Start episode (default: 1)
- `--end INT` - End episode (default: 100)
- `--all` - Generate all in range
- `--progress` - Show progress only

**Custom Mode Parameters**:
- `--topic STR` - Content topic/keywords (required)
- `--prompt STR` - Detailed content requirements (optional)
- `--audience STR` - Target audience (default: "æŠ€æœ¯ä»Žä¸šè€…")
- `--words INT` - Target word count (optional)
- `--style {technical,practical,tutorial}` - Article style (optional)

**Refine Mode Parameters**:
- `--input PATH` - Input file path (required)
- `--platforms {wechat,xiaohongshu,twitter}` - Target platforms (default: all three)

**Important - Understanding `--topic` Parameter**:
- **Auto mode**: `--topic` is **only for file naming** - actual content is fully auto-generated from real-time AI trends. The topic name doesn't affect the content.
- **Custom mode**: `--topic` is the actual content theme to generate
- **Series/Refine mode**: `--topic` is not used

## Architecture Overview

### Big Picture: Multi-Orchestrator Architecture

This system uses a **multi-orchestrator pattern** where each mode has its own orchestrator implementing a different content generation strategy:

```
src/main.py (CLI entry point)
    â”‚
    â”œâ”€â†’ AutoContentOrchestrator (src/auto_orchestrator.py)
    â”‚   â””â”€â†’ LangGraph StateGraph workflow
    â”‚       â””â”€â†’ Agent chain: trend_analyzer â†’ categorizer â†’ scorer â†’ digest
    â”‚
    â”œâ”€â†’ SeriesOrchestrator (src/series_orchestrator.py)
    â”‚   â””â”€â†’ Sequential execution with error recovery
    â”‚       â””â”€â†’ Agent chain: research â†’ longform â†’ quality check â†’ social content
    â”‚
    â”œâ”€â†’ CustomContentOrchestrator (src/custom_orchestrator.py)
    â”‚   â””â”€â†’ Sequential execution for user-defined topics
    â”‚
    â””â”€â†’ RefineOrchestrator (src/refine_orchestrator.py)
        â””â”€â†’ Multi-platform content adaptation
```

**Key Insight**: The LangGraph StateGraph in Auto mode vs sequential execution in other modes represents a fundamental architectural difference - Auto mode uses graph-based state management while other modes use traditional sequential flows.

### Design Patterns

**1. Multi-Orchestrator Pattern**
- Two orchestrators implement different content generation strategies
- Runtime switching via strategy pattern
- Each manages its own workflow and storage

**2. Factory Pattern**
- `StorageFactory` creates storage instances based on mode
- Supports: `DailyStorage` (by date) and `SeriesStorage` (by series)

**3. Chain of Responsibility**
- LangGraph workflow implements agent chain
- Each agent processes and passes state
- Immutable state updates: `{**state, **updates}`

**4. Template Method Pattern**
- All agents inherit from `BaseAgent`
- Implement standard `execute(state: Dict) -> Dict` interface
- Unified logging, error handling, LLM calls

### Four-Orchestrator Comparison

| Feature | AutoContentOrchestrator | SeriesOrchestrator | CustomContentOrchestrator | RefineOrchestrator |
|---------|-------------------------|-------------------|--------------------------|-------------------|
| **File** | `src/auto_orchestrator.py` | `src/series_orchestrator.py` | `src/custom_orchestrator.py` | `src/refine_orchestrator.py` |
| **Data Source** | 7 real-time APIs | 100 preset topics | User-defined keywords | Input file |
| **Trigger** | Scheduled or manual | Manual execution | Manual execution | Manual execution |
| **Storage** | `data/daily/YYYYMMDD/` | `data/series/{id}/episode_{xxx}/` | `data/custom/{timestamp}_topic/` | `data/refine/{source_name}/` |
| **Output** | Raw data + Digest | Longform articles | Longform + Social content | Multi-platform content |
| **Workflow** | LangGraph graph execution | Sequential with error recovery | Sequential execution | Sequential execution |
| **State Fields** | Uses `trending_topics` | Uses `current_topic` + `selected_ai_topic` | Uses `selected_ai_topic` | Uses `longform_article` |
| **Primary Use** | Daily trend tracking | Systematic content library | On-demand content | Multi-platform publishing |
| **Storage Format** | `YYYYMMDD/` | `series_X_name/episode_XXX/` | `YYYYMMDD_HHMMSS_topic/` | `YYYYMMDD_title/` |

### Auto Workflow Agent Chain (v7.0)

The LangGraph StateGraph builds a directed acyclic graph (DAG) of agents:

```python
# From src/auto_orchestrator.py:_build_workflow()
workflow = StateGraph(dict)
workflow.add_entry_point("ai_trend_analyzer")
workflow.add_edge("ai_trend_analyzer", "trend_categorizer")
workflow.add_edge("trend_categorizer", "news_scoring")
workflow.add_edge("news_scoring", "world_class_digest")
workflow.add_edge("world_class_digest", END)
```

**Data Flow**:
```
ai_trend_analyzer (15+ data sources aggregation)
  â†“ state["trends_by_source"] = {...}

trend_categorizer (æŒ‰5å¤§åˆ†ç±»é‡æ–°ç»„ç»‡)
  â†“ state["categorized_trends"] = {...}

news_scoring (v7.0æ–°å¢žï¼š6ç»´åº¦æ™ºèƒ½è¯„åˆ†ç­›é€‰)
  â†“ state["scored_trends"] = {...}

world_class_digest (ç”Ÿæˆå…¨ä¸­æ–‡ä¸–ç•Œé¡¶çº§æ–°é—»ç®€æŠ¥)
  â†“ state["news_digest"] = {...}
```

### Storage Structure (v4.0)

```
data/
â”œâ”€â”€ daily/                     # Autoæ¨¡å¼
â”‚   â””â”€â”€ YYYYMMDD/
â”‚       â”œâ”€â”€ raw/              # åŽŸå§‹æ•°æ®ï¼ˆæŒ‰æ•°æ®æºï¼‰
â”‚       â””â”€â”€ digest/           # å…¨ä¸­æ–‡ç®€æŠ¥
â”‚           â”œâ”€â”€ digest_YYYYMMDD.md
â”‚           â””â”€â”€ digest_YYYYMMDD.json
â”‚
â”œâ”€â”€ series/                    # Seriesæ¨¡å¼
â”‚   â””â”€â”€ {series_id}/
â”‚       â””â”€â”€ episode_{xxx}/
â”‚
â”œâ”€â”€ custom/                    # Customæ¨¡å¼
â”‚   â””â”€â”€ {timestamp}_topic/
â”‚
â””â”€â”€ refine/                    # Refineæ¨¡å¼
    â””â”€â”€ {source_name}/
```

### Refine Mode Workflow

```
Load input file (article.md)
  â†“
Extract title and content
  â†“
Generate storage name: YYYYMMDD_title
  â†“
For each platform:
  â”Œâ”€ WeChat: wechat_generator â†’ article.html
  â”œâ”€ Xiaohongshu: xiaohongshu_long_refiner â†’ note_long.md (2000 chars)
  â”‚              xiaohongshu_short_refiner â†’ note_short.md (800-1000 chars, viral baokuan style)
  â””â”€ Twitter: twitter_generator â†’ thread.md
```

**Refine Mode Key Points**:
- Storage format: `data/refine/YYYYMMDD_title/` (e.g., `20260115_Claude_Coworkå…¥é—¨æŒ‡å—`)
- Xiaohongshu generates **two versions**:
  - **Long note** (~2000 chars, 6 chapters, deep content) - `xiaohongshu/note_long.md`
  - **Short note** (800-1000 chars, viral baokuan style with ## 1ï¸âƒ£ 2ï¸âƒ£ 3ï¸âƒ£ numbered chapters) - `xiaohongshu/note_short.md`
- Short refiner uses viral content style with numbered chapters, colloquial language ("å¤ªç»äº†", "æ‰’ä¸€çš®"), and strong comparisons
- Each platform's content is saved immediately after generation

### AI Trend Data Sources (config.yaml:30-48)

**Currently Enabled (14 sources)**:
- `techcrunch_ai` - TechCrunch AI RSS
- `newsapi` - NewsAPI.org (å…¨çƒAIæ–°é—»èšåˆ)
- `hackernews` - Hacker News API
- `mit_tech_review` - MIT Technology Review RSS
- `openai_blog` - OpenAI Blog RSS
- `bair_blog` - Berkeley AI Research Blog (é¡¶çº§å­¦æœ¯)
- `microsoft_research` - Microsoft Research Blog (å®˜æ–¹)
- `arxiv_news` - arXiv API
- `marktechpost` - MarkTechPost (AIç ”ç©¶æ–°é—»)
- `kdnuggets` - KDnuggets (æ•°æ®ç§‘å­¦æƒå¨)
- `ai_business` - AI Business (è¡Œä¸šæ–°é—»)
- `the_gradient` - The Gradient (AIç ”ç©¶æœŸåˆŠ)
- `infoq_ai` - InfoQ AI (æŠ€æœ¯åª’ä½“)
- `hugging_face_blog` - Hugging Face Blog (å®˜æ–¹)

**Config Params** (`config/config.yaml:50-53`):
- `max_trends: 20` - Max trend count per source
- `min_heat_score: 60` - Minimum heat score
- `cache_ttl: 3600` - Cache TTL (seconds)

**Data Source Implementation**: Sources are integrated into `RealAITrendAnalyzerAgent` in `src/agents/ai_trend_analyzer_real.py`.

**Adding New Sources**:
```python
# Add new source logic in RealAITrendAnalyzerAgent._fetch_all_trends()
# Return format: [{"title": "...", "url": "...", ...}]
```

## Core Patterns

### Agent Implementation Pattern

All agents inherit from `BaseAgent` (`src/agents/base.py:22`), implementing `execute()`:

```python
from src.agents.base import BaseAgent
from typing import Dict, Any

class NewAgent(BaseAgent):
    def __init__(self, config: Dict[str, Any], prompts: Dict[str, Any]):
        super().__init__(config, prompts)

    def execute(self, state: Dict[str, Any]) -> Dict[str, Any]:
        try:
            # Agent logic
            result = self._call_llm("Your prompt")
            return {**state, "new_field": result}
        except Exception as e:
            self.log(f"Error: {e}", "ERROR")
            return {
                **state,
                "error_message": str(e),
                "current_step": "new_agent_failed"
            }
```

**Key Methods**:
- `_call_llm(prompt: str) -> str` - Call LLM (internal)
- `log(message: str, level: str = "INFO")` - Log with loguru
- `_load_system_prompt() -> str` - Load system prompt (from `config/prompts.yaml`)
- `_init_llm() -> ChatOpenAI` - Initialize LLM (supports ZhipuAI/OpenAI)

### LLM Provider Switching

Multiple LLM providers supported via `config/config.yaml` `llm.provider`:

```yaml
llm:
  provider: "zhipuai"  # or "openai"
  zhipuai:
    model: "glm-4.7"  # Latest flagship
    # Other options: glm-4-flash (cheap fast), glm-4-plus (prev flagship)
    base_url: "https://open.bigmodel.cn/api/coding/paas/v4/"  # Coding endpoint
    thinking:
      enabled: false  # Enable deep thinking mode (GLM-4.7 exclusive)
      type: "auto"    # "auto" or "enabled"
  openai:
    model: "gpt-4o"
    base_url: "https://api.openai.com/v1"
```

**GLM-4.7 Thinking Mode** (`config/config.yaml:17-19`):
- `thinking.enabled`: Enable deep thinking mode (GLM-4.7 exclusive feature)
- `thinking.type`: "auto" (auto-trigger) or "enabled" (force enable)
- Improves reasoning quality for complex tasks

**Research Agent Options** (`config/config.yaml:95-100`):
- `search_provider`: "tavily" (default, paid service), "zhipuai" (included in annual plan, recommended), "mock" (offline)
- `max_results`: Maximum search results
- `search_depth`: "basic" or "advanced"
- `mock_mode`: Set to `true` to disable all search APIs

**API Config Management**: `src/utils/api_config.py` provides `APIConfigManager` for unified API key/endpoint management.

```python
from src.utils.api_config import get_api_config

api_config = get_api_config()
zhipu_key = api_config.get_api_key("zhipuai")
tavily_key = api_config.get_api_key("tavily")
base_url = api_config.get_endpoint("llm.zhipuai.base_url")

# Supports both env vars and config.yaml
# Priority: env vars > config.yaml > defaults
```

### Unified Storage System

`src/utils/storage_v2.py` provides unified storage based on factory pattern:

**Storage Structure**:
```
data/
â”œâ”€â”€ daily/                    # Auto mode (trends + digest)
â”‚   â””â”€â”€ YYYYMMDD/
â”‚       â”œâ”€â”€ raw/
â”‚       â””â”€â”€ digest/
â”‚
â”œâ”€â”€ series/                   # Series mode (100-episode blog series)
â”‚   â””â”€â”€ {series_id}/
â”‚       â”œâ”€â”€ episode_{xxx}/
â”‚       â”‚   â””â”€â”€ longform/
â”‚       â””â”€â”€ series_metadata.json
â”‚
â”œâ”€â”€ custom/                   # Custom mode (user-defined content)
â”‚   â””â”€â”€ {timestamp}_{topic}/
â”‚       â”œâ”€â”€ longform/
â”‚       â”œâ”€â”€ xiaohongshu/
â”‚       â””â”€â”€ twitter/
â”‚
â””â”€â”€ refine/                   # Refine mode (multi-platform content)
    â””â”€â”€ {source_name}/
        â”œâ”€â”€ raw/
        â”œâ”€â”€ wechat/
        â”œâ”€â”€ xiaohongshu/
        â””â”€â”€ twitter/
```

**Usage**:
```python
from src.utils.storage_v2 import StorageFactory

# Auto mode (daily trends)
daily_storage = StorageFactory.create_daily()
daily_storage.save_markdown("digest", "digest.md", content)

# Series mode (100-episode series)
series_storage = StorageFactory.create_series(
    series_id="series_1",  # ä½¿ç”¨åŸºç¡€IDï¼Œå†…éƒ¨ä¼šè‡ªåŠ¨å¤„ç†ç›®å½•å
    episode_number=1
)
series_storage.save_article(content, title="æ–‡ç« æ ‡é¢˜")  # ç›´æŽ¥ä¿å­˜åˆ°episodeç›®å½•
series_storage.save_episode_metadata(metadata)

# Custom mode (user-defined content)
custom_storage = StorageFactory.create_custom("20260114_120000_RAGæŠ€æœ¯")
custom_storage.save_markdown("longform", "article.md", content)

# Refine mode (multi-platform)
refine_storage = StorageFactory.create_refine("my_article")
refine_storage.save_text("wechat", "article.html", html_content)
```

**Series Metadata Management**:
```python
from src.utils.series_manager import get_series_metadata, print_progress_summary

metadata = get_series_metadata("config/blog_topics_100_complete.json")
topic = metadata.get_topic_by_episode(1)
series = metadata.get_series_by_id("series_1")
metadata.update_topic_status("topic_001", "completed")
print_progress_summary()
```

### File Naming Convention (TopicFormatter)

`TopicFormatter` (`src/utils/series_manager.py:201`) provides unified file naming:

```python
from src.utils.series_manager import TopicFormatter

prefix = TopicFormatter.generate_filename_prefix(topic)
# Returns: ep001_llm_transformer_attention_mechanism

filename = TopicFormatter.generate_markdown_filename(topic, "article")
# Returns: ep001_llm_transformer_attention_mechanism_article.md

summary = TopicFormatter.format_topic_summary(topic)
# Output: âœ… Episode 001 | LLMçš„Transformeræž¶æž„ä¸Žæ³¨æ„åŠ›æœºåˆ¶ [series_1]
```

### Series Path Management (SeriesPathManager)

`SeriesPathManager` (`src/utils/series_manager.py`) manages series folder naming:

```python
from src.utils.series_manager import SeriesPathManager

path = SeriesPathManager.get_series_path("series_1")
# Returns: "series_1_llm_foundation"

series_id = SeriesPathManager.get_series_id_from_path("series_1_llm_foundation")
# Returns: "series_1"
```

**Series Folder Format**: `series_X_descriptive_name` (v2.5 improvement for semantic paths)

**Complete Series List**:
- `series_1_llm_foundation` (1-10) - LLM Principles
- `series_2_rag_technique` (11-18) - RAG Practice
- `series_3_agent_development` (19-26) - Agent Development
- `series_4_prompt_engineering` (27-32) - Prompt Engineering
- `series_5_model_deployment` (33-40) - Model Deployment
- `series_6_multimodal_frontier` (41-50) - Multimodal Frontiers
- `series_7_ai_coding_tools` (51-60) - AI Coding Tools
- `series_8_ai_data_engineering` (61-70) - AI Data Engineering
- `series_9_ai_applications` (71-85) - AI Application Scenarios
- `series_10_ai_infrastructure` (86-100) - AI Infrastructure

**Series Path Mapping** (hardcoded): `src/utils/series_manager.py:156-167` (SeriesPathManager.SERIES_NAME_MAP)

**Important**: Adding new series requires updating both:
1. `config/blog_topics_100_complete.json` - Add series info and topics
2. `SeriesPathManager.SERIES_NAME_MAP` - Add path mapping

### LangGraph State Management

`WorkflowState` TypedDict (`src/state.py:61`) manages shared state between agents:

```python
from src.state import create_initial_state, update_state

state = create_initial_state(
    topic=None,  # or "AIæŠ€æœ¯" (file naming only)
    target_audience="æŠ€æœ¯ä»Žä¸šè€…",
    content_type="å¹²è´§åˆ†äº«"
)

new_state = update_state(state, {"new_field": value})
```

**LangGraph Node Wrapper** (`src/auto_orchestrator.py:171-184`):
```python
def _create_agent_node(self, agent):
    def node_function(state):
        try:
            result = agent.execute(state)
            return add_agent_to_order(result, agent.name)
        except Exception as e:
            return update_state(state, {
                "error_message": str(e),
                "current_step": f"{agent.name}_failed"
            })
    return node_function
```

**Critical State Pattern**: All agents must return a complete state dict using the immutable pattern `{**state, **updates}`. The LangGraph workflow automatically merges each node's output into the shared state.

## Workflow Execution Order

**AutoContentOrchestrator** (LangGraph mode): Workflow defined in `src/auto_orchestrator.py:_build_workflow()`

**SeriesOrchestrator** (sequential mode): Workflow in `src/series_orchestrator.py:_execute_workflow()` with safety wrappers and delays

**SeriesOrchestrator Safe Execution** (`src/series_orchestrator.py:248-262`):
```python
def _call_agent_safely(agent_name: str, state: Dict[str, Any]) -> Dict[str, Any]:
    try:
        result = self.agents[agent_name].execute(state)
        time.sleep(2)  # Delay to avoid API rate limits
        return result
    except Exception as e:
        logger.error(f"[{agent_name}] Failed: {e}")
        time.sleep(2)
        return state  # Return original state, allow continuation
```

**Execution Order**:
1. AI trend analysis â†’ Trend digest â†’ Content research
2. Longform generation â†’ Code review
3. Xiaohongshu refinement â†’ Twitter generation
4. Title optimization â†’ Image generation
5. Quality evaluation

**Prompt Template System**: Each agent's system prompts stored in `config/prompts.yaml`, organized by lowercase agent class name

### Agent Dependencies

**Auto Mode v7.0 Agents**:
| Agent | Deps On | Outputs | Description |
|-------|---------|---------|-------------|
| ai_trend_analyzer | - | trends_by_source | 15+ data source aggregation |
| trend_categorizer | trends_by_source | categorized_trends | 5-category organization |
| news_scoring | categorized_trends | scored_trends | 6-dimensional scoring (v7.0) |
| world_class_digest | scored_trends | news_digest | Chinese digest + JSON |

**Other Agents** (Series/Custom/Refine modes):
| Agent | Deps On | Outputs | Description |
|-------|---------|---------|-------------|
| trends_digest | trending_topics | digest_content | Optional digest |
| research_agent | selected_ai_topic | research_data, research_summary | Background for longform |
| longform_generator | selected_ai_topic, research_data | longform_article | Core content |
| code_review_agent | longform_article | code_review_result | Quality assurance |
| fact_check_agent | longform_article | fact_check_result | Quality assurance |
| xiaohongshu_refiner | longform_article | xiaohongshu_note | Content adaptation |
| twitter_generator | longform_article | twitter_post | Content adaptation |
| title_optimizer | longform_article | optimized_titles | SEO optimization |
| image_generator | xiaohongshu_note or twitter_post | image_prompts | Image generation |
| quality_evaluator | All outputs | quality_report | Final evaluation |
| consistency_checker_agent | longform_article | consistency_report | Terminology/citation check |
| visualization_generator_agent | longform_article | mermaid_diagrams | Auto-generate diagrams |
| citation_formatter_agent | longform_article | formatted_citations | GB/T 7714-2015 format |

### Critical Notes

1. **Xiaohongshu/Twitter Agents Must Execute Sequentially**: Both read `longform_article` but should not run in parallel to avoid state update conflicts (`src/auto_orchestrator.py:213`)

2. **Longform Generator Needs Research Data**: `longform_generator` prioritizes `research_data`; if unavailable, generates based only on `selected_ai_topic`

3. **Series Mode State Field Special Handling**: Must set both `current_topic` and `selected_ai_topic` for compatibility (`src/series_orchestrator.py:_initialize_state()`)

4. **Staged Generation Avoids Timeout**: `LongFormGeneratorAgent` uses three-stage generation (outline â†’ section expansion â†’ summary), each stage independent LLM call (`src/agents/longform_generator.py:73`)

## Development Guide

### Adding New Agents

**AutoContentOrchestrator Mode**:
1. Create agent class (`src/agents/new_agent.py`) inheriting `BaseAgent`
2. Implement `execute(self, state: Dict[str, Any]) -> Dict[str, Any]`
3. Add config in `config/config.yaml` under `agents`
4. Initialize in `src/auto_orchestrator.py` `_init_agents()`
5. Add to LangGraph workflow in `_build_workflow()`

**SeriesOrchestrator Mode**:
1. Create agent class as above
2. Add to `agent_classes` dict in `src/series_orchestrator.py` `_init_agents()`
3. Add call logic in `_execute_workflow()`

**Important**: Agents must return complete state dict using `{**state, "new_field": value}` pattern.

### Common State Fields

| State Field | Written By | Read By | Description |
|-------------|------------|----------|-------------|
| `trending_topics` | ai_trend_analyzer | trends_digest | AI trend list (Auto mode) |
| `digest_content` | trends_digest | - | Trend digest content |
| `research_data` | research_agent | longform_generator | Web search research data |
| `selected_ai_topic` | ai_trend_analyzer / series_orchestrator | longform_generator | Selected AI topic |
| `current_topic` | series_orchestrator | - | Current topic (Series mode) |
| `longform_article` | longform_generator | code_review_agent, xiaohongshu_refiner, twitter_generator | Longform article |
| `xiaohongshu_note` | xiaohongshu_refiner | - | Xiaohongshu note |
| `twitter_post` | twitter_generator | - | Twitter post |
| `error_message` | Any agent | - | Error info |
| `current_step` | Any agent | - | Current step |
| `execution_time` | orchestrator | - | Execution time stats |
| `agent_execution_order` | orchestrator | - | Agent execution order |

**Note**: `WorkflowState` TypedDict defines possible fields, but actual usage is plain Dict. Auto mode uses `trending_topics`, Series mode uses `current_topic` and `selected_ai_topic`. State updates use immutable pattern: `{**state, **updates}`.

### Error Handling Pattern

**Agent-level Error Handling**:
```python
def execute(self, state: Dict[str, Any]) -> Dict[str, Any]:
    try:
        result = self._call_llm("Your prompt")
        return {**state, "new_field": result}
    except Exception as e:
        self.log(f"Error: {e}", "ERROR")
        return {
            **state,
            "error_message": str(e),
            "current_step": "new_agent_failed"
        }
```

**Series Mode Safe Execution** (`src/series_orchestrator.py:248`):
```python
def _call_agent_safely(agent_name: str, state: Dict[str, Any]) -> Dict[str, Any]:
    try:
        result = self.agents[agent_name].execute(state)
        time.sleep(2)
        return result
    except Exception as e:
        logger.error(f"[{agent_name}] Failed: {e}")
        time.sleep(2)
        return state  # Return original state, allow continuation
```

**Error Recovery Strategy**: Agents return original state on failure, allowing workflow to continue. Records `error_message` and `current_step` for debugging.

### Testing & Debugging

**Enable Verbose Logging**: Edit `config/config.yaml`
```yaml
logging:
  level: "DEBUG"
```

**Mock Mode Testing** (no API quota consumption):
```yaml
agents:
  ai_trend_analyzer:
    mock_mode: true
```

**Test Individual Agent**:
```bash
cd test
PYTHONPATH=/Users/z/Documents/work/content-forge-ai python test_ai_trends.py --source hackernews
```

### Test Files

**Test Directory**: `test/`

| Test File | Purpose |
|-----------|---------|
| `test_ai_trends.py` | Test AI trend fetching from single data source |
| `test_storage.py` | Test storage system functionality |
| `test_topic_logic.py` | Test topic parameter handling across modes |
| `test_digest.py` | Test trend digest generation |
| `test_auto_topic.py` | Test auto mode topic handling |
| `test_new_sources.py` | Test new data source integration |

**Test README**: `test/README.md` contains detailed documentation for test files.

**Running Tests**:
```bash
cd test
PYTHONPATH=/Users/z/Documents/work/content-forge-ai python test_ai_trends.py --source hackernews
PYTHONPATH=/Users/z/Documents/work/content-forge-ai python test_storage.py
```

**Test Single AI Trend Source**:
```bash
cd test
PYTHONPATH=/Users/z/Documents/work/content-forge-ai python test_ai_trends.py --source hackernews
# Sources: hackernews, arxiv, huggingface, stackoverflow, devto, pypi, github_topics, kaggle
```

**Note**: `src/data_sources/` directory removed in v2.5; data sources integrated into `src/agents/ai_trend_analyzer_real.py`.

## Code Standards

- Use type annotations (PEP 484)
- Follow PEP 8
- Add docstrings
- Use `self.log()` instead of `print()`
- Agent `execute()` must return complete state dict
- Exception handling must update `error_message` and `current_step` fields

## Architecture Highlights

### Staged Longform Generation

`LongFormGeneratorAgent` (`src/agents/longform_generator.py:73`) uses three-stage generation to avoid timeouts:

1. **Stage 1**: Generate article outline
2. **Stage 2**: Expand sections section-by-section (loop LLM calls, optional context window)
3. **Stage 3**: Generate summary

This enables 9000-13000 word professional in-depth analysis.

### Web Search Deep Research

`ResearchAgent` uses Tavily API for web search, collecting official docs, GitHub, tech blogs. Research data stored in `state["research_data"]`.

### Quality Assurance Triple Check

1. **CodeReviewAgent**: Reviews code examples for correctness and best practices
2. **FactCheckAgent**: Fact-checks statements, technical parameters, dates/times
3. **QualityEvaluatorAgent**: Comprehensive quality assessment (score 7-10)

## Performance Optimization

### Cost Reduction

- Use `glm-4-flash` instead of `glm-4.7` (~80% cost reduction)
- Reduce data sources (e.g., keep only hackernews + arxiv)
- Lower `max_tokens` settings
- Disable unnecessary agents (set `enabled: false`)

### Speed Up Execution

- Use mock mode for development testing
- Reduce data sources

## Common Issues

**Import Error**: Ensure running from project root with PYTHONPATH set
```bash
export PYTHONPATH=/Users/z/Documents/work/content-forge-ai  # Replace with your actual path
python src/main.py --mode auto --once
```

**API Key Error**: Check environment variables
```bash
echo $ZHIPUAI_API_KEY
# or
cat .env
```

**View Logs**: Logs stored by date in `logs/YYYYMMDD/app.log`
```bash
tail -f logs/$(date +%Y%m%d)/app.log
```

**Generate Only Partial Content**: Edit `config/config.yaml` to disable unwanted agents
```yaml
agents:
  longform_generator:
    enabled: false
```

**Debug Specific Agent**: Enable mock_mode to avoid API calls
```yaml
agents:
  ai_trend_analyzer:
    mock_mode: true
```

**NewsAPI Rate Limiting**: NewsAPI has a free tier limit. If you hit rate limits:
1. Reduce `max_trends` in config.yaml
2. Remove `newsapi` from sources list
3. Or upgrade to NewsAPI paid tier

## Important Architecture Gotchas

1. **State Field Naming Confusion**: Auto mode uses `trends_by_source`/`categorized_trends`/`scored_trends`, but older code and Series mode use `trending_topics`. These are NOT compatible.

2. **Agent Name vs State Field**: The agent is named `ai_trend_analyzer` but outputs `trends_by_source`, not `trending_topics`.

3. **DailyStorage Only Creates Two Directories**: As of v4.0+, `DailyStorage` only creates `raw/` and `digest/` subdirectories. Other directories like `longform/` will NOT be created in Auto mode.

4. **Research Agent Provider**: The `research_agent` uses `search_provider: "tavily"` by default (paid service). You can change to "zhipuai" (included in annual plan) or "mock" (offline development) in config.yaml.

5. **Series ID vs Path**: Series use two different identifiers:
   - `series_id`: "series_1" (internal ID, used in JSON config)
   - `series_path`: "series_1_llm_foundation" (folder name, used in filesystem)
   - Always use `SeriesPathManager` to convert between them.

6. **LLM Provider Base URL**: ZhipuAI uses a special coding endpoint: `https://open.bigmodel.cn/api/coding/paas/v4/` (NOT the standard API endpoint). This is configured in `config.yaml`.

## Key File Locations

### Core Files

| File | Purpose |
|------|---------|
| `src/main.py` | Unified entry (supports --mode switching) |
| `src/auto_orchestrator.py` | LangGraph workflow orchestration (auto mode) |
| `src/series_orchestrator.py` | Series mode orchestrator |
| `src/state.py` | State definition (WorkflowState TypedDict) |
| `src/utils/storage_v2.py` | Unified storage system (StorageFactory) |
| `src/utils/series_manager.py` | Series management tools |
| `src/utils/api_config.py` | API config management (APIConfigManager) |
| `config/config.yaml` | Main config (LLM, agents, workflow) |
| `config/blog_topics_100_complete.json` | 100-episode plan |
| `config/prompts.yaml` | Agent system prompt templates |
| `.env` | Environment variables (API keys) |
| `.env.example` | Environment variable examples |
| `run_and_commit.sh` | Automated deployment script |

### Agent Classes (src/agents/)

**Auto Mode v7.0 Agents**:
| Agent Class | File | Purpose |
|-------------|------|---------|
| `BaseAgent` | `base.py` | Agent base class |
| `RealAITrendAnalyzerAgent` | `ai_trend_analyzer_real.py` | AI trend analysis (15+ data sources) |
| `TrendCategorizerAgent` | `trend_categorizer_agent.py` | 5-category organization |
| `NewsScoringAgent` | `news_scoring_agent.py` | 6-dimensional scoring (v7.0) |
| `WorldClassDigestAgent` | `world_class_digest_agent.py` | Chinese digest + JSON |

**Content Generation Agents** (Series/Custom/Refine modes):
| Agent Class | File | Purpose |
|-------------|------|---------|
| `TrendsDigestAgent` | `trends_digest_agent.py` | Trend digest generation |
| `LongFormGeneratorAgent` | `longform_generator.py` | Longform generation (staged) |
| `XiaohongshuLongRefinerAgent` | `xiaohongshu_long_refiner.py` | Xiaohongshu long note (~2000 chars) |
| `XiaohongshuShortRefinerAgent` | `xiaohongshu_short_refiner.py` | Xiaohongshu short note (800-1000 chars, viral baokuan style) |
| `TwitterGeneratorAgent` | `twitter_generator.py` | Twitter post generation |
| `WechatGeneratorAgent` | `wechat_generator.py` | WeChat HTML generation |
| `TitleOptimizerAgent` | `title_optimizer.py` | Title optimization |
| `ImageGeneratorAgent` | `image_generator.py` | Image prompt generation |
| `ResearchAgent` | `research_agent.py` | Web search deep research |

**Quality Assurance Agents**:
| Agent Class | File | Purpose |
|-------------|------|---------|
| `CodeReviewAgent` | `code_review_agent.py` | Code quality review |
| `FactCheckAgent` | `fact_check_agent.py` | Fact verification |
| `QualityEvaluatorAgent` | `quality_evaluator_agent.py` | Comprehensive quality assessment |
| `ConsistencyCheckerAgent` | `consistency_checker_agent.py` | Terminology/citation check |
| `VisualizationGeneratorAgent` | `visualization_generator_agent.py` | Auto-generate Mermaid diagrams |
| `CitationFormatterAgent` | `citation_formatter_agent.py` | GB/T 7714-2015 format |

## Related Documentation

- **README.md** - Project overview and quick start
- **test/README.md** - Test file documentation

---

**Version**: v7.0
**Updated**: 2026-01-25

## Summary of Key Changes

This CLAUDE.md has been improved with:

1. **Added test commands** - Test files section now includes actual commands to run tests
2. **Corrected data source table** - Removed non-existent sources (Anthropic, Google AI, The Verge, VentureBeat) and aligned with actual config.yaml
3. **Enhanced architecture overview** - Added "Big Picture" section showing multi-orchestrator pattern
4. **Clarified LangGraph workflow** - Added actual code snippet from `src/auto_orchestrator.py:_build_workflow()`
5. **Added "Important Architecture Gotchas" section** - Six critical gotchas that can trip up developers
6. **Added state flow annotations** - Shows exactly which state fields each agent outputs
7. **Added NewsAPI rate limiting note** - Common issue when using free tier
8. **Improved PYTHONPATH reminder** - Added note to replace with actual path
9. **Added test commands** - Shows how to run individual test files
